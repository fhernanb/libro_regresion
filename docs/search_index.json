[["index.html", "Modelos de Regresión con R Bienvenido Estructura del libro Software y convenciones Bloques informativos", " Modelos de Regresión con R Freddy Hernández Olga Usuga Mauricio Mazo 2024-08-06 Bienvenido Este libro está destinado para estudiantes de ingeniería y estadística que deseen aprender sobre modelos de regresión y la forma de aplicarlos por medio del lenguaje de programación R. Freddy Hernández Olga Usuga Mauricio Mazo Estructura del libro En el capítulo 1 se presenta el modelo de regresión lineal simple y en el Capítulo 3 se generaliza el modelo básico con varias covariables. En los capítulos 6, 8 y 9 se muestran como hacer inferencias, particular, se muestra como construir intervalos de confianza y como realizar pruebas de hipótesis. Software y convenciones Para realizar este libro usamos los paquetes knitr (Xie 2015) y bookdown (Xie 2024) que permiten unir la ventajas de LaTeX y R en un mismo archivo. En todo el libro se presentarán códigos que el lector puede copiar y pegar en su consola de R para obtener los mismos resultados aquí del libro. Los códigos se destacan en una caja de color similar a la mostrada a continuación. 4 + 6 a &lt;- c(1, 5, 6) 5 * a 1:10 Los resultados o salidas obtenidos de cualquier código se destacan con dos símbolos de númeral (##) al inicio de cada línea o renglón, esto quiere decir que todo lo que inicie con ## son resultados obtenidos y NO los debe copiar. Abajo se muestran los resultados obtenidos luego de correr el código anterior. ## [1] 10 ## [1] 5 25 30 ## [1] 1 2 3 4 5 6 7 8 9 10 Bloques informativos En varias partes del libro usaremos bloques informativos para resaltar algún aspecto importante. Abajo se encuentra un ejemplo de los bloques y su significado. Nota aclaratoria. Sugerencia. Advertencia. References Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ———. 2024. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. "],["rls.html", "1 Regresión lineal simple Modelo estadístico Función lm Clase lm Mínimos cuadrados ponderados Apéndice", " 1 Regresión lineal simple En este capítulo se presenta una descripción breve del modelo de regresión lineal simple y la forma de estimar los parámetros del modelo con R. Modelo estadístico El modelo estadístico en regresión lineal simple se puede escribir de dos formas como se muestra a continuación. En esta forma la variable respuesta \\(Y\\) se expresa como una suma de \\(\\beta_0 + \\beta_1 X_i\\) y un error aleatorio \\(\\epsilon_i\\) el cual tiene distribución \\(N(0, \\sigma^2)\\). El modelo en esta forma se puede expresar como sigue. \\[\\begin{align} \\label{mod1} Y_i &amp;= \\beta_0 + \\beta_1X_i + \\epsilon_i,\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2) \\end{align}\\] En esta forma la variable respuesta \\(Y\\) tiene distribución normal con media que cambia en función de la variable \\(X\\) pero con varianza constante. El modelo en esta forma se puede expresar como sigue. \\[\\begin{align} \\label{mod2} Y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 X_i, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] En cualquiera de las dos formas el vector de parámetros del modelo es \\(\\boldsymbol{\\theta}=(\\beta_0, \\beta_1, \\sigma)^\\top\\). Para estimar este vector de parámetros se suelen utilizar dos métodos: Estimación por mínimos cuadrados y Estimación por máxima verosimilitud. La siguiente figura, tomada del libro Kutner et al. (2005), muestra con claridad el supuesto de normalidad de la variable \\(Y\\), la dependencia de la media \\(\\mu\\) con \\(X\\) y la varianza constante. En la siguiente figura se muestran dos ilustraciones para donde los supuestos no se cumplen. En el panel izquierdo se observa que la varianza de las distribuciones no es constante mientras que en el panel de la derecha la distribución no es la normal. Función lm La función lm (linear model) de R se usa para ajustar un modelo de regresión lineal simple, la estructura de esta función se muestra a continuación. lm(formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...) A continuación se presenta una corta descripción de los parámetros más usados en la función. formula: es un objeto de la clase fórmula para indicar la variable respuesta y las covariables. Por ejemplo, si formula = y ~ x1 + x2 lo que se indica es que la variable respuesta es y, las covariables serían x1 y x2. data: es el marco de datos donde se buscarán las variables usadas en la fórmula. Si este parámetro queda vacío, R buscará las variables en el ambiente global. Ejemplo Como ilustración vamos a usar los datos del ejemplo 2.1 del libro E. &amp;. V. Montgomery D. &amp; Peck (2006). En el ejemplo 2.1 los autores desean ajustar un modelo de regresión lineal simple para explicar la Resistencia de una soldadura en función de la Edad de la soldadura. Solución A continuación el código para cargar los datos y para mostrar las 6 primeras observaciones de la base de datos, en total tenemos 20 observaciones. file &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/propelente&quot; datos &lt;- read.table(file=file, header=TRUE) head(datos) # shows the first 6 rows ## Resistencia Edad ## 1 2158.70 15.50 ## 2 1678.15 23.75 ## 3 2316.00 8.00 ## 4 2061.30 17.00 ## 5 2207.50 5.50 ## 6 1708.30 19.00 Para crear un diagrama de dispersión que nos muestre la relación entre las dos variables usamos las siguientes instrucciones. library(ggplot2) ggplot(datos, aes(x=Edad, y=Resistencia)) + geom_point() + theme_light() De la figura anterior se ve claramente que a medida que aumenta la edad de la soldadura, la resistencia que ella ofrece disminuye. Adicionalmente, se observa que la relación entre las variables es lineal con una dispersión que parece constante. El modelo que se va a ajustar se muestra a continuación. \\[\\begin{align} Resistencia_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 Edad_i, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] Para obtener las estimaciones de los parámetros del modelo anterior se usa el código mostrado abajo. La función lm se aplica con la fórmula Resistencia ~ Edad para indicar que Resistencia es la variable respuesta y que Edad es la variable explicativa. Los resultados del la función lm se almacenan en el objeto mod1 para luego poder usar el modelo ajustado. La segunda línea del código mostrado abajo se usa para mostrar por pantalla un reporte sencillo del modelo ajustado. mod1 &lt;- lm(Resistencia ~ Edad, data=datos) mod1 # Para imprimir el objeto mod1 ## ## Call: ## lm(formula = Resistencia ~ Edad, data = datos) ## ## Coefficients: ## (Intercept) Edad ## 2627.82 -37.15 En la salida anterior se observan los valores estimados de \\(\\beta_0\\) y \\(\\beta_1\\) pero no aparece la estimación de \\(\\sigma\\). Para obtener una tabla de resumen con detalles del modelo ajustado, se usa la función genérica summary, a continuación el código necesario para obtener la tabla. summary(mod1) La siguiente figura muestra el resultado del código anterior. En la figura se resalta la información sobre los residuales y las estimaciones de \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\sigma\\). Figure 1.1: Tabla de resumen para un modelo lineal. Con los resultados anteriores se puede expresar el modelo ajustado como se muestra a continuación. \\[\\begin{align} \\widehat{Resistencia}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2), \\\\ \\hat{\\mu}_i &amp;= 2627.822 -37.154 \\, Edad_i, \\\\ \\hat{\\sigma} &amp;= 96.11 \\end{align}\\] ¿Cómo se pueden interpretar los efectos \\(\\hat{\\beta}\\)? Por cada semana que envejezca la soldadura, se espera que la resistencia promedio disminuya en 37.154 psi. Si la soldadura es nueva (\\(Edad =0\\)), se espera que la resistencia promedio sea de 2627.822 psi. Para incluir la recta de regresión que representa el modelo ajustado anterior se puede usar el siguiente código. ggplot(datos, aes(x=Edad, y=Resistencia)) + geom_point() + geom_smooth(method=&#39;lm&#39;, formula=y~x, se=FALSE, col=&#39;dodgerblue1&#39;) + theme_light() Clase lm Todo objeto creado con la función lm( ) es de la clase lm, para verificar esto podemos escribir lo siguiente en la consola. class(mod1) ## [1] &quot;lm&quot; Dentro de todo objeto de la clase lm hay doce elementos o valores que se pueden utilizar. A continuación se muestra el código para saber los nombres de esos elementos. names(mod1) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; Cualquiera de los anteriores elementos se pueden extraer usando el operador $ entre el objeto y el elemento, por ejemplo, para extraer los coeficientes estimados se escribe lo siguiente: mod1$coefficients ## (Intercept) Edad ## 2627.82236 -37.15359 Los nombres de los elementos dan una idea clara de lo que ellos almacenan, si el lector quiere más detalles sobre los elementos, sólo debe solicitar la ayuda de la función con help(lm) y en la sección de Value encontrará la información. En el siguiente código se utiliza para extraer el elemento $fitted.value que contiene los valores predichos o ajustados \\(\\hat{y}_i\\). mod1$fitted.values ## 1 2 3 4 5 6 7 8 ## 2051.942 1745.425 2330.594 1996.211 2423.478 1921.904 1736.136 2534.938 ## 9 10 11 12 13 14 15 16 ## 2349.170 2219.133 2144.826 2488.496 1698.983 2265.575 1810.443 1959.058 ## 17 18 19 20 ## 2404.901 2163.402 2553.515 1829.020 A continuación se muestra el código para extraer el elemento $residuals que contiene los residuales usuales \\(\\widehat{e}_i= Y_i - \\widehat{Y}_i\\). mod1$residuals ## 1 2 3 4 5 6 ## 106.758301 -67.274574 -14.593631 65.088687 -215.977609 -213.604131 ## 7 8 9 10 11 12 ## 48.563824 40.061618 8.729573 37.567141 20.374323 -88.946393 ## 13 14 15 16 17 18 ## 80.817415 71.175153 -45.143358 94.442278 9.499187 37.097528 ## 19 20 ## 100.684823 -75.320154 Los valores ajustados y los residuales también se pueden recuperar usando las funciones fitted( ) y residuals( ). Consulte la ayuda de estas funciones para conocer otros detalles. Ejemplo Aquí se retoma el ejemplo de Resistencia en función de la Edad. El objetivo es crear una diagrama de dispersión con los puntos originales \\((X_i, Y_i)\\), las estimaciones \\(\\widehat{Y}_i\\) y los residuales \\(\\widehat{e}_i\\). Lo primero que se debe hacer es agregar a los datos originales el vector con las estimaciones \\(\\hat{y}_i\\), el código necesario se muestra a continuación. datos$predicciones &lt;- predict(mod1) En el diagrama dispersión los puntos originales \\((X_i, Y_i)\\) estarán en color negro y los puntos correspondientes a las estimaciones \\((X_i, \\hat{Y}_i)\\) estarán en color rojo, estos últimos coincidirán con la recta estimada en color gris. Los residuales \\(e_i\\) se mostrarán como líneas a trazos de color rojo. El código necesario se muestra a continuación. ggplot(datos, aes(x=Edad, y=Resistencia)) + geom_smooth(method=&quot;lm&quot;, se=FALSE, color=&quot;lightgrey&quot;) + geom_segment(aes(xend=Edad, yend=predicciones), col=&#39;red&#39;, lty=&#39;dashed&#39;) + geom_point() + geom_point(aes(y=predicciones), col=&#39;red&#39;) + theme_light() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Mínimos cuadrados ponderados Los mínimos cuadrados ponderados son una generalización de los mínimos cuadrados ordinarios y se usan para hacer una regresión en la cual las observaciones tienen diferente peso o importancia en el modelo. Ejemplo Use la información de las siguientes variables y ajuste dos modelos de regresión, uno sin usar los pesos w y otro usando los pesos w. Luego dibuje en una misma figura las rectas o modelos ajustados. x &lt;- c(4, 6, 8, 7, 8, 5) y &lt;- c(1, 2, 3, 4, 5, 4) w &lt;- c(0.1, 0.1, 0.2, 0.1, 0.2, 0.9) Solución Vamos a ajustar los modelos de la siguiente manera. mod_sin_pesos &lt;- lm(y ~ x) mod_con_pesos &lt;- lm(y ~ x, weights=w) Ahora vamos a explorar los coefientes. coef(mod_sin_pesos) ## (Intercept) x ## -0.475 0.575 coef(mod_con_pesos) ## (Intercept) x ## 2.6260504 0.1806723 A continuación se muestra el código para construir el diagrama de dispersión usando la función symbols. La información de la importancia de cada observación se incluye en el dibujo usando circles=w. Finalmente se agregan las rectas de los dos modelos ajustados. symbols(x=x, y=y, circles=w, pch=20, las=1, inches=0.1, fg=&#39;red&#39;, lwd=2) abline(mod_sin_pesos, col=&#39;seagreen&#39;) abline(mod_con_pesos, col=&#39;dodgerblue1&#39;) legend(&#39;topleft&#39;, legend=c(&#39;Sin pesos&#39;, &#39;Con pesos&#39;), col=c(&#39;seagreen&#39;, &#39;dodgerblue1&#39;), bty=&#39;n&#39;, lwd=1) De la anterior figura podemos ver que los ajustes son muy diferentes. La recta de color azul tiene menor pendiente y un mayor punto de corte. Esta recta azul tiende a estar más cerca del punto (5, 4) porque este tiene un peso de 0.9, el mayor de todos los pesos. El efecto de este punto en el modelo es que él hala la recta de regresión hacia él debido a su gran peso de 0.9. El método de mínimos cuadrados ponderados se usa cuando queremos hacer una regresión en la cual cada observación aporta de forma diferente al modelo. Apéndice Abajo están los apéndices que complementan. Estimación por mínimos cuadrados Para estimar \\(\\beta_0\\) y \\(\\beta_1\\) utilizamos el método de mínimos cuadrados ordinarios, minimizando la suma cuadrática del error: \\[\\begin{eqnarray} S(\\beta_0, \\beta_1)&amp;=&amp;\\sum_{i=1}^n \\epsilon_i^2\\\\ &amp;=&amp;\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2 \\end{eqnarray}\\] Derivando e igualando a cero, es decir, \\[\\frac{\\partial S(\\beta_0, \\beta_1)}{\\partial \\beta_0}=0\\] \\[\\frac{\\partial S(\\beta_0, \\beta_1)}{\\partial \\beta_1}=0\\] obtenemos que los estimadores para \\(\\beta_0\\) y \\(\\beta_1\\) están dados por \\[\\widehat{\\beta}_0=\\overline{Y}-\\widehat{\\beta}_1\\overline{X}\\quad \\text{y}\\quad \\widehat{\\beta}_1=\\frac{S_{xy}}{S_{xx}}\\] donde \\(S_{xx}=\\sum_{i=1}^n(X_i-\\overline{X})^2\\) &amp; \\(S_{yy}=\\sum_{i=1}^n(Y_i-\\overline{Y})^2\\) &amp; \\(S_{xy}=S_{yx}=\\sum_{i=1}^n(X_i-\\overline{X})(Y_i-\\overline{Y})\\). Estimación por máxima verosimilitud Sabemos que para nuestro modelo de regresión lineal simple, \\[Y_i\\sim N\\left(\\beta_0+\\beta_1X_i, \\sigma^2\\right),\\quad \\text{para todo $i=1, 2, \\ldots, n$}.\\] Y como los errores \\(\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n\\) son independientes, entonces \\(Y_1, Y_2, \\ldots, Y_n\\) también son independientes. Así, la función de verosimilitud estaría dada por: \\[\\begin{eqnarray*} L\\left(\\beta_0, \\beta_1, \\sigma^2 | X_1, \\ldots, X_n, Y_1, \\ldots, Y_n \\right)&amp;=&amp;\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}(Y_i-\\beta_0-\\beta_1X_i)^2\\right\\}\\\\ &amp;=&amp;\\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2\\right\\} \\end{eqnarray*}\\] Tomando logaritmo natural a ambos lados y denotando la log-verosimilitud por \\[\\ln \\left[L\\left(\\beta_0, \\beta_1, \\sigma^2 | X_1, \\ldots, X_n, Y_1, \\ldots, Y_n \\right)\\right]=l\\left(\\beta_0, \\beta_1, \\sigma^2 | X_1, \\ldots, X_n, Y_1, \\ldots, Y_n \\right)=l,\\] tenemos que \\[ l =-\\left(\\frac{n}{2}\\right)\\ln(2\\pi)-\\left(\\frac{n}{2}\\right)\\ln\\left(\\sigma^2\\right)-\\left(\\frac{1}{2\\sigma^2}\\right)\\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2 \\] Derivando parcialmente con respecto a \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\sigma^2\\) e igualando a cero, obtenemos los estimadores de máxima verosimilitud: \\[\\widetilde{\\beta}_0=\\overline{Y}-\\widetilde{\\beta}_1\\overline{X}\\] \\[\\widetilde{\\beta}_1=\\frac{S_{xy}}{S_{xx}}\\] \\[\\widetilde{\\sigma}^2=\\frac{\\sum_{i=1}^n(Y_i-\\widetilde{\\beta}_0-\\widetilde{\\beta}_1X_i)^2}{n}.\\] Los estimadores de máxima verosimilitud para regresión lineal simple, pueden ser obtenidos mediante la optimización directa de la función de log-verosimilitud. References Kutner, M., C. Nachtsheim, J. Neter, and W. Li. 2005. Applied Linear Statistical Models. Fifth. McGraw Hill/Irwin. Montgomery, E. &amp; Vining, D. &amp; Peck. 2006. Introducción Al Análisis de Regresión Lineal. 3ed ed. México: Cecsa. "],["trans.html", "2 Transformaciones Transformaciones para linealizar el modelo", " 2 Transformaciones En este capítulo se muestra cómo una transformación de la variable respuesta o de las covariables puede mejorar el modelo. Transformaciones para linealizar el modelo En algunas ocasiones el modelo teórico de los datos tiene una estructura matemática que puede ser linealizable al transformar \\(Y\\) y/o \\(X\\). En la siguiente tabla se muestran algunos de los casos en los cuales se puede linealizar el modelo. En la siguiente figura se muestran los patrones de los modelos que se pueden linealizar. Ejemplo Este ejemplo está basado en el ejericio 5.1 propuesto en el libro E. &amp;. V. Montgomery D. &amp; Peck (2006) se presenta una base de datos de un estudio de cómo influye la temperatura (°C) sobre la viscosidad (mPa s) de las mezclas de tolueno y tetralina. La base de datos cuenta con un total de 50 observaciones, a continuación se muestran las primeras seis observaciones de la base. ## temp visc ## 1 85.63 0.27 ## 2 64.48 0.40 ## 3 42.12 0.62 ## 4 76.73 0.31 ## 5 58.18 0.46 ## 6 42.35 0.60 Crear un diagrama de dispersión. ¿Parece apropiado usar una línea recta para modelar los datos? Ajuste el modelo de línea recta. Calcule las estadísticas resumidas y las gráficas residuales. ¿Cuáles son sus conclusiones sobre la adecuación del modelo? Los principios básicos de la química física sugieren que la viscosidad es una función exponencial de la temperatura. Repita la parte b usando la transformación adecuada según esta información. ¿Cuál modelo se debería usar? Escriba la ecuación del modelo. Con el modelo elegido, calcule \\(\\widehat{Visco}\\) para \\(Temp=0\\) y para \\(Temp=50\\). Solución Diagrama de dispersión library(ggplot2) ggplot(datos, aes(x=temp, y=visc)) + geom_point() + theme_light() + xlim(0, 120) + labs(y=&quot;Viscocidad (mPa s)&quot;, x=&quot;Temperatura (°C)&quot;) Ajustando el modelo de línea recta. m1 &lt;- lm(visc ~ temp, data=datos) Los estadísticos de resumen se obtiene así: summary(m1) ## ## Call: ## lm(formula = visc ~ temp, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.048849 -0.026135 -0.006075 0.027772 0.079221 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0295982 0.0101903 101.04 &lt;2e-16 *** ## temp -0.0092809 0.0001646 -56.38 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.03356 on 98 degrees of freedom ## Multiple R-squared: 0.9701, Adjusted R-squared: 0.9698 ## F-statistic: 3179 on 1 and 98 DF, p-value: &lt; 2.2e-16 De la tabla anterior se observa que: La temperatura es significativa porque valor-p es &lt; 2e-16. Por cada grado °C en que aumente la temperatura, la viscocidad disminuye en 0.00928098 unidades. El \\(R^2\\) es alto, presenta un valor de 0.9701. El modelo m1 es significativo porque valor-p es &lt; 2.2e-16. Para ver los residuales del modelo se hace lo siguiente: par(mfrow=c(2, 2)) plot(m1, las=1, col=&#39;purple&#39;, which=1:3, cex=1.5, pch=19) Claramente los residuales muestran un patrón de u, eso indica que hay algo errado con el modelo m1. res_m1 &lt;- residuals(m1) shapiro.test(res_m1) ## ## Shapiro-Wilk normality test ## ## data: res_m1 ## W = 0.93894, p-value = 0.0001661 Vamos a entrenar un modelo asumiendo un patrón exponencial multiplicativo \\(Y = \\beta_0 e^{\\beta_1X}\\varepsilon\\). Para poder ajustar este modelo en R, es necesario transformar la variable respuesta, es decir, vamos a entrenar con \\(Y^*=\\log(Y)\\). En otras palabras, vamos a aplicar \\(\\log()\\) a ambos lados del modelo exponencial multiplicativo \\(Y = \\beta_0 e^{\\beta_1X}\\varepsilon\\) y lo que obtiene es: \\[ Y^* = \\beta_0^* + \\beta_1 X + \\varepsilon^*, \\] donde el nuevo intercepto es \\(\\beta_0^*=\\log(\\beta_0)\\) y el nuevo error es \\(\\varepsilon^*=\\log(\\varepsilon)\\). El valor de \\(\\beta_1\\) es el mismo en el modelo original y en el modelo transformado. Al ajustar el modelo transformado vamos a obtener un intercepto estimado \\(\\hat{\\beta}_0^*\\) y una pendiente estimada \\(\\hat{\\beta}_1\\). Si queremos obtener \\(\\hat{\\beta}_0\\), debemos despejar de la ecuación \\(\\beta_0^*=\\log(\\beta_0)\\) el valor de \\(\\hat{\\beta}_0\\). Ajustemos el modelo m2. m2 &lt;- lm(log(visc) ~ temp, data=datos) Los estadísticos de resumen se obtiene así: summary(m2) ## ## Call: ## lm(formula = log(visc) ~ temp, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.093451 -0.017099 0.001985 0.017799 0.096605 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3656552 0.0098882 36.98 &lt;2e-16 *** ## temp -0.0199453 0.0001597 -124.87 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.03257 on 98 degrees of freedom ## Multiple R-squared: 0.9938, Adjusted R-squared: 0.9937 ## F-statistic: 1.559e+04 on 1 and 98 DF, p-value: &lt; 2.2e-16 De la tabla anterior se observa que: La temperatura es significativa porque valor-p es &lt; 2e-16. Por cada grado °C en que aumente la temperatura, la log(viscocidad) disminuye en 0.0199453 unidades. El \\(R^2\\) es alto, presenta un valor de 0.9938. El modelo m2 es significativo porque valor-p es &lt; 2.2e-16. Para ver los residuales del modelo se hace lo siguiente: par(mfrow=c(2, 2)) plot(m2, las=1, col=&#39;palegreen3&#39;, which=1:3, cex=1.5, pch=19) res_m2 &lt;- residuals(m2) shapiro.test(res_m2) ## ## Shapiro-Wilk normality test ## ## data: res_m2 ## W = 0.99063, p-value = 0.7151 Comparemos las líneas o modejos ajustados m1 y m2 con un diagrama de dispersión. par(mfrow=c(1, 2)) plot(x=datos$temp, y=datos$visc, pch=20, cex=0.8, xlab=&quot;Temperatura (°C)&quot;, ylab=&quot;Viscocidad&quot;, main=&quot;M1&quot;) abline(m1, col=&quot;purple&quot;, lwd=2) plot(x=datos$temp, y=log(datos$visc), pch=20, cex=0.8, xlab=&quot;Temperatura (°C)&quot;, ylab=&quot;log(Viscocidad)&quot;, main=&quot;M2&quot;) abline(m2, col=&quot;palegreen3&quot;, lwd=2) ¿Cuál modelo se debería usar? Escriba la ecuación del modelo. Usando los resultados de summary de m2 es posible escribir la ecuación ajustada para el modelo modelo transformado y para el modelo exponencial multiplicativo. Modelo transformado: \\[ \\widehat{Visco^*} = \\widehat{\\log(Visco)} = 0.3657 - 0.0199 \\, Temp, \\] Modelo exponencial multiplicativo: \\[ \\widehat{Visco} = 1.4415 \\, e^{- 0.0199 \\, Temp}, \\] donde \\(1.4415=\\exp(0.3657)\\). Con el modelo elegido, calcule \\(\\widehat{Visco}\\) para \\(Temp=0\\) y para \\(Temp=50\\). La cantidad \\(\\widehat{Visco} \\vert Temp=0\\) no se debe calcular. La cantidad \\(\\widehat{Visco} \\vert Temp=50\\) es \\(1.4415 \\, e^{- 0.0199 \\times 50} = 0.5330\\). visco_hat &lt;- function(x) 1.4415 * exp(-0.0199*x) plot(x=datos$temp, y=datos$visc, pch=20, cex=0.8, xlab=&quot;Temperatura (°C)&quot;, ylab=&quot;Viscocidad&quot;, main=&quot;Modelo final&quot;) curve(expr=visco_hat(x), add=TRUE, col=&quot;tomato&quot;) References Montgomery, E. &amp; Vining, D. &amp; Peck. 2006. Introducción Al Análisis de Regresión Lineal. 3ed ed. México: Cecsa. "],["rlm.html", "3 Regresión lineal múltiple Modelo estadístico", " 3 Regresión lineal múltiple En este capítulo se presenta una descripción breve del modelo de regresión lineal múltiple y la forma de estimar los parámetros del modelo con R. Modelo estadístico El modelo estadístico en regresión lineal múltiple es una generalización del regresión lineal simple para \\(k\\) covariables. El modelo en este caso se puede escribir de dos formas como se muestra a continuación. En esta forma la variable respuesta \\(Y\\) se expresa como una suma de \\(\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\cdots + \\beta_k X_{ki}\\) y un error aleatorio \\(e_i\\) el cual tiene distribución \\(N(0, \\sigma^2)\\). El modelo en esta forma se puede expresar como sigue. \\[\\begin{align} \\label{mod1} Y_i &amp;= \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\cdots + \\beta_k X_{ki} + \\epsilon_i, \\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2) \\end{align}\\] En esta forma la variable respuesta \\(y\\) tiene distribución normal con media que cambia en función de las variables \\(x_k\\) pero con varianza constante. El modelo en esta forma se puede expresar como sigue. \\[\\begin{align} \\label{mod2} Y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\cdots + \\beta_k X_{ki}, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] En cualquiera de las dos formas el vector de parámetros del modelo es \\(\\boldsymbol{\\theta}=(\\beta_0, \\beta_1, \\cdots, \\beta_k, \\sigma)^\\top\\). Ejemplo Como ilustración vamos a usar los datos del ejemplo 3.1 del libro de E. &amp;. V. Montgomery D. &amp; Peck (2006). En el ejemplo 3.1 los autores ajustaron un modelo de regresión lineal múltiple para explicar el Tiempo necesario para que un trabajador haga el mantenimiento y surta una máquina dispensadora de refrescos en función de las variables Número de Cajas y Distancia. Los datos del ejemplo están disponibles en el paquete MPV (por los apellidos de los autores). A continuación el código para cargar los datos y una muestra de las 6 primeras observaciones de la base de datos, en total se disponen de 20 observaciones. library(MPV) colnames(softdrink) &lt;- c(&quot;tiempo&quot;, &quot;cantidad&quot;, &quot;distancia&quot;) head(softdrink) ## tiempo cantidad distancia ## 1 16.68 7 560 ## 2 11.50 3 220 ## 3 12.03 3 340 ## 4 14.88 4 80 ## 5 13.75 6 150 ## 6 18.11 7 330 Un gráfico esencial para explorar las relaciones entre las variables es el gráfico de dispersión por pares de variables. library(&quot;ggplot2&quot;) library(&quot;GGally&quot;) ggpairs(softdrink) Un gráfico en 3d es obligratorio para explorar la relación entre las variables, este diagrama de puede obtener usando el paquete scatterplot3d. A continuación el código para construirlo. library(scatterplot3d) attach(softdrink) scatterplot3d(x=cantidad, y=distancia, z=tiempo, pch=16, cex.lab=1, highlight.3d=TRUE, type=&quot;h&quot;, xlab=&quot;Cantidad de cajas&quot;, ylab=&quot;Distancia (pies)&quot;, zlab=&quot;Tiempo (min)&quot;) De la figura anterior se ve claramente que a medida que aumenta el número de cajas y la distancia los tiempos tienden a ser mayores. El mismo diagrama de dispersión anterior se puede crear usando el paquete plotly. El lector puede jugar con el diagrama, puede moverlo, girarlo, acercarse y muchas cosas más, la curiosidad le mostrará las diferentes posibilidades. library(plotly) plot_ly(x=cantidad, y=distancia, z=tiempo, type=&quot;scatter3d&quot;, color=tiempo) %&gt;% layout(scene = list(xaxis = list(title = &quot;Cantidad&quot;), yaxis = list(title = &quot;Distancia (pies)&quot;), zaxis = list(title = &quot;Tiempo (min)&quot;))) Otro gráfico de dispersión en 3d se puede construir usando el paquete rgl. A continuación está el código para obtener el diagrama de dispersión. De tarea se deja que el lector copie el código en la consola y reconstruya el gráfico. library(rgl) plot3d(x=cantidad, y=distancia, z=tiempo, type=&quot;s&quot;, col=&quot;pink&quot;, xlab=&quot;Cantidad&quot;, ylab=&quot;Distancia (pies)&quot;, zlab=&quot;Tiempo (min)&quot;) Basándonos en el diagrama de dispersión 3d, el modelo que se va a ajustar se muestra a continuación. \\[\\begin{align} Tiempo_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 Cantidad_i + \\beta_2 Distancia_i, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] Para ajustar (entrenar) el modelo anterior podemos usar la función lm como se muestra a continuación. mod &lt;- lm(tiempo ~ cantidad + distancia, data=softdrink) Luego de ajustar el modelo se debe solicitar la tabla de resumen del modelo y esto se puede hacer de la siguiente manera. summary(mod) ## ## Call: ## lm(formula = tiempo ~ cantidad + distancia, data = softdrink) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7880 -0.6629 0.4364 1.1566 7.4197 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.341231 1.096730 2.135 0.044170 * ## cantidad 1.615907 0.170735 9.464 3.25e-09 *** ## distancia 0.014385 0.003613 3.981 0.000631 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.259 on 22 degrees of freedom ## Multiple R-squared: 0.9596, Adjusted R-squared: 0.9559 ## F-statistic: 261.2 on 2 and 22 DF, p-value: 4.687e-16 Con los resultados anteriores se puede expresar el modelo ajustado como se muestra a continuación. \\[\\begin{align} \\widehat{Tiempo}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2), \\\\ \\hat{\\mu}_i &amp;= 2.341 + 1.616 \\, Cantidad_i + 0.014 \\, Distancia_i, \\\\ \\hat{\\sigma} &amp;= 3.259 \\end{align}\\] ¿Cómo se pueden interpretar los efectos \\(\\hat{\\beta}\\)? Si el camión queda un pie más lejos (30.48 cm) de la máquina, se espera que el tiempo promedio de mantenimineto aumente en 0.014 minutos. Si el camión queda 100 pies más lejos (30.48 mt) de la máquina, se espera que el tiempo promedio de mantenimiento aumente en 1.4 minutos. Por cada caja adicional de refresco que se deba llevar, se espera que el tiempo promedio aumente en 1.616 minutos. Si el camión quedó a 0 pies de distancia y no hay que llevar cajas de refresco, se espera que el tiempo promedio de mantenimiento sea de 2.341 minutos. La interpretación de cada \\(\\hat{\\beta}\\) se hace suponiendo que las demás variables quedan constantes en algún valor. Para incluir el plano de regresión que representa el modelo ajustado anterior se puede usar el siguiente código. # Se crea el grafico 3d y se guarda en un objeto, por ejemplo mi_3d mi_3d &lt;- scatterplot3d(x=cantidad, y=distancia, z=tiempo, pch=16, cex.lab=1, highlight.3d=TRUE, type=&quot;h&quot;, xlab=&quot;Cantidad de cajas&quot;, ylab=&quot;Distancia (pies)&quot;, zlab=&quot;Tiempo (min)&quot;) # Para agregar el plano usamos $plane3d( ) con argumento modelo ajustado mi_3d$plane3d(mod, lty.box=&quot;solid&quot;, col=&quot;mediumblue&quot;) References Montgomery, E. &amp; Vining, D. &amp; Peck. 2006. Introducción Al Análisis de Regresión Lineal. 3ed ed. México: Cecsa. "],["predict.html", "4 Predicción Función predict Intervalo de confianza para la respuesta media \\(E(Y|x_0)\\) Intervalo de confianza para la predicción de nuevas observaciones", " 4 Predicción En este capítulo se presenta una descripción breve de como realizar predicciones a partir de un modelo de regresión lineal. Función predict La función predict es una función genérica de clase S3 que se puede aplicar a un modelo ajustado para obtener los valores de \\(\\hat{y}\\). Abajo se muestra la estructura de la función predict con la lista de sus argumentos. predict.lm(object, newdata, se.fit = FALSE, scale = NULL, df = Inf, interval = c(&quot;none&quot;, &quot;confidence&quot;, &quot;prediction&quot;), level = 0.95, type = c(&quot;response&quot;, &quot;terms&quot;), terms = NULL, na.action = na.pass, pred.var = res.var/weights, weights = 1, ...) Ejemplo Suponga que queremos ajustar un modelo de regresión para explicar el número de trabajadores empleados (Employed) en función de las covariables Unemployed, Armed.Forces y Year del conjunto de datos longley. Luego de ajustar el modelo queremos predecir el valor de \\(E(Employed|\\boldsymbol{X}=\\boldsymbol{x}_0)\\) en dos situaciones: Año 1963 con 420 desempleados y 270 personas en fuerzas armadas. Año 1964 con 430 desempleados y 250 personas en fuerzas armadas. Solución Primero exploremos los datos. library(dplyr) longley |&gt; select(Employed, Unemployed, Armed.Forces, Year) -&gt; datos plot(datos, pch=20) Luego lo que se debe hacer es ajustar el modelo así. mod &lt;- lm(Employed ~ Unemployed + Armed.Forces + Year, data=longley) library(broom) tidy(mod, quick=TRUE) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1797. 68.6 -26.2 5.89e-12 ## 2 Unemployed -0.0147 0.00167 -8.79 1.41e- 6 ## 3 Armed.Forces -0.00772 0.00184 -4.20 1.22e- 3 ## 4 Year 0.956 0.0355 26.9 4.24e-12 De la tabla anterior tenemos que el modelo ajustado es el siguiente: \\[ \\widehat{\\text{Employed}} = -1797.22 - 0.01 \\, \\text{Unemployed} - 0.01 \\, \\text{Armed.Forces} + 0.96 \\, \\text{Year} \\] Podríamos usar la expresión anterior y reemplazar los valores de año, desempleados y personas en fuerzas armadas, dadas arriba, para calcular \\(E(Employed|\\boldsymbol{X}=\\boldsymbol{x}_0)\\). Sin embargo, aquí lo vamos a realizar usando la función predict. Ahora vamos construir un nuevo marco de datos con la información de las covariables, usando los mismos nombres y los mismos tipos de variables (cuali o cuanti) que en el conjunto de datos con el cual se entrenó el modelo. nuevo &lt;- data.frame(Year=c(1963, 1964), Unemployed=c(420, 430), Armed.Forces=c(270, 250)) nuevo ## Year Unemployed Armed.Forces ## 1 1963 420 270 ## 2 1964 430 250 Ahora ya podemos usar la función predict para obtener lo solicitado. predict(object=mod, newdata=nuevo) ## 1 2 ## 71.89467 72.85853 De la salida anterior tenemos los valores de \\(\\widehat{Employed}\\). ¿Qué sucede si en el nuevo marco de datos anterior las variables se llaman diferente a las variables conjunto de datos de entrenamiento? Error, nos sale un error. No se le ocurra usar nombres diferentes. Intervalo de confianza para la respuesta media \\(E(Y|x_0)\\) En regresión lineal simple, Si \\(\\hat{\\mu}_{Y|x_0}\\) es la media estimada para la variable respuesta cuando \\(X=x_0\\), entonces un IC del \\((1−\\alpha⁄2)\\)×100% para \\(E(Y|x_0)\\) está dado por: \\[ \\hat{\\mu}_{Y|x_0} \\pm t_{\\alpha / 2, n - p} \\, \\sqrt{MS_{Res} \\left(\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2} \\right)} \\] Intervalo de confianza para la predicción de nuevas observaciones En regresión lineal simple, si \\(\\hat{Y}_0\\) es el valor estimado para la variable respuesta cuando \\(X=x_0\\), entonces un IC del \\((1−\\alpha⁄2)\\)×100% para \\(Y_0\\) está dado por: \\[ \\hat{Y}_0 \\pm t_{\\alpha / 2, n - p} \\, \\sqrt{MS_{Res} \\left(1 +\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2} \\right)} \\] Ejemplo Como ilustración vamos a usar los datos del ejemplo 2.1 del libro de Montgomery, Peck and Vining (2003). En el ejemplo 2.1 los autores ajustan un modelo de regresión lineal simple para explicar la Resistencia de una soldadura en función de la Edad de la soldadura. El objetivo es obtener: IC del 95% para \\(E(Y|x_0)\\) cuando \\(x_0=13\\) semanas. IC del 95% para \\(E(Y|x_0)\\) cuando \\(x_0=2\\) semanas. IC del 90% para \\(\\hat{Y}_0\\) cuando \\(x_0=10\\) semanas. Crear el diagrama de dispersión agregando las líneas de los IC de 95% para \\(E(Y|x_0)\\) y \\(\\hat{Y}_0\\). Solución Lo primero es disponer los datos. file &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/propelente&quot; datos &lt;- read.table(file=file, header=TRUE) head(datos) ## Resistencia Edad ## 1 2158.70 15.50 ## 2 1678.15 23.75 ## 3 2316.00 8.00 ## 4 2061.30 17.00 ## 5 2207.50 5.50 ## 6 1708.30 19.00 Luego se ajusta el modelo. mod1 &lt;- lm(Resistencia ~ Edad, data=datos) Ahora vamos a explorar la tabla de resumen del modelo anterior. summary(mod1) ## ## Call: ## lm(formula = Resistencia ~ Edad, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -215.98 -50.68 28.74 66.61 106.76 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2627.822 44.184 59.48 &lt; 2e-16 *** ## Edad -37.154 2.889 -12.86 1.64e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 96.11 on 18 degrees of freedom ## Multiple R-squared: 0.9018, Adjusted R-squared: 0.8964 ## F-statistic: 165.4 on 1 and 18 DF, p-value: 1.643e-10 En la tabla anterior está toda la información para hacer las estimaciones manualmente pero aquí lo vamos a realizar de forma computacional. Para obtener el IC del 95% para \\(E(Y|x_0)\\) para \\(x_0=13\\) y \\(x_0=2\\) se usa el siguiente código. new_dt &lt;- data.frame(Edad=c(13, 2)) predict(object=mod1, newdata=new_dt, interval=&quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 2144.826 2099.623 2190.028 ## 2 2553.515 2471.083 2635.947 Para obtener el IC del 90% para \\(\\hat{Y}_0\\) cuando \\(x_0=10\\) semanas se usa el siguiente código. new_dt &lt;- data.frame(Edad=10) predict(object=mod1, newdata=new_dt, interval=&quot;prediction&quot;, level=0.90) ## fit lwr upr ## 1 2256.286 2084.688 2427.885 Observe que en el primer caso se usó interval=\"confidence\" mientras que en el segundo se usó interval=\"prediction\". Ahora vamos a obtener todos los IC \\(\\hat{Y}_0\\) y los vamos a almacenar en el objeto future_y que luego luego vamos a agregar al marco de datos original. future_y &lt;- predict(object=mod1, interval=&quot;prediction&quot;, level=0.95) nuevos_datos &lt;- cbind(datos, future_y) Con el código mostrado a continuación se construye el diagrama de dispersión y se agrega la línea de regresión (en azul) y los IC para \\(E(Y|x_0)\\) (en rosado) por medio de geom_smooth. Los IC para \\(\\hat{Y}_0\\) (en rojo) se agregan por medio de geom_line. library(ggplot2) ggplot(nuevos_datos, aes(x=Edad, y=Resistencia))+ geom_point() + geom_line(aes(y=lwr), color=&quot;red&quot;, linetype=&quot;dashed&quot;) + geom_line(aes(y=upr), color=&quot;red&quot;, linetype=&quot;dashed&quot;) + geom_smooth(method=lm, formula=y~x, se=TRUE, level=0.95, col=&#39;blue&#39;, fill=&#39;pink2&#39;) + theme_light() De la figura anterior se observa claramente que los IC para \\(\\hat{Y}_0\\) son siempre más anchos que los IC para \\(E(Y|x_0)\\). "],["simul.html", "5 Simulación Preguntas sobre simulación en StackOverFlow Simulando datos de un modelo lineal Función simulate Otras herramientas para simular Videos sugeridos", " 5 Simulación En este capítulo se muestra como simular datos que sigan un modelo o estructura dada. Aprender a simular datos es de mucha utilidad para comprender la naturaleza de los datos y para poner a prueba algunos procedimientos de modelos de regresión. Preguntas sobre simulación en StackOverFlow Una de las preguntas más frecuente que hacen los usuarios en StackOverFlow es: “How to simulate linear models in R?”. Muchas respuestas se han dado para ayudar a esos usuarios que necesitan simular datos con una estructura dada. A continuación se muestran algunas de esas preguntas/respuestas hechas en StackOverFlow, invito a lector para que visite los enlaces y explore las respuestas dadas. How to generate observations from a linear model in R. How to generate random Y at specific X from a linear model in R?. Simulate data in R from a linear model where the parameters are correlated. Create Simulated Data Multiple Regression in R. Simulate data from regression model with exact parameters in R. Las respuestas a cómo simular datos de un modelo lineal son muy variadas, algunas sencillas, otras un poco más complejas, y por esa razón en la siguiente sección se mostrará como simular datos de una forma didáctica. Simulando datos de un modelo lineal Para simular de forma exitosa datos de un modelo lineal, se recomienda tener el modelo escrito en forma simbólica ya que eso facilita identificar la variable respuesta, su distribución y los parámetros que van a depender de las variables independientes. A continuación se muestra un ejemplo de como simular observaciones de un modelo lineal. Ejemplo Crear una función que simule n observaciones del siguiente modelo que tiene el vector de parámetros \\(\\boldsymbol{\\Theta}=(\\beta_0=4, \\beta_1=-6, \\sigma=4)^\\top\\). \\[\\begin{align*} y_i &amp;\\sim N(\\mu_i, \\sigma^2) \\\\ \\mu_i &amp;= 4 - 6 x_i \\\\ x_i &amp;\\sim U(-5, 6) \\\\ \\sigma^2 &amp;= 16 \\end{align*}\\] Solución Todo modelo en lenguaje simbólico se lee de arriba hacia abajo \\(\\Downarrow\\) pero al escribirlo en R se hace de abajo hacia arriba \\(\\Uparrow\\). A continuación se muestra la función solicitada la cual sólo tiene un argumento y que entrega como resultado un marco de datos con la información. Estimado lector, mire con detalle el modelo simbólico e identifique todos esos elementos en el código de abajo. gen_dat &lt;- function(n) { varianza &lt;- 16 x &lt;- runif(n=n, min=-5, max=6) media &lt;- 4 - 6 * x y &lt;- rnorm(n=n, mean=media, sd=sqrt(varianza)) marco_datos &lt;- data.frame(y=y, x=x) return(marco_datos) } Vamos a poner a prueba la función para simular 5 observaciones de la siguiente manera. datos &lt;- gen_dat(n=5) datos ## y x ## 1 -1.950118 1.4745156 ## 2 -30.885322 4.8185096 ## 3 27.385065 -3.6382560 ## 4 2.167666 0.6463992 ## 5 -12.361785 2.3000424 Vamos ahora a simular 200 observaciones, a dibujar un diagrama de dispersión y a estimar el vector de parámetros \\(\\boldsymbol{\\Theta}=(\\beta_0=4, \\beta_1=-6, \\sigma=4)^\\top\\). datos &lt;- gen_dat(n=200) library(ggplot2) ggplot(datos, aes(x=x, y=y)) + geom_point() + theme_light() De la figura anterior se observa que la nube tiene pendiente negativa y eso se debe a \\(\\beta_1=-6\\); la nube tiene la misma dispersión y eso de debe a que \\(\\sigma\\) es constante e igual a 4. Para estimar \\(\\boldsymbol{\\Theta}=(\\beta_0=4, \\beta_1=-6, \\sigma=4)^\\top\\) podemos usar el siguiente código. mod &lt;- lm(y ~ x, data=datos) theta_hat &lt;- c(coef(mod), sigma=summary(mod)$sigma) theta_hat ## (Intercept) x sigma ## 3.966018 -6.080078 3.941736 De la salida anterior vemos que el vector estimado \\(\\hat{\\boldsymbol{\\Theta}}\\) está muy cerca del vector de parámetros \\(\\boldsymbol{\\Theta}\\) El código usado en este ejemplo para simular datos de un modelo lineal es un código sencillo y didáctico. Este código busca que el lector aprenda a escribir de una forma sencilla las relaciones entre la variable respuesta, las variables independientes y la influencia de los parámetros. Este código NO es un código óptimo desde el punto de vista computacional. Muy seguramente usa muchos recursos de memoria y puede demorar un poco para simular datos. Sin embargo, es un código FÁCIL de entender. Función simulate La función simulate del paquete básico stats permite simular respuestas a partir de un modelo de clase lm o glm. La estructura de la función es la siguiente: simulate(object, nsim = 1, seed = NULL, ...) Ejemplo Use las 5 primeras observaciones base de datos cars y con esos ajuste un modelo lineal para explicar la distancia promedio para detener el vehículo en función de la velocidad a la cual estaba el vehículo cuando se presionaron los frenos. Luego use ese modelo para simular distancias de frenado. Solución datos &lt;- cars[1:5, ] mod &lt;- lm(dist ~ speed, data=datos) simulate(object=mod, nsim=1, seed=1234) ## sim_1 ## 1 -3.769550 ## 2 8.175135 ## 3 21.954305 ## 4 -5.645603 ## 5 19.110007 Otras herramientas para simular Abajo una lista de herramientas para simular datos a partir de un modelo. Paquete simglm: Simulate Models Based on the Generalized Linear Model. Paquete simrel: Simulation of Multivariate Linear Model Data. Make your R simulation models 20 times faster. Videos sugeridos A continuación se muestran algunos videos que muestran cómo simular datos de un modelo lineal usando R. "],["ic.html", "6 Intervalos de confianza Función confint Función confint_sigma2", " 6 Intervalos de confianza En este capítulo se muestra como construir intervalos de confianza para los parámetros \\(\\beta\\) y \\(\\sigma\\) de un modelo de regresión lineal. Función confint La función confint de R se usa para obtener intevalos de confianza de los parámetros de un modelo de regresión lineal, la estructura de esta función se muestra a continuación. confint(object, parm, level = 0.95, ...) A continuación se presenta una corta descripción de los parámetros de la función. object: es un objeto con el modelo ajustado. parm: es un vector con los nombres de las variables para los cuales nos interesa construir el intervalo de confianza. Si no se especifica ninguna variable se obtienen intervalos de confianza para todos los parámetros. level: nivel de confianza. Ejemplo Como ilustración vamos a usar los datos del ejemplo mostrado en el capítulo 1. En ese ejemplo se ajustó un modelo de regresión lineal simple para explicar la Resistencia de una soldadura en función de la Edad de la misma. El objetivo es construir un intervalo de confianza del 95% para el parámetro \\(\\beta_1\\). Solución A continuación el código para cargar los datos y ajustar el modelo de interés. file &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/propelente&quot; datos &lt;- read.table(file=file, header=TRUE) mod1 &lt;- lm(Resistencia ~ Edad, data=datos) El objetivo ahora es construir un intervalo de confianza del 95% para el parámetro \\(\\beta_1\\). La instrucción para obtener el intervalo de confianza se muestra a continuación. confint(object=mod1, parm=&quot;Edad&quot;, level=0.95) ## 2.5 % 97.5 % ## Edad -43.22338 -31.0838 Función confint_sigma2 La función confint_sigma2 pertenece al paquete model y sirve para obtener un intervalo de confianza para el parámetro \\(\\sigma^2\\). El paquete model (Hernandez and Usuga 2024) está alojado en github y para poder instalarlo se sebe usar el siguiente código. if (!require(&quot;remotes&quot;)) install.packages(&quot;remotes&quot;) remotes::install_github(&quot;fhernanb/model&quot;) Ejemplo Considere el ejemplo anterior pero ahora el interés es obtener un intervalo de confinaza del 95% para \\(\\sigma^2\\). Solución Si deseamos construir un intervalo de confianza del 95% para \\(\\sigma^2\\) del ejemplo anterior se debe escribir el siguiente código. library(model) confint_sigma2(object=mod1, level=0.95) ## 2.5 % 97.5 % ## Sigma2 5273.516 20199.24 References Hernandez, Freddy, and Olga Usuga. 2024. Model: This Package Contains Useful Functions for Modeling Regresion. https://fhernanb.github.io/model. "],["varcuali.html", "7 Modelos con variables cualitativas ¿Es posible incluir variables cualitativas? Variable indicadora, dummy, ficticia o binaria Creando variables indicadoras Modelos con variables cualitativas Significancia de variables cualitativas", " 7 Modelos con variables cualitativas En este capítulo se muestra cómo incluir variables cualitativas en un modelo de regresión con R. ¿Es posible incluir variables cualitativas? Una de las preguntas frecuentes entre los que inician el estudio de los modelos de regresión es: ¿se pueden incluir variables cualitativas en un modelo de regresión? La respuesta es SI, definitivamente si. A continuación una figura que ilustra algunas de las variables que se pueden incluir en la construcción de un modelo de regresión. Figure 7.1: Variables variables cualitativas a incluir en un modelo. Variable indicadora, dummy, ficticia o binaria La palabra indicadora, dummy, ficticia o binaria es la denominación genérica para una variable que toma valores de 0 o de 1 y que se utiliza para re-expresar variables cualitativas. Observe la figura 7.2. En la parte izquierda se tiene una base original de ejemplo con las variables precio, área y piscina, asociadas a seis apartamentos. La variable cualitativa Piscina de niveles sin, pequeña y grande, el nivel sin es el nivel de referencia natural. Al lado derecho de la figura 7.2 está la base transformada y vemos que hay 4 variables. Las nuevas variables PisPeq y PisGra son 2 variables indicadoras que logran resumir la información de la variable Piscina que tiene 3 variables. Para comprender como las 2 variables indicadoras pueden resumir la información de la variable Piscina, vamos a considerar los siguientes tres casos: Si PisPeq = 0 y PisGra = 0, entonces el apartamento está SIN piscina. Si PisPeq = 1 y PisGra = 0, entonces el apartamento tiene piscina PEQUEÑA. Si PisPeq = 0 y PisGra = 1, entonces el apartamento tiene piscina GRANDE. Figure 7.2: Transformando una base de datos con variables cualitativas. Como regla general, si una variable cualitativa tiene \\(k\\) niveles, se necesitarán de \\(k-1\\) variables indicadoras para resumir la variable cualitativa. Creando variables indicadoras Crear manualmente variables indicadoras para re-expresar variables cualitativas es una tarea muy sencilla, manualmente la podemos hacer. Sin embargo, R posee una herramienta que nos permite convertir la base de datos original en una base de datos transformada (con variables indicadoras). Para este fin se usa la función model.matrix. Ejemplo Retomando la base de datos original mostrada en la figura 7.2 vamos a crear la matriz de diseño \\(\\boldsymbol{X}\\) para ajustar el modelo siguiente: \\[\\begin{align} Precio_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 Area_i + \\beta_2 PisciPequena_i + \\beta_3 PisiGrande_i, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] Lo primero a realizar es ingresar los datos para el ejemplo mostrado en la figura 7.2. El código necesario se muestra a continuación. Precio &lt;- c(12, 15, 25, 11, 16, 7) Area &lt;- c(3, 4, 1, 6, 5, 3) Pisci &lt;- factor(x=c(&#39;Grande&#39;, &#39;Sin&#39;, &#39;Pequena&#39;, &#39;Pequena&#39;, &#39;Sin&#39;, &#39;Grande&#39;), levels=c(&#39;Sin&#39;,&#39;Pequena&#39;,&#39;Grande&#39;)) Al crear el vector Pisci se usó el argumento levels dentro de la función factor para indicarle a R que el nivel de referencia es Sin, seguido de Pequena y luego Grande. Para obtener la matriz \\(\\boldsymbol{X}\\) con las variables indicadoras (no con la variable original Pisci) se hace lo siguiente: model.matrix(Precio ~ Area + Pisci) ## (Intercept) Area PisciPequena PisciGrande ## 1 1 3 0 1 ## 2 1 4 0 0 ## 3 1 1 1 0 ## 4 1 6 1 0 ## 5 1 5 0 0 ## 6 1 3 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 2 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$Pisci ## [1] &quot;contr.treatment&quot; Modelos con variables cualitativas Para ajustar un modelo de regresión lineal con variables cualitativas se procede de la forma usual como se ajustan modelos con lm, no es necesario crear de antemano la matriz \\(\\boldsymbol{X}\\), esto porque lm internamente crea la matriz de diseño \\(\\boldsymbol{X}\\). Ejemplo En este ejemplo vamos a utilizar la base de datos Cars93 del paquete MASS. El objetivo es ajustar el siguiente modelo para explicar el precio del auto en función del tamaño del motor y del tipo de auto, es decir, el objetivo es ajustar el siguiente modelo. \\[\\begin{align} Precio_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 EngSize_i + \\beta_2 TypeC_i + \\beta_3 TypeS_i + \\beta_4 TypeM_i + \\beta_5 TypeL_i + \\beta_6 TypeV_i, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] Solución Lo primero a realizar es cargar el paquete y explorar las variables de interés con la ayuda de la función str. require(MASS) str(Cars93[, c(&#39;Price&#39;, &#39;EngineSize&#39;, &#39;Type&#39;)]) ## &#39;data.frame&#39;: 93 obs. of 3 variables: ## $ Price : num 15.9 33.9 29.1 37.7 30 15.7 20.8 23.7 26.3 34.7 ... ## $ EngineSize: num 1.8 3.2 2.8 2.8 3.5 2.2 3.8 5.7 3.8 4.9 ... ## $ Type : Factor w/ 6 levels &quot;Compact&quot;,&quot;Large&quot;,..: 4 3 1 3 3 3 2 2 3 2 ... La variable Type tiene 6 niveles, para ver todos niveles usamos el siguiente código. levels(Cars93$Type) ## [1] &quot;Compact&quot; &quot;Large&quot; &quot;Midsize&quot; &quot;Small&quot; &quot;Sporty&quot; &quot;Van&quot; De la salida anterior se observan que los niveles son Compact, Large, Midsize, Small, Sporty y Van. Al observar cuidadosamente los niveles vemos que ellos están ordenados por orden lexicográfico, primero Compact por iniciar con la letra C, por último Van por iniciar con la letra V. Al mirar el modelo requerido se nota que el nivel Small no aparece en la ecuación de \\(\\mu\\), esto significa que ese es el nivel de referencia que se encuentra en el intercepto \\(\\beta_0\\). Para redefinir los niveles en el orden requerido usamos el siguiente código. Cars93$Type &lt;- relevel(Cars93$Type, ref = &#39;Small&#39;) levels(Cars93$Type) # Para verificar el cambio ## [1] &quot;Small&quot; &quot;Compact&quot; &quot;Large&quot; &quot;Midsize&quot; &quot;Sporty&quot; &quot;Van&quot; A continuación vamos a crear un diagrama de dispersión para ver la relación entre las variables del problema. library(ggplot2) ggplot(Cars93, aes(x=EngineSize, y=Price, color=Type)) + geom_point() + theme_light() Ahora si podemos ajustar el modelo solicitado usando el siguiente código. mod &lt;- lm(Price ~ EngineSize + Type, data=Cars93) summary(mod) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.794 2.348 1.190 0.23732 EngineSize 4.621 1.110 4.164 7.4e-05 *** TypeCompact 4.644 2.484 1.870 0.06489 . TypeLarge 2.053 3.916 0.524 0.60138 TypeMidsize 10.286 2.700 3.810 0.00026 *** TypeSporty 5.078 2.634 1.928 0.05721 . TypeVan 1.517 3.332 0.455 0.65006 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Usando la información de la salida anterior se puede construir el siguiente modelo ajustado. \\[\\begin{align} \\widehat{Precio}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2), \\\\ \\hat{\\mu}_i &amp;= 2.794 + 4.621 EngSize_i + 4.644 TypeC_i + 5.078 TypeS_i + 10.286 TypeM_i + \\ldots \\\\ \\hat{\\sigma} &amp;= 7.068 \\end{align}\\] ¿Cómo se interpretan los coeficientes? Para cada tipo de auto, si el tamaño del motor se pudiera aumentar en 1 litro, se espera que el precio promedio aumente en 4.621 miles de dólares. Si tenemos dos autos, uno small y otro compacto, ambos con el mismo tamaño del motor, se espera que el precio promedio del compacto sea 4.644 miles de dólares mayor con respecto al auto small. Si comparamos un auto small y uno midsize, ambos con el mismo tamaño del motor, es de esperarse que el precio promedio del auto midsize sea 10.286 miles de dólares más que el small. Dos autos con el mismo tamaño del motos, uno de tipo sporty y otro de tipo large, se espera que el de tipo sporty tenga un precio promedio de 3.025 miles de dólares más que el large (valor obtenido de 5.078-2.053). Cuidado, no intente interpretar \\(\\hat{\\beta}_0\\) en este ejemplo. ¿Por qué? Significancia de variables cualitativas Una pregunta frecuente entre los usuarios es ¿cómo saber si una variable cualitativa es significativa para un modelo? Cuando se incluye una variable cualitativa de \\(k\\) niveles en un modelo de regresión, aparecen \\(k-1\\) variables indicadoras y por lo tanto \\(k-1\\) valores-P en la tabla resumen. Usar esos valores-P nos puede llevar a conclusiones erróneas. Para saber si una variable cualitativa es significativa para un modelo hay dos formas: Crear una anova y ver si la variable cualitativa es significativa en el modelo es decir, usando anova(mod). Crear dos modelos, uno reducido sin la variable cualitativa y otro completo con la variable cualitativa, luego usar un análisis de varianza, anova(mod.redu, mod). Ejemplo En este ejemplo vamos a retomar los datos del ejemplo anterior en el cual se usa la base de datos Cars93 del paquete MASS. El objetivo es ajustar un modelo para explicar el precio del auto en función del tamaño del motor y del tipo de auto. Supongamos el tamaño del motor está presente en el modelo, ¿será la variable tipo de auto significativa para el modelo? Solución Forma 1 En la forma 1 debemos ajustar el modelo y usar la función anova para ver si la variable cualitativa es significativa en el modelo. Al usar la función anova sobre un modelo mod obtenido con la función lm, aparecerán tantas filas (con valor-P) como número de variables tenga el modelo ajustado. El conjunto de hipótesis para cada una de las filas es: \\[\\begin{align} H_0 &amp;: \\text{la variable de la FILA no aporta información para el modelo}, \\\\ H_A &amp;: \\text{la variable de la FILA si aporta información para el modelo} \\end{align}\\] A continuación el código para usar la forma 1. require(MASS) data(&quot;Cars93&quot;) mod &lt;- lm(Price ~ EngineSize + Type, data=Cars93) anova(mod) ## Analysis of Variance Table ## ## Response: Price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## EngineSize 1 3063.8 3063.78 61.3270 1.16e-11 *** ## Type 5 1223.8 244.77 4.8994 0.000542 *** ## Residuals 86 4296.4 49.96 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 De la salida anterior se observa que el valor-P asociado a Type es de 0.000542, usando un nivel de significancia usual del 5% se concluye que hay evidencias para rechazar \\(H_0\\), es decir, la variable Type si aporta información al modelo. Forma 2 En la forma 2 debemos crear dos modelos, uno reducido sin la variable cualitativa y otro completo con la variable cualitativa, luego usar un análisis de varianza, anova(mod_redu, mod_comp). Al usar la función anova el conjunto de hipótesis es: \\[\\begin{align} H_0 &amp;: \\text{la variable Type no aporta información para el modelo}, \\\\ H_A &amp;: \\text{la variable Type si aporta información para el modelo} \\end{align}\\] A continuación el código para usar la forma 2. El modelo mod_redu contiene un modelo sin la variable cualitativa de interés Type, mientras que el modelo mod_comp si la contiene. require(MASS) data(&quot;Cars93&quot;) mod_redu &lt;- lm(Price ~ EngineSize, data=Cars93) mod_comp &lt;- lm(Price ~ EngineSize + Type, data=Cars93) anova(mod_redu, mod_comp) ## Analysis of Variance Table ## ## Model 1: Price ~ EngineSize ## Model 2: Price ~ EngineSize + Type ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 91 5520.2 ## 2 86 4296.4 5 1223.8 4.8994 0.000542 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 De la salida anterior se tiene un valor-P de 0.000542, usando un nivel de significancia usual del 5% se concluye que hay evidencias para rechazar \\(H_0\\), es decir, la variable Type si aporta información para el modelo y por lo tanto es una variable útil. La decisión final con las formas 1 y 3 siempre coinciden. Algunas veces una variable cualitativa puede ser importante en el modelo pero los valores-P asociados a sus variables indicadoras (auxiliares) son todos mayores al valor \\(\\alpha\\), el resultado del summary puede ser engañoso. A seguir un ejemplo claro de esta situación. Ejemplo El ejemplo aquí mostrado está basado en una pregunta de StackOverFlow. El ejemplo consiste en simular un conjunto de 30 valores de \\(y \\sim N(\\mu, 1)\\), donde las observaciones 1 a 10 tienen \\(\\mu=0\\), las observaciones 11 a 20 tienen \\(\\mu=-0.5\\) y las restantes diez tienen \\(\\mu=0.5\\). Para diferenciar las observaciones se tendrá la variable de agruación cualitativa g que contendrá las letras A, B y C diez veces cada una. El código para simular los datos se muestra a continuación. set.seed(8867) # this makes the example exactly reproducible y &lt;- c(rnorm(10, mean=0, sd=1), rnorm(10, mean=-0.5, sd=1), rnorm(10, mean=0.5, sd=1)) g &lt;- rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each=10) ¿Será la variable cualitativa g significativa en un modelo de regresión? Solución Vamos a ajustar el modelo con fórmula y ~ g para estudiar el efecto de la agrupación g en la media de la variable respuesta y. model &lt;- lm(y ~ g) Obviamente esperamos concluir que la media de la variable y dependa de la variable de agrupación g. Para esto vamos a explorar el resultado con la función summary. summary(model) ## ## Call: ## lm(formula = y ~ g) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.59080 -0.54685 0.04124 0.79890 2.56064 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.4440 0.3855 -1.152 0.260 ## gB -0.9016 0.5452 -1.654 0.110 ## gC 0.6729 0.5452 1.234 0.228 ## ## Residual standard error: 1.219 on 27 degrees of freedom ## Multiple R-squared: 0.2372, Adjusted R-squared: 0.1807 ## F-statistic: 4.199 on 2 and 27 DF, p-value: 0.02583 De la salida anterior vemos que los efectos gB y gC tienen valores-P altos, superiores al usual 5%, y por lo tanto estaríamos tentados a decir que la variable g no tiene efecto sobre la media de y. El lector podría encontrar esto un poco desconcertante. Vamos a realizar el análisis pero ahora con la función anova. anova(model) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## g 2 12.484 6.2418 4.199 0.02583 * ## Residuals 27 40.135 1.4865 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 En la fila donde aparece la variable g tenemos el resultado de la prueba de hipótesis \\[H_0: \\text{la variable g no influye en la media de y},\\] \\[H_A: \\text{la variable g si influye en la media de y}\\] El valor-P de esta prueba es de 0.02583, esto indica que hay evidencias para rechazar \\(H_0\\), es decir, encontramos que la variable g si influye sobre la media de la variable y. Cuando se quiera explorar el efecto de una variable cualitativa en un modelo es mejor usar la función anova que los resultados del summary. "],["ph1.html", "8 Pruebas de hipótesis parte I Pruebas sobre los coeficientes \\(\\beta\\) Función summary cuando \\(\\beta_{j0} = 0\\) Función beta_test cuando \\(\\beta_{k0} \\neq 0\\)", " 8 Pruebas de hipótesis parte I En este capítulo se muestra como realizar pruebas de hipótesis para un modelo de regresión lineal. La prueba explicada a continuación se conoce como prueba de Wald en honor a Abraham Wald (1902-1950). Pruebas sobre los coeficientes \\(\\beta\\) Cuando se tiene un modelo de regresión con \\(k\\) variables en la matriz de diseño \\(\\boldsymbol{X}\\) (la primera columna de \\(\\boldsymbol{X}\\) son unos y no se cuenta como variable), es usual que nos interese estudiar \\[H_0: \\beta_j = \\beta_{j0},\\] frente a una de las tres siguientes hipótesis alternas: \\[H_A: \\beta_j &lt; \\beta_{j0}, \\quad H_A: \\beta_j \\neq \\beta_{j0}, \\quad H_A: \\beta_j &gt; \\beta_{j0},\\] para algún \\(j = 0, 1, 2, \\ldots, k\\). Para estas pruebas el estadístico de prueba está dado por \\[ t_0 = \\frac{\\hat{\\beta}_j - \\beta_{j0}}{s.e.(\\hat{\\beta}_j)}, \\] y bajo la hipótesis nula cierta, \\(t_0 \\sim t_{n-k-1}\\). Cuando se realiza una prueba de hipótesis sobre uno de los coeficientes \\(\\beta\\), se asume que las variables restantes permanecen en el modelo, en otras palabras, esta es una prueba marginal. Función summary cuando \\(\\beta_{j0} = 0\\) Para realizar pruebas de hipótesis cuando el valor de referencia \\(\\beta_{j0}\\) es igual a cero se puede usar la función summary. Ejemplo Aquí vamos a retomar el ejemplo 2.1 del libro de Montgomery, Peck and Vining (2003). En el ejemplo 2.1 los autores ajustaron un modelo de regresión lineal simple para explicar la Resistencia de una soldadura en función de la Edad de la misma. ¿Será la variable Edad una variable significativa para el modelo? es decir, ¿será el coeficiente de la Edad igual a cero o no? Solución Las anteriores preguntas se pueden resumir por medio del siguiente conjunto de hipótesis. \\[H_0: \\beta_{Edad} = 0,\\] \\[H_A: \\beta_{Edad} \\neq 0\\] Para responder a esta pregunta vamos a ajustar el modelo de la forma usual y luego vamos a construir la tabla de resumen del modelo, el código para hacer esto es el siguiente. file &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/propelente&quot; datos &lt;- read.table(file=file, header=TRUE) mod &lt;- lm(Resistencia ~ Edad, data=datos) summary(mod) ## ## Call: ## lm(formula = Resistencia ~ Edad, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -215.98 -50.68 28.74 66.61 106.76 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2627.822 44.184 59.48 &lt; 2e-16 *** ## Edad -37.154 2.889 -12.86 1.64e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 96.11 on 18 degrees of freedom ## Multiple R-squared: 0.9018, Adjusted R-squared: 0.8964 ## F-statistic: 165.4 on 1 and 18 DF, p-value: 1.643e-10 De la tabla anterior tenemos que el valor-P asociado a Edad es 1.64e-10, por lo tanto a un nivel de significancia usual de 5%, hay evidencias para rechazar \\(H_0\\) y se concluye que la variable Edad si aporta información para predecir la media de la Resistencia. Función beta_test cuando \\(\\beta_{k0} \\neq 0\\) Para realizar pruebas de hipótesis cuando el valor de referencia \\(\\beta_{k0}\\) es diferente de cero, se puede usar la función beta_test del paquete model (Hernandez and Usuga 2024). Este paquete está alojado en github y para poder instalarlo se sebe usar el siguiente código. if (!require(&quot;remotes&quot;)) install.packages(&quot;remotes&quot;) remotes::install_github(&quot;fhernanb/model&quot;) La estructura de la función se muestra a continuación. beta_test(object, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), parm, ref.value) Los argumentos de esta función son: object: un objeto de la clase lm. alternative: una cadena de caracteres indicando el signo de la hipótesis alterna, los valores posibles son two.sided (valor por defecto), greater o less. parm: vector con el nombre de la variable. ref.value: valor de referencia \\(\\beta_{j0}\\) de la prueba. Ejemplo Aquí vamos a retomar el ejemplo 2.1 del libro de Montgomery, Peck and Vining (2003). En el ejemplo 2.1 los autores ajustaron un modelo de regresión lineal simple para explicar la Resistencia de una soldadura en función de la Edad de la misma. El proveedor de la soldadura afirma que la resistencia media para soldaduras nuevas es 2700 psi. Pruebe la hipótesis de que la resistencia media es diferente a un nivel de significancia del 5%. Solución La anterior pregunta se pueden resumir por medio del siguiente conjunto de hipótesis. \\[H_0: \\beta_{0} = 2700,\\] \\[H_A: \\beta_{0} \\neq 2700\\] Para responder a esta pregunta vamos usar la función beta_test. library(model) beta_test(object=mod, parm=&#39;(Intercept)&#39;, ref.value=2700, alternative=&#39;two.sided&#39;) ## Estimate Std.Err t value Pr(&gt;t) ## (Intercept) 2627.822 44.184 -1.6336 0.1197 Como el valor-P obtenido es 0.1197, entonces la resistencia media para soldaduras nuevas sigue siendo de 2700 psi, en otras palabras, no hay evidencias para rechazar \\(H_0\\), esto a un nivel de significancia del 5%. References Hernandez, Freddy, and Olga Usuga. 2024. Model: This Package Contains Useful Functions for Modeling Regresion. https://fhernanb.github.io/model. "],["ph2.html", "9 Pruebas de hipótesis parte II Prueba sobre todos los coeficientes Prueba para comparar modelos anidados Función anova summary versus anova Prueba razón de verosimilitud Comparaciones múltiples", " 9 Pruebas de hipótesis parte II En este capítulo se muestra como realizar pruebas de significancia en un modelo de regresión. Las pruebas aquí explicadas son las siguientes: Prueba sobre todos los coeficientes (prueba de significancia de la regresión). Prueba para comparar modelos anidados (prueba parcial F). Prueba sobre todos los coeficientes Supongamos que tenemos un modelo de regresión múltiple como se muestra a continuación. \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_k x_{ki}, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] Para este modelo nos podemos preguntar si alguna de las covariables aporta información al modelo o si ninguna aporta información al modelo. Esta duda se puede resumir simbólicamente por medio del siguiente conjunto de hipótesis. \\[\\begin{align} H_0 &amp;: \\beta_1=\\beta_2=\\ldots=\\beta_k=0 \\\\ H_1 &amp;= \\text{al menos uno de los} \\, \\beta_j\\neq0 \\, \\text{con} \\, j=1,2,\\ldots,k, \\end{align}\\] La prueba para analizar las hipótesis anteriores se llama prueba de significancia de la regresión. En todo modelo de regresión vamos a tener una variabilidad Total (\\(SS_T\\)), una variabilidad explicada por el modelo de Regresión (\\(SS_R\\)) y una variabilidad Residual (\\(SS_{Res}\\)) que no logra ser explicada por el modelo, abajo una figura ilustrativa de las tres variabilidades. En esta prueba la idea es determinar si la variabilidad explicada por la Regresión (\\(SS_R\\)) es una parte considerable de la variabilidad Total (\\(SS_T\\)) o no. Para realizar esta prueba se construye la tabla anova (analysis of variance) tal como se muestra a continuación. Asumiendo \\(H_0\\) verdadera, la distribución del estadístico \\(F_0\\) es \\(F_{k, n-k-1}\\). Ejemplo para RLS Como ilustración vamos a usar los datos del ejemplo sobre la resistencia de piezas soldadas en función de la edad de la soldadura. ¿Será que la variable edad si ayuda a explicar la resistencia?, ¿será que la variable edad es significativa para el modelo? Solución En este problema nos interesa estudiar el siguiente conjunto de hipótesis. \\[\\begin{align} H_0 &amp;: \\beta_{edad}=0 \\\\ H_1 &amp;: \\beta_{edad} \\neq 0 \\end{align}\\] Para responder esta pregunta vamos a aplicar la prueba de significancia de la regresión. Lo primero que se debe hacer es ajustar el modelo. file &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/propelente&quot; datos &lt;- read.table(file=file, header=TRUE) mod1 &lt;- lm(Resistencia ~ Edad, data=datos) Luego de ajustar el modelo debemos calcular los elementos de la tabla anova, para eso podemos usar la función anova_table_lm del paquete model propuesto por Hernandez and Usuga (2024). Para instalar el paquete model podemos usar el siguiente código. if (!require(&quot;remotes&quot;)) install.packages(&quot;remotes&quot;) remotes::install_github(&quot;fhernanb/model&quot;) Luego de esto ya podemos usar la función anova_table_lm así: library(model) anova_table_lm(mod1) ## Anova Table ## Sum Sq Df Mean Sq F value Pr(&gt;F) ## Regression 1527483 1 1527483 165.38 1.643e-10 *** ## Residuals 166255 18 9236 ## Total 1693738 19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 De la tabla anterior se observa que el valor-P es muy pequeño, por lo tanto hay evidencias para rechazar \\(H_0: \\beta_{edad}=0\\), eso significa que la variable edad si ayuda a explicar la media de la variable respuesta. Otra forma de aplicar la prueba de significancia de la regresión es usando la función summary la cual nos entrega una parte de la tabla anova anterior (no toda la tabla anova). summary(mod1) ## ## Call: ## lm(formula = Resistencia ~ Edad, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -215.98 -50.68 28.74 66.61 106.76 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2627.822 44.184 59.48 &lt; 2e-16 *** ## Edad -37.154 2.889 -12.86 1.64e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 96.11 on 18 degrees of freedom ## Multiple R-squared: 0.9018, Adjusted R-squared: 0.8964 ## F-statistic: 165.4 on 1 and 18 DF, p-value: 1.643e-10 En la última línea de la salida anterior tenemos la información de la prueba de hipótesis sobre significancia de la regresión. El valor-P de esta prueba es 1.643e-10 y por lo tanto podemos rechazar \\(H_0\\) a un nivel de significancia usual del 5%, eso significa que la variable edad si ayuda a explicar la resistencia. Ejemplo para RLM Como ilustración vamos a usar los datos del ejemplo 3.1 del libro de E. &amp;. V. Montgomery D. &amp; Peck (2006). En el ejemplo 3.1 los autores ajustaron un modelo de regresión lineal múltiple para explicar el Tiempo necesario para que un trabajador haga el mantenimiento y surta una máquina dispensadora de refrescos en función de las variables Número de Cajas y Distancia. ¿Será que las variables Número de Cajas y Distancia son significativas en el modelo? Solución En este problema nos interesa estudiar el siguiente conjunto de hipótesis. \\[\\begin{align} H_0 &amp;: \\beta_{cant}=\\beta_{dis}=0 \\\\ H_1 &amp;= \\text{al menos uno de los coefiencientes} \\, \\beta_{cant} \\, \\text{o} \\, \\beta_{dis} \\, \\text{es diferente de cero} \\end{align}\\] Para responder esta pregunta vamos a aplicar la prueba de significancia de la regresión. Lo primero que se debe hacer es ajustar el modelo. require(MPV) colnames(softdrink) &lt;- c(&#39;tiempo&#39;, &#39;cantidad&#39;, &#39;distancia&#39;) mod2 &lt;- lm(tiempo ~ cantidad + distancia, data=softdrink, x=TRUE) Luego de ajustar el modelo debemos calcular los elementos de la tabla anova, para eso usamos el siguiente código. anova_table_lm(mod2) ## Anova Table ## Sum Sq Df Mean Sq F value Pr(&gt;F) ## Regression 5550.8 2 2775.41 261.24 4.687e-16 *** ## Residuals 233.7 22 10.62 ## Total 5784.5 24 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 De la tabla anterior se observa que el valor-P es muy pequeño por lo tanto hay evidencias para rechazar \\(H_0: \\beta_{cant}=\\beta_{dis}=0\\), eso significa que al menos una (o ambas) de las variables si ayudan a explicar la media de la variable respuesta. Otra forma de aplicar la prueba de significancia de la regresión es usando la función summary la cual nos entrega una parte de la tabla anova anterior (no toda la tabla anova). summary(mod2) ## ## Call: ## lm(formula = tiempo ~ cantidad + distancia, data = softdrink, ## x = TRUE) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7880 -0.6629 0.4364 1.1566 7.4197 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.341231 1.096730 2.135 0.044170 * ## cantidad 1.615907 0.170735 9.464 3.25e-09 *** ## distancia 0.014385 0.003613 3.981 0.000631 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.259 on 22 degrees of freedom ## Multiple R-squared: 0.9596, Adjusted R-squared: 0.9559 ## F-statistic: 261.2 on 2 and 22 DF, p-value: 4.687e-16 En la última línea de la salida anterior tenemos la información de la prueba de hipótesis sobre significancia de la regresión. El valor-P de esta prueba es 4.687e-16 y por lo tanto podemos rechazar \\(H_0\\) a un nivel de significancia usual del 5%, eso significa que al menos una de las dos covariables del modelo es significativa para explicar el tiempo medio. El no rechazar \\(H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_k = 0\\) significa que ninguna de las variables aporta información para explicar la media de \\(Y\\). Lo que se debe hacer es buscar nuevas covariables que SI sean significativas. El rechazar \\(H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_k = 0\\) significa que una, o dos, o tres, o cuatro, …, o que todas las \\(k\\) covariables son significativas. Pero, ¿cómo saber cuales variables son significativas? Prueba para comparar modelos anidados Esta prueba se usa para comparar modelos que comparten una estructura anidada. A continuación se muestran dos modelos (reducido y completo) anidados a manera de ilustración. Modelo reducido \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] Modelo completo \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + \\beta_4 x_{4i}, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] La pregunta que surge aquí es: ¿vale la pena incluir las variables \\(x_3\\) y \\(x_4\\) simultáneamente al modelo reducido? ¿esas variables mejoran el modelo reducido? ¿será mejor el modelo reducido o el modelo completo? Los interrogantes anteriores se pueden resumir simbólicamente así: \\[\\begin{align} H_0 &amp;: \\beta_3=\\beta_4=0 \\\\ H_1 &amp;= \\text{al menos uno de los coefiencientes} \\, \\beta_{3} \\, \\text{o} \\, \\beta_{4} \\, \\text{es diferente de cero} \\end{align}\\] Para realizar esta prueba se usa el siguiente estadístico \\[ F_0 = \\frac{\\frac{SS_R(complete) - SS_R(reduced)}{p_1-p_0}}{MS_{Res}}, \\] donde \\(SS_R(complete)\\) es la suma de cuadrados de la regresión para el modelo completo, \\(SS_R(reduced)\\) es la suma de cuadrados de la regresión para el modelo reducido, \\(p_1\\) es el número de \\(\\beta\\)’s en el modelo completo, \\(p_0\\) es el número de \\(\\beta\\)’s en el modelo reducido y \\(MS_{Res}\\) es la estimación de \\(\\sigma^2\\) en el modelo completo. La distribución del estadístico \\(F_0\\) es: \\(F_{p_1-p_0, n-p_1}\\) si \\(H_0\\) es cierta. \\(F_{p_1-p_0, n-p_1, \\lambda}\\) si \\(H_1\\) es cierta, siendo \\(\\lambda\\) el parámetro de no centralidad de una distribución F. Para conocer la expresión de \\(\\lambda\\) se recomienda revisar la página 90 de D. C. Montgomery, Peck, and Vining (2012) y los comentarios al respecto. Ejemplo Usando la base de datos table.b4 del paquete MPV de Braun and MacQueen (2023), queremos comparar dos modelos anidados: el modelo reducido con la fórmula y ~ x1 + x2, y el modelo completo con la fórmula y ~ x1 + x2 + x3 + x4. ¿Será que la inclusión simultánea de las variables \\(x_3\\) y \\(x_4\\) mejora el modelo para explicar la variable respuesta \\(y\\)? Use \\(\\alpha=0.03\\) para concluir. Solución Primero veamos los datos. require(MPV) data(table.b4) head(table.b4, n=4) ## y x1 x2 x3 x4 x5 x6 x7 x8 x9 ## 1 29.5 5.0208 1 3.531 1.500 2 7 4 62 0 ## 2 27.9 4.5429 1 2.275 1.175 1 6 3 40 0 ## 3 25.9 4.5573 1 4.050 1.232 1 6 3 54 0 ## 4 29.9 5.0597 1 4.455 1.121 1 6 3 42 0 Ajustemos los dos modelos de interés. redu_mod &lt;- lm(y ~ x1 + x2, data=table.b4, x=TRUE) comp_mod &lt;- lm(y ~ x1 + x2 + x3 + x4, data=table.b4, x=TRUE) En este ejercicio nos interesa estudiar el siguiente conjunto de hipótesis. \\[\\begin{align} H_0 &amp;: \\beta_3 = \\beta_4 = 0 \\\\ H_1 &amp;= \\text{al menos uno de los coefiencientes} \\, \\beta_{3} \\, \\text{o} \\, \\beta_{4} \\, \\text{es diferente de cero} \\end{align}\\] Ahora construyamos el estadístico \\(F_0\\). n &lt;- 24 # numero de observaciones p0 &lt;- 3 # numero de betas en modelo reducido p1 &lt;- 5 # numero de betas en modelo completo ssr_reduced &lt;- sum(table.b4$y) - sum(redu_mod$residuals^2) ssr_complete &lt;- sum(table.b4$y) - sum(comp_mod$residuals^2) ms_res &lt;- summary(comp_mod)$sigma^2 F0 &lt;- ((ssr_complete - ssr_reduced) / (p1-p0)) / ms_res F0 ## [1] 0.2831532 Para tomar la decisión podemos hacerlo de dos formas: usando un valor de referencia o usando el valor-P. Usando el valor de referencia Buscamos el cuantil \\(f_{\\alpha, p_1-p_0, n-p_1}\\) usando el \\(\\alpha=0.03\\) dado en el enunciado. qf(p=0.03, df1=p1-p0, df2=n-p1, lower.tail=FALSE) ## [1] 4.241262 Como \\(F_0=0.2831\\) es menor que \\(f_{\\alpha, p_1-p_0, n-p_1}=4.2412\\), por lo tanto no hay evidencias para rechazar \\(H_0\\). Usando el valor-P Buscamos el área a la derecha de \\(F_0=0.2831\\) en una distribución F con \\(p_1-p_0\\) y \\(n-p_1\\) grados de libertad. pf(q=0.2831, df1=p1-p0, df2=n-p1, lower.tail=FALSE) ## [1] 0.7565673 Como el valor-P es 0.7566 y es mayor que el nivel de significancia \\(\\alpha=0.03\\), no hay evidencias para rechazar \\(H_0\\). En conclusión, las dos variables \\(x_3\\) y \\(x_4\\) no mejoran el modelo. Es mejor un modelo sólo con \\(x_2\\) y \\(x_1\\). Exploremos la tabla de resumen para el modelo completo. summary(comp_mod) ## ## Call: ## lm(formula = y ~ x1 + x2 + x3 + x4, data = table.b4, x = TRUE) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0005 -1.8955 0.0506 1.8327 5.1112 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.8409 3.2571 3.021 0.00702 ** ## x1 2.4375 0.6704 3.636 0.00176 ** ## x2 6.4373 3.8206 1.685 0.10836 ## x3 0.3233 0.4324 0.748 0.46377 ## x4 -0.2177 3.7475 -0.058 0.95429 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.893 on 19 degrees of freedom ## Multiple R-squared: 0.8082, Adjusted R-squared: 0.7679 ## F-statistic: 20.02 on 4 and 19 DF, p-value: 1.332e-06 Si miramos la tabla anterior podríamos caer en la tentación de decir que las variables \\(x_4\\), \\(x_3\\) y \\(x_2\\) no aportan al modelo, ya que sus valores-P de contribución individual son muy grandes. Sin embargo, esto es una TRAMPA, nos podemos equivocar. Por esa razón, obtengamos ahora la tabla de resumen para el modelo reducido. summary(redu_mod) ## ## Call: ## lm(formula = y ~ x1 + x2, data = table.b4, x = TRUE) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7639 -1.9454 -0.1822 1.8068 5.0423 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.0418 2.9585 3.394 0.00273 ** ## x1 2.7134 0.4849 5.595 1.49e-05 *** ## x2 6.1643 3.1864 1.935 0.06663 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.792 on 21 degrees of freedom ## Multiple R-squared: 0.8025, Adjusted R-squared: 0.7837 ## F-statistic: 42.67 on 2 and 21 DF, p-value: 4.007e-08 La variable \\(x_2\\) que en la tabla de modelo completo aparecía como no significativa ahora si aparece como significativa a un nivel \\(\\alpha=0.10\\). Este fenómeno se llama efecto de enmascaramiento. Al tener en el modelo \\(x_3\\) y \\(x_4\\), ellas opacan la importancia o enmascaran a la variable \\(x_2\\). Nunca use los resultados del summary para juzgar si varias variables son importantes o no, use la prueba para modelos anidados. Recuerde el efecto de enmascaramiento. ** Lo que viene a continuación es opcional** Ahora vamos a calcular el parámetro de no centralidad \\(\\lambda\\). Recuerde revisar la página 90 de D. C. Montgomery, Peck, and Vining (2012) para conocer su expresión. beta2 &lt;- matrix(c(0.3233333, -0.2176622), ncol=1) x1 &lt;- comp_mod$x[, 1:3] x2 &lt;- comp_mod$x[, 4:5] a1 &lt;- t(beta2) %*% t(x2) a2 &lt;- diag(n) - x1 %*% solve(t(x1) %*% x1) %*% t(x1) a3 &lt;- x2 %*% beta2 lambda &lt;- (a1 %*% a2 %*% a3) / summary(comp_mod)$sigma^2 lambda ## [,1] ## [1,] 0.5663064 Ahora vamos a calcular el valor-P de la prueba usando la distribución F no central y la distribución F (usual) así: pf(q=F0, df1=p1-p0, df2=n-p1, ncp=lambda, lower.tail=FALSE) ## [1] 0.8088857 pf(q=F0, df1=p1-p0, df2=n-p1, lower.tail=FALSE) ## [1] 0.7565282 Usando el último valor-P que es mayor que un nivel de significancia \\(\\alpha\\), no hay evidencias para rechazar \\(H_0: \\beta_3 = \\beta_4 = 0\\), y por lo tanto podemos concluir que las variables \\(x_3\\) y \\(x_4\\) no mejoran el modelo. Función anova La función anova permite realizar pruebas de hipótesis como las mostradas en las secciones anteriores, en particular la función sirve para: comparar secuencialmente las variables de un modelo. comparar modelos anidados. El método S3 anova.lm (o simplemente anova) tiene la estructura mostrada a continuación. anova(object, test, scale=0) Los argumentos de esta función son: object: un objeto de la clase lm. test: una cadena de caracteres indicando el tipo de prueba, F, Chisq o Cp, el valor por defecto es F. scale: valor de la estimación de \\(\\sigma^2\\). Cuando es igual a cero se usa el estimador del modelo con más parámetros. La función anova se puede aplicar a un solo modelo y el resultado será una prueba de hipótesis secuencial en las variables. La función anova se puede aplicar a varios modelos y el resultado será una comparación de modelos. A continuación se muestran dos ejemplos en los cuales se ilustra la utilidad de la función anova. Ejemplo En este ejemplo se mostrará la segunda utilidad de la función anova. Para esto vamos a utilizar la base de datos Cars93 del paquete MASS. El objetivo es construir un modelo para explicar la media del Price de los autos en función de las variables Horsepower, Type y Weight. Usar un nivel de significancia \\(\\alpha=0.07\\). Solución La solución de este ejercicio tendrá dos partes, en la primera se construirán varios modelos, iniciando con uno sin covariables (mod0) hasta uno con todas las covariables (mod3). En la segunda parte se analizará el modelo con todas las covariables directamente. Primera parte A continuación el código para crear y comparar los modelos sin covariables y el modelo con solo Horsepower. library(MASS) mod0 &lt;- lm(Price ~ 1, data=Cars93) mod1 &lt;- lm(Price ~ Horsepower, data=Cars93) anova(mod0, mod1) ## Analysis of Variance Table ## ## Model 1: Price ~ 1 ## Model 2: Price ~ Horsepower ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 92 8584.0 ## 2 91 3250.9 1 5333.1 149.29 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 De la salida anterior se tiene que el valor-P es &lt; 2.2e-16 y por lo tanto se concluye que la variable Horsepower mejora el modelo (la misma conclusión se pudo obtener del summary). Ahora vamos a crear y comparar el modelo con sólo Horsepower con el modelo con Horsepower y Type. mod2 &lt;- lm(Price ~ Horsepower + Type, data=Cars93) anova(mod1, mod2) ## Analysis of Variance Table ## ## Model 1: Price ~ Horsepower ## Model 2: Price ~ Horsepower + Type ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 91 3250.9 ## 2 86 2758.1 5 492.81 3.0733 0.01337 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 De la salida anterior se tiene que el valor-P es 0.01337 y por lo tanto se concluye que el modelo mod2 con dos covariables explica mejor la variable Price. Ahora vamos a crear y comparar el modelo con Horsepower y Type con el modelo con las tres covariables. mod3 &lt;- lm(Price ~ Horsepower + Type + Weight, data=Cars93) anova(mod2, mod3) ## Analysis of Variance Table ## ## Model 1: Price ~ Horsepower + Type ## Model 2: Price ~ Horsepower + Type + Weight ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 86 2758.1 ## 2 85 2751.3 1 6.7934 0.2099 0.648 De esta última salida vemos que el valor-P es 0.648, esto indica que la inclusión de la variable Weight no mejora el modelo mod2 (la misma conclusión se pudo obtener del summary). Segunda parte En esta segunda parte del ejemplo se usará la función anova directamente sobre el modelo completo mod3, a continuación los resultados. anova(mod3) ## Analysis of Variance Table ## ## Response: Price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Horsepower 1 5333.1 5333.1 164.7661 &lt; 2e-16 *** ## Type 5 492.8 98.6 3.0451 0.01411 * ## Weight 1 6.8 6.8 0.2099 0.64803 ## Residuals 85 2751.3 32.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 En la tabla anterior aparecen los resultados de una prueba de hipótesis secuencial a partir de la fórmula de mod3. La fórmula de mod3 es Price ~ Horsepower + Type + Weight, por lo tanto en la primera línea de la tabla aparece la variable Horsepower, luego en la segunda aparece Type y así hasta la última variable Weight. El valor-P reportado en la primer línea es &lt; 2e-16, esto indica que el modelo con Horsepower es mejor que el modelo sin covariables; el valor-P de la segunda línea es 0.01411, esto indica que es mejor un modelo con las covariables Horsepower y Type; por último el valor-P de la tercera línea es 0.64803, esto indica que la variable Weight no mejora el modelo, es decir, que es mejor un modelo con Horsepower y Type solamente. Ejemplo Usando la base de datos table.b4 del paquete MPV propuesto por Braun and MacQueen (2023), queremos comparar dos modelos anidados: el modelo reducido con la fórmula y ~ x1 + x2, y el modelo completo con la fórmula y ~ x1 + x2 + x3 + x4. ¿Será que la inclusión simultánea de las variables \\(x_3\\) y \\(x_4\\) mejora el modelo para explicar la variable respuesta \\(y\\)? Use \\(\\alpha=0.03\\) para concluir. Solución Para este ejemplo vamos a usar los datos que se muestran a continuación. require(MPV) data(table.b4) head(table.b4, n=4) ## y x1 x2 x3 x4 x5 x6 x7 x8 x9 ## 1 29.5 5.0208 1 3.531 1.500 2 7 4 62 0 ## 2 27.9 4.5429 1 2.275 1.175 1 6 3 40 0 ## 3 25.9 4.5573 1 4.050 1.232 1 6 3 54 0 ## 4 29.9 5.0597 1 4.455 1.121 1 6 3 42 0 Ahora vamos a ajustar ambos modelos usando el siguiente código. mod1 &lt;- lm(y ~ x1 + x2, data=table.b4) mod2 &lt;- lm(y ~ x1 + x2 + x3 + x4, data=table.b4) El objetivo en este ejercicio analizar el siguiente conjunto de hipótesis. \\[H_0: \\text{las variables x3 y x4 no mejoran el modelo},\\] \\[H_A: \\text{al menos una de ellas si mejora el modelo}\\] Para comparar los dos modelos usamos la siguiente instrucción. anova(mod1, mod2, test=&#39;F&#39;) ## Analysis of Variance Table ## ## Model 1: y ~ x1 + x2 ## Model 2: y ~ x1 + x2 + x3 + x4 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 21 163.71 ## 2 19 158.98 2 4.7384 0.2832 0.7565 De la salida anterior se observa que el valor-P de la prueba es de 0.7565, usando un nivel de significancia del 5%, se concluye que la inclusión de las variables \\(x_3\\) y \\(x_4\\) no mejoran el modelo. summary versus anova La función summary permite evaluar el efecto de una variable asumiendo que las restantes variables siguen en el modelo, esto es llamado prueba de hipótesis marginal. Cuando se tiene una variable cualitativa (con \\(k\\) niveles) dentro del modelo, ella aparecerá en la tabla del summary por medio de \\(k-1\\) variables indicadoras y por lo tanto se tendrán \\(k-1\\) valores-P asociados. Usar esos valores-P para decidir si una variable cualitativa es importante en el modelo puede ser engañoso, a continuación un ejemplo de esta situación. Ejemplo El ejemplo aquí mostrado está basado en una pregunta de StackOverFlow. El ejemplo consiste en simular un conjunto de 30 valores de \\(y \\sim N(\\mu, 1)\\), donde las observaciones 1 a 10 tienen \\(\\mu=0\\), las observaciones 11 a 20 tienen \\(\\mu=-0.5\\) y las restantes diez tienen \\(\\mu=0.5\\). Para diferenciar las observaciones se tendrá la variable de agruación cualitativa g que contendrá las letras A, B y C diez veces cada una. El código para simular los datos se muestra a continuación. set.seed(8867) # this makes the example exactly reproducible y &lt;- c(rnorm(10, mean=0, sd=1), rnorm(10, mean=-0.5, sd=1), rnorm(10, mean=0.5, sd=1)) g &lt;- rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each=10) ¿Será la variable cualitativa g significativa en un modelo de regresión? Solución Vamos a ajustar el modelo con fórmula y ~ g para estudiar el efecto de la agrupación g en la media de la variable respuesta \\(y\\). model &lt;- lm(y ~ g) Obviamente esperamos concluir que la media de la variable \\(y\\) dependa de la variable de agrupación g. Para esto vamos a explorar el resultado con la función summary. summary(model) ## ## Call: ## lm(formula = y ~ g) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.59080 -0.54685 0.04124 0.79890 2.56064 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.4440 0.3855 -1.152 0.260 ## gB -0.9016 0.5452 -1.654 0.110 ## gC 0.6729 0.5452 1.234 0.228 ## ## Residual standard error: 1.219 on 27 degrees of freedom ## Multiple R-squared: 0.2372, Adjusted R-squared: 0.1807 ## F-statistic: 4.199 on 2 and 27 DF, p-value: 0.02583 De la salida anterior vemos que los efectos gB y gC tienen valores-P altos, superiores al usual 5%, y por lo tanto estaríamos tentados a decir que la variable g no tiene efecto sobre la media de \\(y\\). El lector podría encontrar esto un poco desconcertante. Vamos a realizar el análisis pero ahora con la función anova. anova(model) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## g 2 12.484 6.2418 4.199 0.02583 * ## Residuals 27 40.135 1.4865 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 En la fila donde aparece la variable g tenemos el resultado de la prueba de hipótesis \\[H_0: \\text{la variable g no influye en la media de y},\\] \\[H_A: \\text{la variable g si influye en la media de y}\\] El valor-P de esta prueba es de 0.02583, esto indica que hay evidencias para rechazar \\(H_0\\), es decir, encontramos que la variable g si influye sobre la media de la variable \\(y\\). Cuando se quiera explorar el efecto de una variable cualitativa en un modelo es mejor usar la función anova que los resultados del summary. Prueba razón de verosimilitud Esta prueba evalúa la bondad de ajuste de dos modelos estadísticos competitivos en función de la razón de sus verosimilitudes, específicamente uno encontrado por maximización en todo el espacio de parámetros y otro encontrado después de imponer alguna restricción (\\(H_0\\)). El estadístico de la prueba razón de verosimilitud se muestra a continuación. \\[ \\lambda = -2 \\log \\left[ \\frac{L(\\Theta_0)}{L(\\Theta)} \\right] = -2 \\left[ l(\\Theta_0)-l(\\Theta) \\right], \\] donde \\(L\\) representa el valor de la verosimilitud y \\(l\\) el valor de log-verosimilitud. Bajo la hipótesis nula, \\(\\lambda \\sim \\chi^2_{k}\\) donde \\(k\\) es la diferencia entre el número de parámetros de los modelos comparados. Ejemplo Usando la base de datos table.b4 del paquete MPV propuesto por Braun and MacQueen (2023), queremos comparar dos modelos anidados: el modelo reducido con la fórmula y ~ x1 + x2, y el modelo completo con la fórmula y ~ x1 + x2 + x3 + x4. ¿Será que la inclusión simultánea de las variables \\(x_3\\) y \\(x_4\\) mejora el modelo para explicar la variable respuesta \\(y\\)? Use \\(\\alpha=0.03\\) para concluir. Solución Para este ejemplo vamos a usar los datos que se muestran a continuación. require(MPV) data(table.b4) head(table.b4, n=4) ## y x1 x2 x3 x4 x5 x6 x7 x8 x9 ## 1 29.5 5.0208 1 3.531 1.500 2 7 4 62 0 ## 2 27.9 4.5429 1 2.275 1.175 1 6 3 40 0 ## 3 25.9 4.5573 1 4.050 1.232 1 6 3 54 0 ## 4 29.9 5.0597 1 4.455 1.121 1 6 3 42 0 Ahora vamos a ajustar ambos modelos usando el siguiente código. mod0 &lt;- lm(y ~ x1 + x2, data=table.b4) mod1 &lt;- lm(y ~ x1 + x2 + x3 + x4, data=table.b4) El objetivo en este ejercicio analizar el siguiente conjunto de hipótesis. \\[H_0: \\text{las variables x3 y x4 no mejoran el modelo},\\] \\[H_A: \\text{al menos una de ellas si mejora el modelo}\\] Para aplicar la prueba razón de verosimilitud usamos el siguiente código. Los grados de libertad en esta prueba son 2 porque esa es la diferencia entre el número de parámetros de los modelos. lambda &lt;- -2 * (logLik(mod0) - logLik(mod1)) lambda ## &#39;log Lik.&#39; 0.7048811 (df=4) pchisq(q=lambda, df=2, lower.tail=FALSE) ## &#39;log Lik.&#39; 0.7029704 (df=4) Como el valor-P es grande, no hay evidencias para rechazar \\(H_0: \\beta_3 = \\beta_4 = 0\\), y por lo tanto podemos concluir que las variables \\(x_3\\) y \\(x_4\\) no mejoran el modelo. Comparaciones múltiples En esta sección se muestra como utilizar el paquete multcomp propuesto por Hothorn, Bretz, and Westfall (2023) que está basado en el libro “Multiple comparisons using R” de Frank Bretz (2010) para estudiar pruebas de hipótesis múltiples. Ejemplo Usando la base de datos table.b4 del paquete MPV de Braun and MacQueen (2023), queremos ajustar el siguiente modelo: \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + \\beta_4 x_{4i}, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] ¿Será que la inclusión simultánea de las variables \\(x_3\\) y \\(x_4\\) mejora el modelo para explicar la variable respuesta \\(y\\)? Use \\(\\alpha=0.03\\) para concluir. Solución Lo primero que debemos hacer es ajustar el modelo de interés. require(MPV) data(table.b4) mod &lt;- lm(y ~ x1 + x2 + x3 + x4, data=table.b4) coef(mod) ## (Intercept) x1 x2 x3 x4 ## 9.8408825 2.4374996 6.4373377 0.3233333 -0.2176622 En este ejemplo nos interesa estudiar las siguientes dos hipótesis simultáneamente (no individualmente). \\[\\begin{align*} H_0 &amp;: \\beta_{3} = 0 &amp; H_0 &amp;: \\beta_{4} = 0 \\\\ H_A &amp;: \\beta_{3} \\neq 0 &amp; H_A &amp;: \\beta_{4} \\neq 0 \\end{align*}\\] Los dos conjuntos de hipótesis anteriores se pueden escribir matricialmente usando una matriz de constrates \\(\\boldsymbol{C}\\) que multiplica al vector de parámetros \\(\\boldsymbol{\\beta}\\). A continuación se muestra la matriz \\(\\boldsymbol{C}\\) que permite obtener las dos parejas de hipótesis anteriores: \\[ \\boldsymbol{C} \\boldsymbol{\\beta} = \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\end{pmatrix} = \\begin{pmatrix} \\beta_3 \\\\ \\beta_4 \\end{pmatrix} \\] Los lados derechos de las hipótesis son constantes y se escriben como un vector \\(\\boldsymbol{a}=(0, 0)^\\top\\). Teniendo definida la matriz \\(\\boldsymbol{C}\\), el vector \\(\\boldsymbol{a}\\) la prueba simultánea de las hipótesis se puede hacer de la siguiente manera. library(multcomp) C &lt;- matrix(c(0, 0, 0, 1, 0, 0, 0, 0, 0, 1), ncol=5, byrow=TRUE) mult_test &lt;- glht(model=mod, linfct=C, alternative=&#39;two.sided&#39;, rhs=c(0, 0)) summary(mult_test, test = adjusted(type=&quot;single-step&quot;)) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: lm(formula = y ~ x1 + x2 + x3 + x4, data = table.b4) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 0.3233 0.4324 0.748 0.704 ## 2 == 0 -0.2177 3.7475 -0.058 0.998 ## (Adjusted p values reported -- single-step method) De la salida anterior se obtiene que los dos valores-P “ajustados” (ver sección 2.1.2 de Frank Bretz (2010)) son: \\[ q_1 = 0.704 \\qquad q_2 = 0.998 \\] Esto significa que no hay evidencias para rechazar \\(H_0 : \\beta_{3} = 0\\) ni evidencias para rechazar \\(H_0 : \\beta_{4} = 0\\) porque ambos valores-P “ajustados” fueron mayores del nivel de significancia de 0.03. En otras palabras, ni x3 ni x4 aportan información para explicar la media de \\(y\\). Si un valor-P “ajustado” es menor que el \\(\\alpha\\), se rechaza su correspondiente hipótesis nula \\(H_0\\), pero si el valor-P “ajustado” es mayor que el \\(\\alpha\\), no se rechaza \\(H_0\\). En una aplicación es posible que se rechacen todas/algunas/ninguna de las \\(H_0\\). References Braun, W. J., and S. MacQueen. 2023. MPV: Data Sets from Montgomery, Peck and Vining. https://CRAN.R-project.org/package=MPV. Frank Bretz, Peter Westfall, Torsten Hothorn. 2010. Multiple Comparisons Using r. Chapman; Hall/CRC. Hernandez, Freddy, and Olga Usuga. 2024. Model: This Package Contains Useful Functions for Modeling Regresion. https://fhernanb.github.io/model. Hothorn, Torsten, Frank Bretz, and Peter Westfall. 2023. Multcomp: Simultaneous Inference in General Parametric Models. http://multcomp.R-forge.R-project.org. Montgomery, Douglas C., Elizabeth A. Peck, and Geoffrey G. Vining. 2012. Introduction to Linear Regression Analysis (5th Ed.). Wiley &amp; Sons. Montgomery, E. &amp; Vining, D. &amp; Peck. 2006. Introducción Al Análisis de Regresión Lineal. 3ed ed. México: Cecsa. "],["diag1.html", "10 Diagnósticos parte I Supuestos Residuales Chequeando normalidad de los errores Chequeando si errores con media cero Chequeando si los errores tiene varianza constante Chequeando si errores no están correlacionados Gráficos de residuales usando car Prueba de falta de ajuste", " 10 Diagnósticos parte I En este capítulo se presentan varias herramientas útiles para hacer diagnósticos de un modelo ajustado. Supuestos Los supuestos en un modelo de regresión se pueden escribir de dos formas: Forma I Los errores \\(\\epsilon_i\\) tienen distribución normal. Los errores \\(\\epsilon_i\\) tienen media cero. Los errores \\(\\epsilon_i\\) tiene varianza constante. Los errores \\(\\epsilon_i\\) no están correlacionados. Forma II La respuesta \\(y\\) tiene distribución normal. La varianza de la respuesta \\(y\\) es constante. Las observaciones son independientes \\(y\\). Relación lineal entre la variable respuesta y las covariables. Ambos conjuntos de supuestos son equivalentes, la forma I está dirigida hacia los errores mientras que en la forma II está dirigida hacia los \\(y_i\\). Residuales Los residuales en los modelos de regresión nos ayudan a: determinar qué tan bien el modelo explica el patrón de los datos, verificar el cumplimiento de los supuestos del modelo. A continuación se muestran los diferentes tipos de residuales que se pueden definir para un modelo de regresión. Todas las fórmulas mostradas coinciden con las fórmulas usadas internamente por R para calcular los residuales. La cantidad \\(w_i\\) usada en varios residuales corresponde al peso o importancia de cada observación en el modelo, por defecto es \\(w_i=1\\). La cantidad \\(h_{ii}\\) se llama leverage y corresponde al elemento \\(i\\) de la diagonal de la matriz sombrero o hat \\(\\boldsymbol{H} = \\boldsymbol{X}(\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\). La varianza \\(\\hat{\\sigma}_{(i)}^{2}\\) es la varianza estimada al NO tener en cuenta la observación \\(i\\)-ésima. La cantidad \\(\\hat{y}_{(i)}\\) es la estimación de la \\(i\\)-ésima observación usando un modelo en el cual la observación \\(i\\)-ésima NO fue usada en el ajuste del modelo. En algunos textos los residuales estandarizados y estudentizados se definen de una forma más sencilla sin involucrar los pesos \\(w_i\\) ni los valores \\(h_{ii}\\). Las fórmulas sencillas de esos residuales son: \\(d_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma^2}}}\\). \\(r_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma^2} (1-h_{ii})}}\\) Para obtener los residuales arriba definidos tenemos las siguientes funciones: residuals(object, type=c(&quot;working&quot;, &quot;response&quot;, &quot;deviance&quot;, &quot;pearson&quot;, &quot;partial&quot;)) rstandard(object) rstudent(object) Si un modelo lineal está correctamente especificado (ajustado), los residuos de Pearson serán independientes de los valores ajustados e independientes de los predictores, y estos gráficos deben ser “gráficos nulos”, es decir, gráficos sin características sistemáticas, en el sentido de que la distribución condicional de los residuos (en el eje vertical del gráfico) no debe cambiar con los valores ajustados o con un predictor (en el eje horizontal) (J. Fox and Weisberg 2019). Los errores del modelo se denotan como \\(\\epsilon_i\\) mientras que los residuales usuales se denotan como \\(e_i\\), no los confunda. Ejemplo Considere los datos mostrados abajo, ajuste un modelo de regresión lineal para explicar la media de \\(y\\) en función de \\(x\\) y usando como pesos los valores de \\(w\\). Obtenga los residuales usando las definiciones (fórmulas) y luego usando las funciones de R, compruebe que los resultados coinciden. x &lt;- c(4, 6, 8, 7, 8, 5) y &lt;- c(1, 2, 3, 4, 5, 4) w &lt;- c(0.1, 0.1, 0.2, 0.1, 0.2, 0.9) Solución Primero se van a calcular los residuales manualmente aplicando las definiciones. mod &lt;- lm(y ~ x, weights=w) ei &lt;- y - fitted(mod) pi &lt;- ei * sqrt(mod$weights) hii &lt;- lm.influence(mod)$hat di &lt;- ei * sqrt(mod$weights) / sqrt(summary(mod)$sigma^2 * (1-hii)) ri &lt;- ei * sqrt(mod$weights) / sqrt(lm.influence(mod)$sigma^2 * (1-hii)) cbind(ei=ei, pi=pi, di=di, ri=ri) ## ei pi di ri ## 1 -2.3487395 -0.74273664 -1.36444485 -1.61615603 ## 2 -1.7100840 -0.54077605 -0.92897379 -0.90846102 ## 3 -1.0714286 -0.47915742 -1.05401601 -1.07406432 ## 4 0.1092437 0.03454589 0.06072186 0.05261092 ## 5 0.9285714 0.41526977 0.91348054 0.88927302 ## 6 0.4705882 0.44643920 1.63607317 2.46342873 Ahora se van a calcular los residuales usando las funciones de R. cbind(ei=residuals(mod, type=&#39;working&#39;), pi=residuals(mod, type=&#39;pearson&#39;), di=rstandard(mod), ri=rstudent(mod)) ## ei pi di ri ## 1 -2.3487395 -0.74273664 -1.36444485 -1.61615603 ## 2 -1.7100840 -0.54077605 -0.92897379 -0.90846102 ## 3 -1.0714286 -0.47915742 -1.05401601 -1.07406432 ## 4 0.1092437 0.03454589 0.06072186 0.05261092 ## 5 0.9285714 0.41526977 0.91348054 0.88927302 ## 6 0.4705882 0.44643920 1.63607317 2.46342873 Estimado lector, ¿qué puede usted concluir de los resultados de este ejemplo? Chequeando normalidad de los errores Para estudiar si lo errores tienen una distribución aproximadamente normal se construyen los residuales estandarizados \\(d_i\\). Una vez calculados los \\(d_i\\) se construye un gráfico de normalidad o qqplot usando la función qqnorm, el resultado es un gráfico similar al mostrado a continuación. En la siguiente figura se muestran los diferentes patrones que se pueden encontrar en el gráfico de normalidad para \\(d_i\\). Para que se cumpla el supuesto de normalidad de los errores \\(e_i\\) se necesita que los \\(d_i\\) estén lo más alineados con la recta de referencia, alejamientos severos de esta recta significa que se viola el supuesto de normalidad de los errores. Chequeando si errores con media cero Para determinar si los errores tienen una media cerca al valor de cero se puede usar la función mean sobre los residuales \\(e_i\\). Chequeando si los errores tiene varianza constante En la siguiente figura se muestra el caso de varianza \\(\\sigma^2\\) constante (homocedasticidad) y el caso de varianza \\(\\sigma^2\\) no constante (heterocedasticidad). La homocedasticidad es el supuesto exigido en modelos de regresión. Para chequear si los errores tiene varianza constante se construye un gráfico de \\(e_i\\) versus \\(\\hat{\\mu}_i\\), un gráfico similar al mostrado a continuación. En la siguiente figura se muestran los diferentes patrones que se pueden encontrar en el gráfico de \\(e_i\\) versus \\(\\hat{\\mu}_i\\). Para que se cumpla el supuesto de homocedasticidad de los errores se necesita que los puntos se ubiquen como una nube de puntos sin ningún patrón claro. Cualquier patrón que se observe es evidencia de que no se cumple el supuesto de homocedasticidad de los errores. Una analogía útil para recordar si se cumple la homocedasticidad es que el gráfico de \\(e_i\\) versus \\(\\hat{\\mu}_i\\) tenga una apariencia como la mostrada en la siguiente figura. Otro gráfico útil para chequear el supuesto de homocedasticidad es dibujar un diagrama de dispersión de \\(\\sqrt{|d_i|}\\) versus \\(\\hat{\\mu}_i\\), un gráfico similar al mostrado a continuación. Al igual que en el gráfico de \\(e_i\\) versus \\(\\hat{\\mu}_i\\), se espera que no existan patrones claros en la nube de puntos. Chequeando si errores no están correlacionados Para estudiar esta situación se debe tener la historia de los errores, es decir, el orden en que las observaciones fueron tomadas. Usando es información se puede dibujar un diagrama de dispersión del residual versus tiempo, un gráfico similar al mostrado a continuación. En la siguiente figura se muestran los diferentes patrones que se pueden encontrar en el gráfico de \\(e_i\\) versus el tiempo. Para que se cumpla el supuesto de independencia se espera que los puntos se ubiquen como una nube de puntos sin ningún patrón claro. Ejemplo cumpliendo los supuestos En este ejemplo vamos a simular 500 observaciones del modelo mostrado abajo, luego vamos a ajustar un modelo correcto a los datos y por último vamos a realizar el análisis de residuales para saber si el modelo fue bien ajustado. \\[\\begin{align*} y_i &amp;\\sim N(\\mu_i, \\sigma^2) \\\\ \\mu_i &amp;= 4 - 6 x_i \\\\ x_i &amp;\\sim U(-5, 6) \\\\ \\sigma^2 &amp;= 16 \\end{align*}\\] Solución Lo primero que se debe hacer es simular los datos y ajustar el modelo. gen_dat &lt;- function(n) { varianza &lt;- 16 x &lt;- runif(n=n, min=-5, max=6) media &lt;- 4 - 6 * x y &lt;- rnorm(n=n, mean=media, sd=sqrt(varianza)) marco_datos &lt;- data.frame(y=y, x=x) return(marco_datos) } datos &lt;- gen_dat(n=500) mod &lt;- lm(y ~ x, data=datos) Los gráficos de residuales explicados anteriormente se pueden obtener usando la función plot sobre el modelo ajustado mod. par(mfrow=c(2, 2)) plot(mod, las=1, col=&#39;darkseagreen3&#39;, which=1:3) En la figura anterior se observa que los puntos del gráfico de normalidad de los residuales estandarizados \\(d_i\\) están muy cerca de la línea de referencia. Los diagramas de dispersión entre los residuales versus \\(\\hat{\\mu}_i\\) no muestran ninguna anomalía. Por estas razones podemos asumir que los supuestos del modelo se cumplen. Para estar más seguros de la normalidad de los errores vamos a aplicar la prueba Shapiro-Wilks. En esta prueba se tienen las siguiente hipótesis. \\[\\begin{align} H_0 &amp;: \\text{los residuales tienen distribución normal.} \\\\ H_1 &amp;= \\text{los residuales NO tienen distribución normal.} \\end{align}\\] Para aplicar la prueba podemos usar el siguiente código: ei &lt;- residuals(mod) shapiro.test(ei) ## ## Shapiro-Wilk normality test ## ## data: ei ## W = 0.99719, p-value = 0.5528 De la salida anterior vemos que el valor-p es mayor que un nivel de significancia del 7 por ciento, eso significa que no hay evidencias para rechazar \\(H_0\\). En este ejemplo se usó plot(mod, which=1:3) para obtener los tres primeros gráficos que entrega la función plot, el cuarto gráfico no es un gráfico de residuales y por eso se evitó en el ejemplo. Ejemplo violando los supuestos En este ejemplo vamos a simular 500 observaciones del modelo mostrado abajo en el cual modelo la media es función de \\(x\\) y de \\(x^2\\). Luego vamos a ajustar un modelo incorrecto en el cual la media solo dependa de \\(x\\) y por último vamos a construir los gráficos de residuales para ver si se logra identificar el problema. \\[\\begin{align*} y_i &amp;\\sim N(\\mu_i, \\sigma^2) \\\\ \\mu_i &amp;= 4 - 6 x_i + 2 x_i^2 \\\\ x_i &amp;\\sim U(-5, 6) \\\\ \\sigma^2 &amp;= 16 \\end{align*}\\] Solución Lo primero que se debe hacer es simular los datos y ajustar el modelo. gen_dat &lt;- function(n) { varianza &lt;- 16 x &lt;- runif(n=n, min=-5, max=6) media &lt;- 4 - 6 * x + 2 * x^2 y &lt;- rnorm(n=n, mean=media, sd=sqrt(varianza)) marco_datos &lt;- data.frame(y=y, x=x) return(marco_datos) } datos &lt;- gen_dat(n=500) mod &lt;- lm(y ~ x, data=datos) Los gráficos de residuales explicados anteriormente se pueden obtener usando la función plot sobre el modelo ajustado mod. par(mfrow=c(2, 2)) plot(mod, las=1, col=&#39;tomato&#39;, which=1:3) De los gráficos anteriores se observa claramente que lo residuales no dicen que algo está mal con el modelo ajustado. Para estar más seguros de la normalidad de los errores vamos a aplicar la prueba Shapiro-Wilks. En esta prueba se tienen las siguiente hipótesis. \\[\\begin{align} H_0 &amp;: \\text{los residuales tienen distribución normal.} \\\\ H_1 &amp;= \\text{los residuales NO tienen distribución normal.} \\end{align}\\] Para aplicar la prueba podemos usar el siguiente código: ei &lt;- residuals(mod) shapiro.test(ei) ## ## Shapiro-Wilk normality test ## ## data: ei ## W = 0.93258, p-value = 3.105e-14 De la salida anterior vemos que el valor-p es menor que un nivel de significancia del 7 por ciento, eso significa que no hay evidencias para rechazar \\(H_0\\). Ejemplo violando los supuestos En este ejemplo vamos a simular 500 observaciones del modelo mostrado abajo en el cual la varianza de \\(Y\\) es función de \\(x\\). Luego vamos a ajustar un modelo incorrecto en el cual asumimos que la varianza es constante y por último vamos a construir los gráficos de residuales para ver si se logra identificar el problema. \\[\\begin{align*} y_i &amp;\\sim N(\\mu_i, \\sigma^2) \\\\ \\mu_i &amp;= 4 - 6 x_i \\\\ x_i &amp;\\sim U(-5, 6) \\\\ \\log(\\sigma^2) &amp;= -1 + 0.5 x_i \\end{align*}\\] Solución Lo primero que se debe hacer es simular los datos y ajustar el modelo. gen_dat &lt;- function(n) { x &lt;- runif(n=n, min=-5, max=6) media &lt;- 4 - 6 * x varianza &lt;- exp(-1 + 0.5 * x) y &lt;- rnorm(n=n, mean=media, sd=sqrt(varianza)) marco_datos &lt;- data.frame(y=y, x=x) return(marco_datos) } datos &lt;- gen_dat(n=500) mod &lt;- lm(y ~ x, data=datos) Los gráficos de residuales explicados anteriormente se pueden obtener usando la función plot sobre el modelo ajustado mod. par(mfrow=c(2, 2)) plot(mod, las=1, col=&#39;purple&#39;, which=1:3) De los gráficos anteriores se observa claramente que lo residuales no dicen que algo está mal con el modelo ajustado. Para estar más seguros de la normalidad de los errores vamos a aplicar la prueba Shapiro-Wilks. En esta prueba se tienen las siguiente hipótesis. \\[\\begin{align} H_0 &amp;: \\text{los residuales tienen distribución normal.} \\\\ H_1 &amp;= \\text{los residuales NO tienen distribución normal.} \\end{align}\\] Para aplicar la prueba podemos usar el siguiente código: ei &lt;- residuals(mod) shapiro.test(ei) ## ## Shapiro-Wilk normality test ## ## data: ei ## W = 0.91551, p-value = 4.272e-16 De la salida anterior vemos que el valor-p es menor que un nivel de significancia del 7 por ciento, eso significa que no hay evidencias para rechazar \\(H_0\\). Gráficos de residuales usando car El paquete car de John Fox, Weisberg, and Price (2023), tiene unas funciones especiales para crear otro tipo de gráficos de residuales y que son útiles para identificar posibles anomalías en el modelo ajustado. A continuación las funciones para crear nuevos gráficos de residuales. residualPlots(model): dibuja una gráfica de los residuos de Pearson versus cada término del predictor lineal y los valores ajustados \\(\\hat{\\mu}_i\\). También entrega los resultados de una prueba de hipótesis para saber si se debe agregar un término cuadrático de cada variable. mmps o marginalModelPlots(model): dibuja una gráfica de la respuesta \\(y_i\\) versus cada covariable cuantitativa y los valores ajustados \\(\\hat{\\mu}_i\\), es una variación la propuesta de Cook and Weisberg (1997). Ejemplo Este ejemplo corresponde al ejemplo mostrado en el capítulo 6 de J. Fox and Weisberg (2019). En este ejemplo se desea ajustar un modelo de regresión para explicar la media de la variable prestige en función de las variables education, income y type, usando la base de datos Prestige del paquete car (John Fox, Weisberg, and Price 2023). library(car) prestige_mod1 &lt;- lm(prestige ~ education + income + type, data=Prestige) Para construir los gráficos de los residuos de Pearson versus cada término del predictor lineal y los valores ajustados \\(\\hat{\\mu}_i\\) se usa la siguiente instrucción (las=1 para poner los números vertical en el eje Y). residualPlots(prestige_mod1, las=1) ## Test stat Pr(&gt;|Test stat|) ## education -0.6836 0.495942 ## income -2.8865 0.004854 ** ## type ## Tukey test -2.6104 0.009043 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 De la figura anterior se observa lo siguiente: El gráfico de residuales vs education se asemeja a un “gráfico nulo”, en la que ningún patrón particular es aparente. El gráfico de residuales vs income presenta una curvatura. El gráfico de residuales vs type (var. cuali.) presenta una apariencia de un “gráfico nulo”, todas las cajas con a proximadamente el mismo centro y extensión. De los tres comentarios anteriores parece que falta el término \\(Income^2\\) en el predictor lineal. La sospecha de que falta el término \\(Income^2\\) se ve reforzada por la tabla que acompaña la salida de residualPlots(prestige_mod). En la línea para la variable income se tiene la prueba de hipótesis: \\(H_0:\\) no se necesita \\(Income^2\\), \\(H_A:\\) si se necesita \\(Income^2\\). El valor-P de esta prueba es 0.004854 y por lo tanto se justifica incluir \\(Income^2\\). Para dibujar los gráficos marginales de \\(y_i\\) versus cada covariable cuantitativa y los valores ajustados \\(\\hat{\\mu}_i\\) se usa la siguiente instrucción. marginalModelPlots(prestige_mod1, las=1) ## Warning in mmps(...): Interactions and/or factors skipped La línea de color azul es una regresión lowess entre \\(y_i\\) y la variable que está en el eje horizontal. La línea de color rojo es una regresión lowess entre \\(\\hat{\\mu}_i\\) y la variable que está en el eje horizontal. Según J. Fox and Weisberg (2019), si el modelo se ajusta bien a los datos, las líneas (azul y roja) estarán próximas, por el contrario, si las líneas difieren demasiado, es evidencia de que el modelo no explica bien los datos. De la figura anterior se observa que las líneas (azul y roja) difieren un poco en el panel de \\(y_i\\) versus income, eso refuerza lo observado antes de que sería bueno agregar un término I(income^2). prestige_mod2 &lt;- lm (prestige ~ education + income + I(income^2) + type, data=Prestige) residualPlots(prestige_mod2, las=1) ## Test stat Pr(&gt;|Test stat|) ## education -0.3844 0.70156 ## income 0.4760 0.63520 ## I(income^2) 2.1493 0.03426 * ## type ## Tukey test -2.1877 0.02869 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(prestige_mod1, prestige_mod2) ## Analysis of Variance Table ## ## Model 1: prestige ~ education + income + type ## Model 2: prestige ~ education + income + I(income^2) + type ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 93 4681.3 ## 2 92 4292.5 1 388.74 8.3317 0.004854 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Prueba de falta de ajuste Para el caso de la RLS, se quiere probar: \\[ \\begin{aligned} \\begin{cases} H_0:\\ E(Y_i) = E(Y\\vert X_i) = \\beta_0 + \\beta_1 X_i\\\\ \\\\ H_1:\\ E(Y_i) = E(Y\\vert X_i) \\neq \\beta_0 + \\beta_1 X_i \\end{cases} \\end{aligned} \\] Para aplicar esta prueba construye la tabla anova de la siguiente manera: Bajo la hipótesis nula \\({H_0:\\ E(Y_i) = \\beta_0 + \\beta_1x_i}\\), el estadístico se distribuye como una \\(F\\) con \\((m - 2)\\) y \\((n - m)\\) grados de libertad. Así, a un nivel de significancia \\({\\alpha}\\) se rechaza la hipótesis nula de que el modelo lineal es adecuado (en favor de la hipótesis de que el modelo lineal tiene falta de ajuste) si \\({F_0 &gt; F_{\\alpha, m - 2, n - m}}\\). Ejemplo 1 Suponga que se tienen los siguientes datos. # Example for section 4.5 Montgomery, Peck &amp; Vining (2006) x &lt;- c(1.0, 1.0, 2.0, 3.3, 3.3, 4.0, 4.0, 4.0, 4.7, 5.0, 5.6, 5.6, 5.6, 6.0, 6.0, 6.5, 6.9) y &lt;- c(10.84, 9.30, 16.35, 22.88, 24.35, 24.56, 25.86, 29.16, 24.59, 22.25, 25.90, 27.20, 25.61, 25.45, 26.56, 21.03, 21.46) ¿Es el modelo de regresión lineal simple apropiado para explicar \\(Y\\) en función de \\(X\\)? Solución Primero exploremos la relación entre las variables. plot(x=x, y=y, pch=20, cex=2, col=&quot;red&quot;) Luego vamos a ajustar el modelo. mod &lt;- lm(y ~ x) Luego construimos la tabla anova por medio de la función lack_fit_test del paquete model. Para instalar el paquete model propuesto por Hernandez and Usuga (2024) podemos usar el siguiente código. if (!require(&quot;remotes&quot;)) install.packages(&quot;remotes&quot;) remotes::install_github(&quot;fhernanb/model&quot;) Luego de esto ya podemos usar la función lack_fit_test. library(model) lack_fit_test(mod) ## Lack of fit test - Anova Table ## Sum Sq Df Mean Sq F value Pr(&gt;F) ## Regression 237.48 1 237.479 14.241 0.001839 ** ## Residuals 250.13 15 16.676 ## Lack of fit 234.57 8 29.321 13.188 0.001389 ** ## Pure error 15.56 7 2.223 ## Total 487.61 16 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 De la salida anterior tenemos un valor-P de 0.001389 con lo cual podemos decir que hay evidencias para rechazar \\(H_0\\). plot(x=x, y=y, pch=20, cex=2, col=&quot;red&quot;) abline(mod, lty=&quot;dashed&quot;, col=&quot;blue&quot;, lwd=2) Ejemplo 2 Suponga que se tienen los siguientes datos. # Example 9.6 Montgomery &amp; Runger (1996) x &lt;- c(1.0, 1.0, 2.0, 3.3, 3.3, 4.0, 4.0, 4.0, 5.0, 5.6, 5.6, 5.6, 6.0, 6.0, 6.5, 6.9) y &lt;- c(2.3, 1.8, 2.8, 1.8, 3.7, 2.6, 2.6, 2.2, 2.0, 3.5, 2.8, 2.1, 3.4, 3.2, 3.4, 5.0) ¿Es el modelo de regresión lineal simple apropiado para explicar \\(Y\\) en función de \\(X\\)? Solución Primero exploremos la relación entre las variables. plot(x=x, y=y, pch=20, cex=2, col=&quot;chartreuse3&quot;) Luego vamos a ajustar el modelo. mod &lt;- lm(y ~ x) Luego construimos la tabla anova por medio de la función lack_fit_test del paquete model. Para instalar el paquete model podemos usar el siguiente código. if (!require(&quot;remotes&quot;)) install.packages(&quot;remotes&quot;) remotes::install_github(&quot;fhernanb/model&quot;) Luego de esto ya podemos usar la función lack_fit_test. library(model) lack_fit_test(mod) ## Lack of fit test - Anova Table ## Sum Sq Df Mean Sq F value Pr(&gt;F) ## Regression 3.4928 1 3.4928 6.6645 0.02174 * ## Residuals 7.3372 14 0.5241 ## Lack of fit 4.3005 7 0.6144 1.4162 0.32882 ## Pure error 3.0367 7 0.4338 ## Total 10.8300 15 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 De la salida anterior tenemos un valor-P de 0.32882 con lo cual podemos decir que NO hay evidencias para rechazar \\(H_0\\). plot(x=x, y=y, pch=20, cex=2, col=&quot;chartreuse3&quot;) abline(mod, lty=&quot;dashed&quot;, col=&quot;mediumpurple2&quot;, lwd=2) References Cook, R. Dennis, and Sanford Weisberg. 1997. “Graphics for Assessing the Adequacy of Regression Models.” Journal of the American Statistical Association 92 (438): 490–99. https://doi.org/10.1080/01621459.1997.10474002. Fox, John, Sanford Weisberg, and Brad Price. 2023. Car: Companion to Applied Regression. https://r-forge.r-project.org/projects/car/. Fox, J., and W. Weisberg. 2019. An r Companion to Applied Regression. Third. SAGE Publications, Inc. Hernandez, Freddy, and Olga Usuga. 2024. Model: This Package Contains Useful Functions for Modeling Regresion. https://fhernanb.github.io/model. "],["diag2.html", "11 Diagnósticos parte II Matriz sombrero o hat ¿Qué es extrapolación oculta? Punto de balanceo Punto atípico (outlier) y punto influyente Uso de los residuales para detectar atípicos (outliers) Prueba de Bonferroni para detectar atípicos (outliers) Distancia de Cook DFFITS DFBETAS Estadística PRESS y \\(R^2\\) de predicción Valores de corte para diagnósticos", " 11 Diagnósticos parte II En este capítulo se presentan otras herramientas útiles para realizar diagnósticos. Matriz sombrero o hat La matriz sombrero o matriz Hat se define así: \\[ \\boldsymbol{H} = \\boldsymbol{X}(\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\] Esta matriz contiene en su diagonal las distancias relativas desde el centroide de los datos hasta cada uno de los puntos. En la siguiente figura se ilustra el concepto de distancia relativa entre el centroide (color rojo) de las variables explicativas y cada uno de los puntos para un caso con tres variables explicativas \\(x_1\\), \\(x_2\\) y \\(x_3\\). La cantidad \\(h_{ii}\\) se llama leverage y corresponde al elemento \\(i\\) de la diagonal de la matriz sombrero \\(\\boldsymbol{H}\\). Los valores de \\(h_{ii}\\) cumplen lo siguiente: siempre están entre \\(1/n\\) y 1. la suma \\(\\sum h_{ii}\\) es igual al número de \\(\\beta\\)’s del modelo (incluyendo \\(\\beta_0\\)). Si la observación \\(i\\)-ésima tiene un valor grande de \\(h_{ii}\\) significa que ella tiene valores inusuales de \\(\\boldsymbol{x}_i\\), mientras que valores pequeños de \\(h_{ii}\\) significa que la observación se encuentra cerca del centroide de los datos. La distancia \\(h_{ii}\\) no incluye información de la variable respuesta \\(y\\), solo de las covariables, esto se nota claramente en la fórmula de \\(\\boldsymbol{H}\\) dada arriba. Los valores \\(h_{ii}\\) se pueden obtener de dos formas: Construyendo la matriz \\(\\boldsymbol{H}\\). Ajustando el modelo de regresión mod y luego usando la función hatvalues(mod) o lm.influence(mod) sobre el modelo. ¿Para qué se usan los \\(h_{ii}\\) en la práctica? En el siguiente apartado se explicará el uso de los \\(h_{ii}\\). ¿Qué es extrapolación oculta? Suponga tenemos un modelo de regresión una variable respuesta y dos covariables \\(x_1\\) y \\(x_2\\). En la siguiente figura se ilustra los posibles datos desde una vista superior (sin ver los valores de \\(y\\)). Esa elipse o forma se llama Regressor Variable Hull (RVH) o cascarón de los datos. Una vez se tenga el modelo ajustado podríamos usar valores de \\(x_1\\) y \\(x_2\\) para estimar la media de \\(y\\). Lo ideal es usar el modelo para predecir la media de \\(y\\) con valores de \\(x_1\\) y \\(x_2\\) que se encuentren dentro del cascarón. Si tratamos de estimar la media de \\(y\\) para valores de las covariables fuera del cascarón, como en el caso del punto rojo, no podemos garantizar que el modelo tenga un buen desempeño debido a que el modelo no se entrenó con ese tipo de ejemplos. El problema de extrapolación oculta se presenta cuando tratamos de predecir información de \\(y\\) con covariables fuera del cascarón. La extrapolación oculta es fácil de identificarla cuando sólo se tiene dos covariables, pero, ¿cómo saber si se está haciendo extrapolación oculta cuando se tienen varias covariables. Supongamos que queremos saber si el vector de covariables \\(\\boldsymbol{x}_0=(1, x_1, x_2, \\ldots, x_p)^\\top\\) está o no dentro del cascarón, dicho de otra manera, ¿se cometería extrapolación oculta usando \\(\\boldsymbol{x}_0\\)?. Los pasos para determinar si \\(\\boldsymbol{x}_0\\) está o no dentro del cascarón son: Calcular la matriz \\(\\boldsymbol{H}\\). Obtener los valores \\(h_{ii}\\) de la matriz \\(\\boldsymbol{H}\\). Identificar \\(h_{max} = max\\{h_{11}, h_{22}, \\ldots, h_{nn}\\}\\). Calcular \\(h_{00} = \\boldsymbol{x}_0 (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{x}_0^\\top\\). Si \\(h_{00} &gt; h_{max}\\) el punto \\(\\boldsymbol{x}_0\\) está fuera del cascarón y se podría estár cometiendo extrapolación oculta. Ejemplo Calcular los valores \\(h_{ii}\\) para un modelo de regresión y ~ x + z con los siguientes datos. y &lt;- c(2, 3, 6, 5) x &lt;- c(3, 5, 6, 7) z &lt;- c(5, 4, 6, 3) Solución A seguir se muestran las tres formas para obtener los valores \\(h_{ii}\\). # Forma 1 X &lt;- cbind(1, x, z) H &lt;- X %*% solve(t(X) %*% X) %*% t(X) H ## [,1] [,2] [,3] [,4] ## [1,] 8.333333e-01 3.333333e-01 -3.552714e-15 -0.1666667 ## [2,] 3.333333e-01 3.333333e-01 -9.992007e-16 0.3333333 ## [3,] -2.664535e-15 -1.110223e-15 1.000000e+00 0.0000000 ## [4,] -1.666667e-01 3.333333e-01 -1.332268e-15 0.8333333 diag(H) ## [1] 0.8333333 0.3333333 1.0000000 0.8333333 # Forma 2 mod &lt;- lm(y ~ x + z) hatvalues(mod) ## 1 2 3 4 ## 0.8333333 0.3333333 1.0000000 0.8333333 # Forma 3 lm.influence(mod)$hat ## 1 2 3 4 ## 0.8333333 0.3333333 1.0000000 0.8333333 Reto para el lector Use la información del ejemplo anterior y determine si la observación con valores de \\(x=4\\) y \\(z=1\\) está o no dentro del cascarón de los datos, en otras palabras, determine si se podría cometer extrapolación oculta al usar el modelo ajustado con \\(x=4\\) y \\(z=1\\). Punto de balanceo Un punto de balanceo es una observación en el espacio de las predictoras pero alejada del resto de la muestra y que puede controlar ciertas propiedades del modelo ajustado. Este tipo de observaciones posiblemente no afecte los coeficientes de regresión estimados pero sí las estadísticas de resumen como el \\(R^2\\) y los errores estándar de los coeficientes estimados. Los puntos de balanceo son detectados mediante el análisis de los elementos de la diagonal principal de la matriz \\(\\boldsymbol{H}\\), los \\(h_{ii}\\) proporcionan una medida estandarizada de la distancia de la \\(i\\)-ésima observación al centro del espacio definido por las predictoras. Se asume que la observación \\(i\\) es un punto de balanceo si \\(h_{ii} &gt; 2p/n\\), pero si \\(2p/n &gt; 1\\), este criterio no funciona pues los \\(h_{ii}\\) siempre son menores que 1. Punto atípico (outlier) y punto influyente Los conceptos de atípico e influyente son diferentes y se definen así: Punto atípico (outlier): es una observación que es numéricamente distante del resto de los datos. Punto influyente: es una observación que tiene impacto en las estimativas del modelo. En la siguiente figura se ilustra la diferencia entre los conceptos de atípico e influyente. Dos preguntas que surgen al mirar la figura anterior son: ¿Cómo saber si un punto es un atípico (outlier)? ¿Cómo saber si un punto es influyente? Para saber si un punto es atípico (outlier) se puede usar la prueba de Bonferroni o los residuales \\(d_i\\) o \\(r_i\\), y para saber si un punto es influyente se puede utilizar Distancia de Cook. En las siguientes secciones se mostrará cómo identificar atípicos y puntos influyentes. Uso de los residuales para detectar atípicos (outliers) Una forma sencilla de detectar posibles observaciones atípicas (outliers) es usando las siguientes reglas: Un \\(d_i\\) grande, \\(|di| &gt; 3\\), es indicio de una observación potencialmente atípica. Un \\(r_i\\) grande, \\(|ri| &gt; 3\\), es indicio de una observación potencialmente atípica. Prueba de Bonferroni para detectar atípicos (outliers) El paquete car de John Fox, Weisberg, and Price (2023) tiene la función outlierTest para realizar una prueba de hipótesis de \\(H_0:\\) la observación \\(i\\)-ésima NO es un outlier. \\(H_1:\\) la observación \\(i\\)-ésima SI es un outlier. La estructura de la función outlierTest se muestra a continuación. outlierTest(model, cutoff=0.05, n.max=10, order=TRUE, labels=names(rstudent), ...) En la sección 11.3.1 del libro John Fox (2015) están los detalles de la prueba, se invita al lector para que los consulte. Ejemplo En este ejemplo se usará una base de datos que contiene medidas corporales para un grupo de estudiantes universitarios que vieron el curso de modelos de regresión en el año 2013. Abajo se muestra una figura ilustrativa de los datos. El objetivo es determinar si hay alguna observación que se pueda considerar como outlier cuando se ajusta un modelo de regresión para explicar el peso corporal en función de la estatura, circunferencia del cuello y circunferencia de la muñeca. Solución Primero vamos a ajustar el modelo y luego vamos a aplicar la prueba de Bonferroni para detectar outliers. url &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/medidas_cuerpo2&quot; datos &lt;- read.table(file=url, sep=&quot;\\t&quot;, header=TRUE) mod &lt;- lm(Peso ~ Estatura + circun_cuello + circun_muneca, data=datos) library(car) outlierTest(mod, cutoff=Inf, n.max=4) ## rstudent unadjusted p-value Bonferroni p ## 11 4.206280 0.00011567 0.006015 ## 52 -2.221747 0.03115300 NA ## 40 2.146046 0.03706400 NA ## 28 -2.043770 0.04660300 NA En la salida de arriba vemos las cuatro observaciones (n.max=4) que tienen los mayores valores de residual estudentizado \\(r_i\\). La observación ubicada en la línea 11 es la única con un valor-P muy pequeño y por lo tanto hay evidencias para considerar esa observación como un posible outlier. Es posible dibujar los resultados de la prueba para cada observación usando la función influenceIndexPlot del paquete car. En la siguiente figura se observa que sólo la observacion 11 es identificada como un posible outlier ya que su valor-P es muy pequeño. influenceIndexPlot(mod, vars=&quot;Bonf&quot;, las=1) Distancia de Cook Es una medida de cómo influye la observación \\(i\\)-ésima sobre la estimación de \\(\\boldsymbol{\\beta}\\) al ser retirada del conjunto de datos. Una distancia de Cook grande significa que una observación tiene un peso grande en la estimación de \\(\\boldsymbol{\\beta}\\). \\[ D_i = \\frac{\\sum_{j=1}^{n} (\\hat{y}_j - \\hat{y}_{j(i)} )^2 }{p \\hat{\\sigma^2}}, \\] donde la notación \\((i)\\) significa “sin la observación \\(i\\)-ésima”, eso quiere decir que \\(\\hat{y}_{j(i)}\\) es la estimación de \\(j\\)-ésima sin haber tenido en cuenta \\(i\\)-ésima observación en el ajuste del modelo. La cantidad \\(p=k+1\\) se refiere a todos los \\(\\beta\\)’s en el modelo (\\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\)). Una forma sencilla de calcular los valores de la distancia de Cook \\(D_i\\) es por medio de la expresión. \\[ D_i = \\frac{d_i^2}{k+1} \\times \\frac{h_{ii}}{1-h_{ii}}, \\] donde \\(d_i\\) corresponde al residual estandarizado. Otra forma de obtener las distancias de Cook \\(D_i\\) es ajustando el modelo de interés mod y luego aplicar la función cooks.distance sobre el modelo. Son puntos influyentes las observaciones que presenten \\(D_i&gt;1\\). Ejemplo: En este ejemplo se usará una base de datos que contiene medidas corporales para un grupo de estudiantes universitarios que vieron el curso de modelos de regresión en el año 2013. Abajo se muestra una figura ilustrativa de los datos. El objetivo es ajustar un modelo de regresión para explicar el peso promedio en función de la circunferencia de la muñeca, cuello y estatura. Luego de ajustar el modelo se deben identificar los posibles estudiantes influyentes y el efecto de ellos en el modelo. Solución Lo primero es cargar los datos en nuestra sesión de R. url &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/medidas_cuerpo2&quot; datos &lt;- read.table(file=url, sep=&quot;\\t&quot;, header=TRUE) head(datos, n=5) ## Ano Semestre Peso Sexo Estatura circun_cuello circun_muneca ## 1 2020 1 47.6 F 1.57 29.5 13.9 ## 2 2020 1 68.1 M 1.66 38.4 16.0 ## 3 2020 1 68.0 M 1.90 36.5 16.6 ## 4 2020 1 80.0 M 1.76 38.0 17.1 ## 5 2020 1 68.1 M 1.83 38.0 17.1 Antes de ajustar cualquier modelo es fundamental hacer un análisis descriptivo de los datos. Comenzaremos construyendo un diagrama de dispersión con ggpairs. library(&quot;GGally&quot;) ggpairs(datos[, c(&quot;Peso&quot;, &quot;Estatura&quot;, &quot;circun_cuello&quot;, &quot;circun_muneca&quot;)]) De la figura anterior se observa que hay un punto que se aleja de la nube, es un estudiante que pesa un poco más de 100 kilogramos. Vamos ahora a ajustar nuestro primer modelo. mod1 &lt;- lm(Peso ~ Estatura + circun_cuello + circun_muneca, data=datos) summary(mod1) ## ## Call: ## lm(formula = Peso ~ Estatura + circun_cuello + circun_muneca, ## data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.277 -3.141 -0.364 2.980 20.065 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -57.109 14.176 -4.029 0.000199 *** ## Estatura 10.773 10.492 1.027 0.309674 ## circun_cuello 2.302 0.387 5.949 3e-07 *** ## circun_muneca 1.428 1.048 1.362 0.179546 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.822 on 48 degrees of freedom ## Multiple R-squared: 0.7692, Adjusted R-squared: 0.7548 ## F-statistic: 53.32 on 3 and 48 DF, p-value: 2.585e-15 Como en la tabla anterior aparecen variables que nos son significativas vamos a realizar una selección de variables usando el paquete mixlm creado por Liland (2023). Vamos a realizar una selección de variables de manera que sólo queden variables significativas con un \\(\\alpha=0.04\\). mod2 &lt;- mixlm::backward(mod1, alpha=0.04) ## Backward elimination, alpha-to-remove: 0.04 ## ## Full model: Peso ~ Estatura + circun_cuello + circun_muneca ## ## Step RSS AIC R2pred Cp F value Pr(&gt;F) ## Estatura 1 1662.6 186.17 0.71720 3.0543 1.0543 0.30967 ## circun_muneca 2 1761.7 187.18 0.71701 3.9787 2.9212 0.09375 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod2) ## ## Call: ## lm(formula = Peso ~ circun_cuello, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.4526 -3.7980 -0.5279 3.6603 18.9962 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -37.1552 8.4079 -4.419 5.33e-05 *** ## circun_cuello 2.9002 0.2368 12.250 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## s: 5.936 on 50 degrees of freedom ## Multiple R-squared: 0.7501, ## Adjusted R-squared: 0.7451 ## F-statistic: 150.1 on 1 and 50 DF, p-value: &lt; 2.2e-16 En la siguiente tabla se comparan los modelos 1 y 2 ajustados hasta ahora. De la tabla anterior podemos destacar lo siguiente: El intercepto estimado cambia bastante. En el modelo 2 la variable cuello aumenta su efecto. El \\(R^2\\) se mantiene constante. La varianza de los errores disminuye, eso significa que el modelo 2 deja menos sin explicar. Vamos ahora a crear el diagrama de dispersión con el modelo ajustado. # Para construir el grafico de dispersion with(datos, plot(x=circun_cuello, y=Peso, pch=19, las=1, xlab=&quot;Circunferencia cuello (cm)&quot;, ylab=&quot;Peso (Kg)&quot;)) # Ahora agregamos la linea de tendencia abline(mod2, lwd=3, col=&#39;blue2&#39;) # por ultimo un texto con la ecuacion o modelo ajustado text(x=34, y=95, expression(hat(Peso) == -44.61 + 3.10 * C.cuello), col=&#39;blue3&#39; ) De la figura anterior vemos que hay un estudiante (el de 100 kilos de peso) que está muy alejado de la recta de regresión. Vamos a calcular las distancias de Cook para las observaciones del modelo 2 así: cooks.distance(mod2) ## 1 2 3 4 5 6 ## 7.804662e-04 1.938685e-02 1.563287e-04 2.219833e-02 1.128458e-02 2.524701e-03 ## 7 8 9 10 11 12 ## 4.092417e-03 1.222835e-01 7.151786e-04 1.149054e-03 4.808540e-01 3.722745e-02 ## 13 14 15 16 17 18 ## 3.938290e-02 1.849408e-04 1.029180e-02 6.228690e-04 1.681916e-02 9.700850e-06 ## 19 20 21 22 23 24 ## 7.536443e-03 1.538284e-03 6.643811e-03 7.225225e-03 3.624830e-03 1.331368e-02 ## 25 26 27 28 29 30 ## 1.895684e-03 8.745440e-05 4.836285e-04 2.002813e-02 1.742378e-03 4.885748e-03 ## 31 32 33 34 35 36 ## 6.336181e-04 1.163887e-02 1.529500e-02 5.686460e-04 7.534900e-03 1.800503e-02 ## 37 38 39 40 41 42 ## 1.347941e-02 6.164670e-04 9.932676e-03 5.753702e-02 3.519145e-02 2.780173e-04 ## 43 44 45 46 47 48 ## 5.921498e-03 4.435951e-03 2.649280e-03 2.559527e-02 3.105562e-02 2.775930e-04 ## 49 50 51 52 ## 3.554232e-03 4.082713e-03 6.130210e-01 3.859845e-02 Es mejor representar las distancias de Cook en forma gráfica para identificar los posible puntos influyentes así: cutoff &lt;- 4 / (26-2-2) # Cota plot(mod2, which=4, cook.levels=cutoff, las=1) abline(h=cutoff, lty=&quot;dashed&quot;, col=&quot;dodgerblue2&quot;) De esta figura es claro que las observaciones 11 y 8 tienen \\(D_i\\) por encima de la cota y se consideran observaciones influyentes. Otra forma de dibujar las distancias de Cook es por medio de la función influenceIndexPlot del paquete car. library(car) influenceIndexPlot(mod2, vars=&quot;Cook&quot;) Ahora vamos a revisar los residuales del modelo 2. par(mfrow=c(2, 2)) plot(mod2, col=&#39;deepskyblue4&#39;, pch=19) De la anterior figura vemos que las observaciones 8, 11 y 13 son identificadas por tener valores de residuales grandes. Vamos ahora a identificar las observaciones 8, 11, 12 y 13 en un diagrama de dispersión. La observación 11 es un hombre que pesa más de 100 kilos y que solo mide 1.79 metros. Las observaciones 8, 12 y 13 son mujeres con las mayores diferencias entre \\(y_i\\) y \\(\\hat{y}_i\\), para ellas el modelo sobreestima el peso corporal. En la siguiente tabla se muestran los resultados de ajustar nuevamente el modelo 2 bajo tres situaciones: con todas las observaciones, sin la observación 11 y sin las observaciones 8, 11, 12 y 13. De la tabla vemos que la observación 11 es muy influyente, al sacar esa observación el modelo aumenta su \\(R^2\\) y disminuye su \\(\\sigma^2\\). De la última columna se observa el mismo comportamiento, \\(R^2\\) aumenta y disminuye su \\(\\sigma^2\\) al sacar todas las observaciones sospechosas. Pero, ¿cuál modelo debo usar como modelo final? ¿El modelo 2, el modelo 2 sin la obs 11 o el modelo 2 sin las obs 8, 11, 12 y 13? Lo que se recomienda es que el analista se asesore de un experto en el área de aplicación para que juntos estudien esas observaciones sospechosas. Si hay una razón de peso para considerarlas como observaciones atípicas, ellas deben salir del modelo. Si por el contrario, no hay nada raro con las observaciones, ellas deben seguir en el modelo. Las observaciones sospechosas NO se deben sacar inmediatamente del modelo. Antes se deben estudiar para ver si hay algo raro con ellas, en caso afirmativo se sacan de la base y se ajusta nuevamente el modelo. Una observación influyente NO es una observación mala en el modelo. Al contrario, ella es una observación clave en el ajuste porque “lidera” la estimación. Una observación que no es influyente es una observación que estando presente o no, el modelo ajustado no se ve afectado. DFFITS Es una medida de la influencia de la \\(i\\)-ésima observación sobre el valor predicho o ajustado, la expresión para calcularla es la siguiente: \\[ DFFITS_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{\\sqrt{s_{(i)}^2 h_{ii}}}. \\] Una observación será influyente si \\(|DFFITS| &gt; 2 \\sqrt{p/n}\\). La función en R para obtener los \\(DFFITS_i\\) es dffits. DFBETAS Es una medida que indica cuánto cambia el coeficiente de regresión, en unidades de desviación estándar, si se omitiera la \\(i\\)-ésima observación. \\[ DFBETAS_{i,j} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{\\sqrt{s_{(i)}^2 C_{jj}}}, \\] donde \\(C_{jj}\\) es \\(j\\)-ésimo elemento de la diagonal de \\((\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\). Una observación será influyente si \\(|DFBETAS| &gt; 2 \\sqrt{n}\\). La función en R para obtener los \\(DFBETAS_{i,j}\\) es dfbetas. Ejemplo Considere los datos que se muestran a continuación y ajuste un modelo de regresión para explicar \\(y\\) en función de \\(x\\). Luego identifique las observaciones influyentes usando las medidas DFFITS y DFBETAS. x &lt;- c(2, 5, 3, 4, 7) y &lt;- c(5, 9, 8, 7, 19) Solución Vamos a crear un diagrama de dispersión para los datos del ejemplo. plot(x=x, y=y, pch=19, col=&quot;tomato&quot;, las=1) text(1:5, x=x-0.1, y=y, cex=0.5) Lo siguiente es ajustar el modelo de regresión de interés. mod &lt;- lm(y ~ x) Vamos a calcular DFFITS usando la función dffits sobre el modelo ya ajustado. dffits(mod) ## 1 2 3 4 5 ## 0.5838725 -0.7757081 0.4112718 -0.4420348 5.8081955 De la salida anterior vemos que la observación 5 es la única que tiene un \\(|DFFITS|\\) mayor a \\(2\\sqrt{\\frac{p}{n}}=2\\sqrt{\\frac{2}{5}}=1.26\\). Por esa razón podemos considerar a la observación 5 como una observación influyente. Vamos a calcular DFBETAS usando la función dfbetas sobre el modelo ya ajustado. dfbetas(mod) ## (Intercept) x ## 1 0.56194908 -0.45993179 ## 2 0.03603076 -0.32706727 ## 3 0.34558823 -0.23527935 ## 4 -0.21367111 0.05104178 ## 5 -3.42671836 4.94865731 De la salida anterior vemos nuevamente que la observación 5 tiene la mayor influencia sobre \\(\\hat{\\beta}_0\\) y sobre \\(\\hat{\\beta}_1\\). Como ambos valores de \\(|DFBETAS|\\) son mayores a \\(\\frac{2}{\\sqrt{n}}=\\frac{2}{\\sqrt{5}}=0.89\\), podemos considerar a la observación 5 como una observación influyente. Existe otra función llamda dfbeta (sin la letra s), veamos para qué sirve esta función. dfbeta(mod) ## (Intercept) x ## 1 1.93127413 -0.34826255 ## 2 0.09555985 -0.19111969 ## 3 1.16424116 -0.17463617 ## 4 -0.67017865 0.03527256 ## 5 -4.69729730 1.49459459 En conclusión, La función dfbetas entrega \\(\\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{\\sqrt{s_{(i)}^2 C_{jj}}}\\). La función dfbeta entrega \\(\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}\\). Estadística PRESS y \\(R^2\\) de predicción Es una medida de lo BIEN que el modelo logra predecir \\(\\hat{y}\\) para nuevas observaciones. \\[ PRESS = \\sum_{i=1}^{n} e_{(i)}^2 = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(i)})^2 = \\sum_{i=1}^{n} \\left( \\frac{e_i}{1-h_{ii}} \\right)^2 \\] El \\(R^2\\) de predicción es una medida del porcentaje de la variabilidad explicada por el modelo para nuevas observaciones. El \\(R^2\\) de predicción se define en función de la estadística PRESS así: \\[ R^2 \\, \\text{de predicción} = 1 - \\frac{PRESS}{SST} \\] Las funciones mostradas abajo fueron creadas por Tom Hopper y sirven para obtener la estadística PRESS y el \\(R^2\\) de predicción. PRESS &lt;- function(linear.model) { # calculate the predictive residuals pr &lt;- residuals(linear.model) / (1-lm.influence(linear.model)$hat) # calculate the PRESS PRESS &lt;- sum(pr^2) return(PRESS) } pred_r_squared &lt;- function(linear.model) { #&#39; Use anova() to get the sum of squares for the linear model lm.anova &lt;- anova(linear.model) #&#39; Calculate the total sum of squares tss &lt;- sum(lm.anova$&#39;Sum Sq&#39;) # Calculate the predictive R^2 pred.r.squared &lt;- 1-PRESS(linear.model)/(tss) return(pred.r.squared) } Ejemplo Como ilustración vamos a usar los datos del ejemplo 3.1 del libro de (E. &amp;. V. Montgomery D. &amp; Peck 2006). En el ejemplo 3.1 los autores ajustaron un modelo de regresión lineal múltiple para explicar el Tiempo necesario para que un trabajador haga el mantenimiento y surta una máquina dispensadora de refrescos en función de las variables Número de Cajas y Distancia. ¿Cuál es el valor de la estadística PRESS y del \\(R^2\\) de predicción? Solución Los datos del ejemplo están disponibles en el paquete MPV (por los apellidos de los autores). A continuación el código para cargar los datos y ajustar el modelo de interés. require(MPV) colnames(softdrink) &lt;- c(&#39;tiempo&#39;, &#39;cantidad&#39;, &#39;distancia&#39;) mod &lt;- lm(tiempo ~ cantidad + distancia, data=softdrink) A continuación se muestran las dos medidas de interés. PRESS(mod) ## [1] 459.0393 pred_r_squared(mod) ## [1] 0.9206438 De la salida anterior se observa que el \\(R^2\\) de predicción toma un valor de 0.9206, esto indica que el modelo mod tiene una buena capacidad de predicción para nuevas observaciones. Valores de corte para diagnósticos John Fox (2015) en la sección 11.5 hace una recopilación de los puntos de corte que se podrían usar para identificar observaciones con alguna anomalía al ajustar un modelo de regresión. En el siguiente listado se muestran esos puntos de corte pero se recomienda al lector para que revise John Fox (2015) y conozca unas recomendaciones adicionales sobre los puntos de corte. Observaciones con valores de \\(h_{ii}\\) que excedan dos veces el valor \\(\\bar{h} = (k+1)/n\\) podrían ser observaciones atípicas o outliers. Este punto de corte se obtiene como una aproximación para identificar el 5% de las observaciones más extremas si las \\(X\\)’s se distribuyen normal multivariada con \\(k\\) y \\(n-k-1\\) grandes (\\(k\\) se refiere al número de \\(\\beta\\)’s diferentes al intercepto). Como los residuales estandarizados \\(d_i\\), tienen media cero y varianza aproximadamente unitaria, un residual estandarizado grande, \\(|d_i| &gt; 3\\), indica que se trata de un valor atípico potencial. Como los residuales studentizados \\(r_i\\) tienen distribución \\(t\\)-student, es decir, \\(t_{n-k-2}\\), se podrían considerar como posibles residual atípico aquel cuyo valor cumpla \\(|r_i| &gt; 2\\). Una observación con un residual PRESS grande se puede considerar como una observación influyente. Una observación se puede considerar como influyente si su distancia de Cook cumple que \\(D_i &gt; 4/(n-k-1)\\), donde \\(k\\) se refiere al número de \\(\\beta\\)’s diferentes al intercepto. Una observación se puede considerar como influyente si valor de DFFITS cumple que \\(|DFFITS_i| &gt; 2 \\sqrt{\\frac{k+1}{n-k-1}}\\). References Fox, John. 2015. Applied Regression Analysis and Generalized Linear Models. Sage Publications. Fox, John, Sanford Weisberg, and Brad Price. 2023. Car: Companion to Applied Regression. https://r-forge.r-project.org/projects/car/. Liland, Kristian Hovde. 2023. Mixlm: Mixed Model ANOVA and Statistics for Education. https://github.com/khliland/mixlm/. Montgomery, E. &amp; Vining, D. &amp; Peck. 2006. Introducción Al Análisis de Regresión Lineal. 3ed ed. México: Cecsa. "],["homo.html", "12 Pruebas de Homocedasticidad Breusch-Pagan Test White test Score test for nonconstant error variance Goldfeld-Quandt Test Harrison-McCabe test", " 12 Pruebas de Homocedasticidad En este capítulo se presentan varias pruebas para explorar si se cumple el supuesto de homocedasticidad de los errores en regresión lineal. En las prueba mostradas a continuación se estudian las siguientes hipótesis. \\[\\begin{align*} H_0 &amp;: \\text{los errores tienen varianza constante.} \\\\ H_1 &amp;: \\text{los errores no tienen varianza constante.} \\end{align*}\\] Breusch-Pagan Test Esta prueba fue propuesta por Breusch and Pagan (1979) y consiste en ajustar un modelo de regresión lineal con variable respuesta dada por residuales del modelo original al cuadrado \\(e_i^2\\) y como covariables las variables del modelo original. Por ejemplo, si se tienen \\(k=2\\) covariables para explicar a \\(Y\\), entonces el modelo de regresión para estudiar la homocedasticidad es: \\[ \\hat{e}_i^2 = \\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 + u \\] Si se concluye que \\(\\delta_1=\\delta_2=0\\), significa que los residuales no son función de las covariables del modelo. El estadístico en esta prueba está dado por \\(n \\times R^2\\) y bajo la hipótesis nula verdadera, el estadístico tiene distribución \\(\\chi^2_k\\). La función bptest del paquete lmtest Hothorn et al. (2022) implementa esta prueba. Ejemplo Simule un conjunto de datos donde se viole la hipótesis de varianza constante (de los \\(e_i\\) o de las \\(y_i\\)) y aplique las pruebas de hipótesis para ver si son capaces de detectar la violación del supuesto de homocedasticidad. Solución En el código mostrado a continuación se simulan observaciones en las cuales la varianza de \\(e_i\\) no es constante ya que para generar los datos se usa la instrucción ei &lt;- rnorm(n=n, sd=x2), es decir que la varianza depende de la variable \\(x_2\\). gen_data &lt;- function(n) { x1 &lt;- rpois(n, lambda=5) x2 &lt;- rbinom(n, size=6, prob=0.4) ei &lt;- rnorm(n=n, sd=x2) y &lt;- -3 + 2 * x1 + 4 * x2 + ei data.frame(y, x1, x2) } n &lt;- 200 datos &lt;- gen_data(n=n) mod &lt;- lm(y ~ x1 + x2, data=datos) # Modelo de interes Vamos a aplicar la prueba de forma manual. ei &lt;- resid(mod) fit &lt;- lm(ei^2 ~ x1 + x2, data=datos) # Modelando ei^2 ~ x1 + x2 R2 &lt;- summary(fit)$r.squared k &lt;- 2 estadistico &lt;- n * R2 valorP &lt;- pchisq(q=estadistico, df=k, lower.tail=FALSE) cbind(estadistico, valorP) ## estadistico valorP ## [1,] 29.23532 4.483652e-07 Vamos a aplicar la prueba de forma automática con la función bptest. library(lmtest) bptest(mod) ## ## studentized Breusch-Pagan test ## ## data: mod ## BP = 29.235, df = 2, p-value = 4.484e-07 De la salida anterior se observa que el valor-P es menor que el nivel de significancia usual de 5%, por lo tanto, hay evidencias para decir que no se cumple la homocedasticidad de los \\(e_i\\). White test El test de Breusch-Pagan sólo detecta formas lineales de heterocedasticidad. Para resolverlo, el test de White, propuesto por White (1980), permite contrastar no linealidades utilizando los cuadrados y los productos cruzados de todos los regresores. Si \\(k=2\\) el test de White crea el siguiente modelo de regresión: \\[ \\hat{e}_i^2 = \\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 + \\delta_3 x_1 x_2 + \\delta_4 x_1^2 + \\delta_5 x_2^2 + u \\] Este test se puede implementar por medio de la función bptest pero especificando los términos no lineales de la expresión anterior. Ejemplo Aplicar White test para los datos simulados del ejemplo anterior. Solución Para aplicar el test se usa el argumento varformula y se escribe la fórmula con los términos no lineales \\(\\delta_3 x_1 x_2 + \\delta_4 x_1^2 + \\delta_5 x_2^2\\), los términos lineales están por defecto. bptest(mod, varformula = ~ x1 * x2 + I(x1^2) + I(x2^2), data=datos) ## ## studentized Breusch-Pagan test ## ## data: mod ## BP = 35.551, df = 5, p-value = 1.168e-06 Como el valor-P es pequeño entonces hay evidencias para rechazar la hipótesis de homocedasticidad. La prueba se puede también realizar de forma manual, a continuación se muestra el procedimiento. fit &lt;- lm(resid(mod)^2 ~ x1 + x2 + x1 * x2 + I(x1^2) + I(x2^2), data=datos) R2 &lt;- summary(fit)$r.squared estadistico &lt;- n * R2 valorP &lt;- pchisq(q=estadistico, df=5, lower.tail=FALSE) cbind(estadistico, valorP) ## estadistico valorP ## [1,] 35.55051 1.168159e-06 Score test for nonconstant error variance Esta prueba sirve para estudiar la hipótesis nula de varianza constante de los errores frente a la hipótesis alternativa de que la varianza de los errores cambia con el nivel de la respuesta o con alguna combinación lineal de los predictores. La función ncvTest del paquete car John Fox, Weisberg, and Price (2023) implementa esta prueba. Ejemplo Aplicar Score test para los datos simulados del ejemplo anterior. Solución library(car) ncvTest(mod) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 11.3376, Df = 1, p = 0.00075954 Goldfeld-Quandt Test Este test está implementado en la función gqtest del paquete lmtest. Harrison-McCabe test Este test está implementado en la función hmctest del paquete lmtest. References Breusch, Trevor S, and Adrian R Pagan. 1979. “A Simple Test for Heteroscedasticity and Random Coefficient Variation.” Econometrica: Journal of the Econometric Society, 1287–94. Fox, John, Sanford Weisberg, and Brad Price. 2023. Car: Companion to Applied Regression. https://r-forge.r-project.org/projects/car/. Hothorn, Torsten, Achim Zeileis, Richard W. Farebrother, and Clint Cummins. 2022. Lmtest: Testing Linear Regression Models. https://CRAN.R-project.org/package=lmtest. White, Halbert. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.” Econometrica: Journal of the Econometric Society, 817–38. "],["indep.html", "13 Pruebas de independencia de los errores Durbin-Watson test Breusch-Godfrey Test", " 13 Pruebas de independencia de los errores En este capítulo se presentan varias pruebas para explorar si se cumple el supuesto de independencia de los errores en regresión lineal. En las prueba mostradas a continuación se estudian las siguientes hipótesis. \\[\\begin{align*} H_0 &amp;: \\text{los errores son independientes.} \\\\ H_1 &amp;: \\text{los errores no son independientes.} \\end{align*}\\] Durbin-Watson test La función dwtest del paquete lmtest (Hothorn et al. 2022) implementa esta prueba. En esta publicación de StackOverFlow hay una excelente discusión sobre la prueba, se invita al lector a explorar la publicación. Ejemplo El siguiente ejemplo fue tomado de la documentación de la función dwtest. ## generate regressor x &lt;- rep(c(-1, 1), 50) ## generate the AR(1) error terms with parameter rho = 0 (white noise) err1 &lt;- rnorm(100) ## generate dependent variable y1 &lt;- 1 + x + err1 library(lmtest) mod1 &lt;- lm(y1 ~ x) dwtest(mod1) ## perform Durbin-Watson test ## ## Durbin-Watson test ## ## data: mod1 ## DW = 2.0407, p-value = 0.6204 ## alternative hypothesis: true autocorrelation is greater than 0 plot(residuals(mod1), pch=19, col=&quot;deepskyblue1&quot;) ## generate the AR(1) error terms with parameter rho = 0.9 respectively err2 &lt;- stats::filter(x=err1, filter=0.9, method=&quot;recursive&quot;) ## generate dependent variable y2 &lt;- 1 + x + err2 mod2 &lt;- lm(y2 ~ x) dwtest(mod2) ## perform Durbin-Watson test ## ## Durbin-Watson test ## ## data: mod2 ## DW = 0.27804, p-value &lt; 2.2e-16 ## alternative hypothesis: true autocorrelation is greater than 0 plot(residuals(mod2), pch=19, col=&quot;tomato&quot;) Breusch-Godfrey Test La función bgtest del paquete lmtest Hothorn et al. (2022) implementa esta prueba. Ejemplo Usando los datos el ejemplo anterior aplicar la prueba Breusch-Godfrey. library(lmtest) mod1 &lt;- lm(y1 ~ x) bgtest(mod1) ## perform Durbin-Watson test ## ## Breusch-Godfrey test for serial correlation of order up to 1 ## ## data: mod1 ## LM test = 0.051656, df = 1, p-value = 0.8202 mod2 &lt;- lm(y2 ~ x) bgtest(mod2) ## perform Durbin-Watson test ## ## Breusch-Godfrey test for serial correlation of order up to 1 ## ## data: mod2 ## LM test = 73.866, df = 1, p-value &lt; 2.2e-16 References Hothorn, Torsten, Achim Zeileis, Richard W. Farebrother, and Clint Cummins. 2022. Lmtest: Testing Linear Regression Models. https://CRAN.R-project.org/package=lmtest. "],["mod_poli.html", "14 Modelos polinomiales Modelos polinomiales de una o varias variables Funciones I() y poly() Modelos polinomiales con splines Función bs Función lowess Función loess Optimización con superficies de respuesta Polinomios ortogonales", " 14 Modelos polinomiales En este capítulo se presenta una descripción breve de como ajustar modelos polinomiales con R. Modelos polinomiales de una o varias variables Cuando se construyen modelos para explicar la media de una variable respuesta \\(Y\\), a veces no basta con incluir las covariables \\(X_1, \\ldots, X_k\\) sino que es necesario incluir potencias de esas variables para mejorar el modelo. En las siguientes secciones se muestra como incluir potencias de las covariables en un modelo. Al incluir en un modelo lineal una potencia de una variables \\(X\\) el modelo sigue siendo lineal. El término lineal se refiere a los parámetros \\(\\beta\\) y no a las covariables. Funciones I() y poly() Las funciones I() y poly() son utilizadas para incluir elementos polinomiales en un modelo de regresión. Use I() para incluir un término específico en un modelo, por ejemplo, I(x^3) indica que queremos solo el término \\(x^3\\). Use poly() para incluir todos los términos de un polinomio hasta cierto grado, por ejemplo, poly(x, degree=3) indica que queremos incluir \\(x\\), \\(x^2\\) y \\(x^3\\). Ejemplo Como ilustración vamos a usar los datos del ejemplo 7.1 del libro de Montgomery, Peck and Vining (2003). En el ejemplo 7.1 se busca crear un modelo que explique la resistencia a la tensión de una bolsa en función del porcentaje de madera dura. A continuación se muestran los datos usados en el ejemplo. conc &lt;- c(1, 1.5, 2, 3, 4, 4.5, 5, 5.5, 6, 6.5, 7, 8, 9, 10, 11, 12, 13, 14, 15) resis &lt;- c(6.3, 11.1, 20, 24, 26.1, 30, 33.8, 34, 38.1, 39.9, 42, 46.1, 53.1, 52, 52.5, 48, 42.8, 27.8, 21.9) datos &lt;- data.frame(concentracion=conc, resistencia=resis) El siguiente código es usado para construir el diagrama de dispersión entre las variables resistencia y concentración. library(ggplot2) ggplot(datos, aes(x=concentracion, y=resistencia)) + geom_point() + theme_light() De este diagrama se ve claramente que hay una relación de tipo no lineal entre las variables. ¿Será mejor un modelo de grado 2 que un modelo de grado 1? Vamos a ajustar ambos modelos y luego los comparamos, el elemento cuadrático lo vamos a crear usando la función I(). mod1 &lt;- lm(resistencia ~ concentracion, data=datos) mod2 &lt;- lm(resistencia ~ concentracion + I(concentracion^2), data=datos) Para hacer una comparación de ambos modelos vamos a agregar al diagrama de dispersión original la recta y la curva asociadas a los modelos mod1 y mod2 respectivamente. ggplot(datos, aes(x=concentracion, y=resistencia)) + geom_point() + geom_smooth(method=&#39;lm&#39;, formula=y~x, se=FALSE, col=&#39;dodgerblue1&#39;) + geom_smooth(method=&#39;lm&#39;, formula=y~x+I(x^2), se=FALSE, col=&#39;tomato&#39;) + theme_light() De la figura anterior se observa claramente que el modelo lineal no es capaz de explicar los datos, se observan zonas donde el mod1 siempre sub-estima y otras zonas donde siempre sobre-estima. Vamos a comparar ahora los modelos por medio de un análisis de varianza, el código para hacer esto es el siguiente. anova(mod1, mod2) ## Analysis of Variance Table ## ## Model 1: resistencia ~ concentracion ## Model 2: resistencia ~ concentracion + I(concentracion^2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 17 2373.46 ## 2 16 312.64 1 2060.8 105.47 1.894e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El valor-P de la tabla anterior nos indica que es mejor mod2. A continuación se analizan los residuales (\\(e_i\\) vs \\(\\hat{y}_i\\)) para ambos modelos. par(mfrow=c(1, 2)) plot(mod1, which=1, caption=&#39;Modelo lineal&#39;) plot(mod2, which=1, caption=&#39;Modelo cuadratico&#39;) En la parte izquierda de la figura anterior se ve que para el modelo lineal mod1 los residuales presentan una curvatura evidente, esto significa que falta un elemento de grado dos en la estructura del modelo mod1. Al lado derecho de la figura están los residuales para el modelo cuadrático, de esta figura se observa que los residuales son menores (en valor absoluto) que los residuales del mod1 y que no presentan un patrón claro como en el caso anterior. Otro punto a favor del modelo cuadrático es su \\(R^2_{Adj}\\) que es de 0.897 frente al 0.265 del modelo lineal. De lo anterior se concluye que es mejor el modelo cuadrático para explicar la resistencia en función de la concentración, el modelo ajustado está dado en la siguiente expresión. \\[\\begin{align} {Resi}_i &amp;\\sim N(\\hat{\\mu}_i, \\sigma^2), \\\\ \\hat{\\mu}_i &amp;= -6.674 + 11.764 \\times Conc - 0.635 \\times Conc^2, \\\\ \\hat{\\sigma} &amp;= 4.42 \\end{align}\\] Modelos polinomiales con splines Los splines ofrecen una forma útil de ajustar un modelo cuando los datos se comportan en forma distinta en diferentes partes del rango de las \\(X´s\\). Los splines son polinomios de orden \\(k\\) por segmentos y los puntos de unión se llaman nodos o knots. En la siguiente figura se muestra un spline hecho de un listón de madera y con nodos de hierro para lograr una curvatura deseada. Un spline cúbico básico con \\(h\\) nodos en los puntos \\(t_1, t_2, \\ldots, t_h\\) se puede escribir como: \\[ E(Y) = \\sum_{j=0}^{3} \\beta_{0j} x^j + \\sum_{i=1}^{h} \\beta_{i} (x-t_i)^3_{+} \\] donde \\[ (x-t_i)^3_+ = \\begin{cases} (x-t_i)^3 &amp;\\mbox{if } x &gt; t_i \\\\ 0 &amp; \\mbox{if } x \\leq t_i \\end{cases} \\] Ejemplo Como ilustración vamos a usar los datos del ejemplo 7.2 del libro de E. &amp;. V. Montgomery D. &amp; Peck (2006). En el ejemplo 7.2 se busca crear un modelo que explique la caída de voltaje en función del tiempo por medio dos modelos: un modelo polinomial de grado tres. un spline cúbico con dos nodos en \\(t=6.5\\) y \\(t=13\\). Los datos utilizados en el ejemplo se muestran a continuación. drop &lt;- c(8.33, 8.23, 7.17, 7.14, 7.31, 7.60, 7.94, 8.30, 8.76, 8.71, 9.71, 10.26, 10.91, 11.67, 11.76, 12.81, 13.30, 13.88, 14.59, 14.05, 14.48, 14.92, 14.37, 14.63, 15.18, 14.51, 14.34, 13.81, 13.79, 13.05, 13.04, 12.60, 12.05, 11.15, 11.15, 10.14, 10.08,9.78,9.80,9.95,9.51) time &lt;- seq(from=0, to=20, by=0.5) datos &lt;- data.frame(time=time, drop=drop) Usando los datos anteriores podemos construir un diagrama de dispersión para entender la relación de las variables y la ubicación de los posibles nodos (sitios donde hay cambio de curvatura). plot(datos, ylab=&quot;Voltage drop&quot;, xlab=&quot;Time (seconds)&quot;, pch=19, ylim=c(0, 15), las=1) abline(v=6.5, lty=&quot;dotted&quot;, col=&#39;tomato&#39;) abline(v=13, lty=&quot;dotted&quot;, col=&#39;tomato&#39;) text(x=6.5, y=0.3, &#39;t=6.5&#39;, col=&#39;tomato&#39;) text(x=13, y=0.3, &#39;t=13&#39;, col=&#39;tomato&#39;) Primero vamos a ajustar el modelo polinomial cúbico, el codigo necesario es el siguiente. mod1 &lt;- lm(drop ~ time + I(time^2) + I(time^3), data=datos) summary(mod1) ## ## Call: ## lm(formula = drop ~ time + I(time^2) + I(time^3), data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3503 -0.7340 -0.1859 0.6440 1.8390 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.4910163 0.5336473 12.163 1.71e-14 *** ## time 0.7031952 0.2339552 3.006 0.004738 ** ## I(time^2) 0.0340179 0.0273762 1.243 0.221829 ## I(time^3) -0.0033072 0.0008992 -3.678 0.000743 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9335 on 37 degrees of freedom ## Multiple R-squared: 0.8773, Adjusted R-squared: 0.8673 ## F-statistic: 88.14 on 3 and 37 DF, p-value: &lt; 2.2e-16 De la anterior salida se pueden destacar los siguientes resultados: Los residuales varían entre -1.35 y 1.83. El \\(R^2_{Adj}=0.8673\\). La estimación de \\(\\sigma\\) es 0.9335. El término \\(t^3\\) es significativo en el modelo a un nivel de significancia del 5%, eso implica que todos los términos de grado 2 y grado 1 deben permanecer en el modelo, sean o no significativos. Ahora vamos a ajustar el modelo spline cúbico con 2 nodos en \\(t=6.5\\) y \\(t=13\\), el codigo necesario es el siguiente. xplus &lt;- function(x) ifelse(x &gt;= 0, x, 0) # Auxiliar function time65 &lt;- xplus(time - 6.5) # New variable 1 time13 &lt;- xplus(time - 13) # New variable 2 mod2 &lt;- lm(drop ~ time + I(time^2) + I(time^3) + I(time65^3) + I(time13^3), data=datos) summary(mod2) ## ## Call: ## lm(formula = drop ~ time + I(time^2) + I(time^3) + I(time65^3) + ## I(time13^3), data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.45168 -0.18499 -0.03547 0.20577 0.61694 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.465678 0.200520 42.219 &lt; 2e-16 *** ## time -1.453124 0.181586 -8.002 2.04e-09 *** ## I(time^2) 0.489889 0.043018 11.388 2.54e-13 *** ## I(time^3) -0.029467 0.002848 -10.347 3.44e-12 *** ## I(time65^3) 0.024706 0.004039 6.116 5.43e-07 *** ## I(time13^3) 0.027112 0.003578 7.577 6.98e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2678 on 35 degrees of freedom ## Multiple R-squared: 0.9904, Adjusted R-squared: 0.9891 ## F-statistic: 725.5 on 5 and 35 DF, p-value: &lt; 2.2e-16 De la anterior salida se pueden destacar los siguientes resultados: Los residuales están más cerca del cero cuando se comparan con los residuales del mod1. El \\(R^2_{Adj}=0.9891\\) aumentó bastante. La estimación de \\(\\sigma\\) es 0.2678. Todos los términos son significativos. Para hacer una comparación visual de ambos modelos vamos a construir nuevamente el diagrama de dispersión original y agregaremos las curvas ajustadas de ambos modelos. El código para hacer esto se muestra a continuación. plot(datos, ylab=&quot;Voltage drop&quot;, xlab=&quot;Time (seconds)&quot;, pch=19, ylim=c(0, 15), las=1) i &lt;- order(time) lines(time[i], fitted(mod1)[i], col=2, lwd=3) lines(time[i], fitted(mod2)[i], col=4, lwd=3) legend(&quot;bottomright&quot;, lwd=3, col=c(4,2), bty=&quot;n&quot;, legend=c(&quot;Cubic spline model&quot;, &quot;Cubic polynomial model&quot;)) Al observar la figura anterior se nota con claridad que el modelo cubic spline (azul) logra explicar mejor los datos, tanto en los extremos como en la parte central, eso significa que es mejor usar el modelo cubic spline para hacer predicciones futuras. Usando la tabla de resumen obtenida con summary(mod2) se puede escribir el modelo cubic spline ajustado así: \\[\\begin{align} {Drop}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2), \\\\ \\hat{\\mu}_i &amp;= 8.4657 -1.4531 t + 0.4899 t^2 -0.0295 t^3 + 0.0247 (t-6.5)^3_{+} + 0.0271 (t-13)^3_{+}, \\\\ \\hat{\\sigma} &amp;= 0.2678 \\end{align}\\] Función bs La función bs del paquete splines se puede utilizar para incluir basic splines en un modelo de regresión, la estructura básica de la función se muestra a continuación. bs(x, df = NULL, knots = NULL, degree = 3, intercept = FALSE, Boundary.knots = range(x)) Los argumentos básicos de la función son: x: the predictor variable. Missing values are allowed. df: degrees of freedom. knots: the internal breakpoints that define the spline. degree: degree of the piecewise polynomial—default is 3 for cubic splines. Ejemplo En este ejemplo vamos a retomar el ejemplo 7.2 del libro de E. &amp;. V. Montgomery D. &amp; Peck (2006) y que fue explicado anteriormente. El objetivo es repetir el modelo mod2 pero con la ayuda de la función bs, a ese nuevo modelo lo vamos a llamar mod3. El código necesario se muestra a continuación. require(splines) mod3 &lt;- lm(drop ~ bs(time, knots=c(6.5, 13), degree=3), data=datos) summary(mod3) ## ## Call: ## lm(formula = drop ~ bs(time, knots = c(6.5, 13), degree = 3), ## data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.45168 -0.18499 -0.03547 0.20577 0.61694 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.4657 0.2005 42.219 &lt; 2e-16 ## bs(time, knots = c(6.5, 13), degree = 3)1 -3.1484 0.3934 -8.002 2.04e-09 ## bs(time, knots = c(6.5, 13), degree = 3)2 4.3532 0.2843 15.312 &lt; 2e-16 ## bs(time, knots = c(6.5, 13), degree = 3)3 8.5518 0.3691 23.169 &lt; 2e-16 ## bs(time, knots = c(6.5, 13), degree = 3)4 0.5990 0.3059 1.958 0.058192 ## bs(time, knots = c(6.5, 13), degree = 3)5 1.2414 0.2871 4.324 0.000121 ## ## (Intercept) *** ## bs(time, knots = c(6.5, 13), degree = 3)1 *** ## bs(time, knots = c(6.5, 13), degree = 3)2 *** ## bs(time, knots = c(6.5, 13), degree = 3)3 *** ## bs(time, knots = c(6.5, 13), degree = 3)4 . ## bs(time, knots = c(6.5, 13), degree = 3)5 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2678 on 35 degrees of freedom ## Multiple R-squared: 0.9904, Adjusted R-squared: 0.9891 ## F-statistic: 725.5 on 5 and 35 DF, p-value: &lt; 2.2e-16 Al comparar los resultados de summary(mod2) con summary(mod3) vemos sólo una pequeña coincidencia. ¿Serán iguales, similares o diferentes el mod2 y mod3? Para compararlos vamos a crear nuevamente el diagrama de dispersión original y vamos a agregar las curvas ajustadas para los tres modelos. El modelo mod1 estará en color rojo, el modelo mod2 en color azul y línea gruesa, y el modelo mod3 en color naranja y línea delgada. a continuación el código usado. plot(datos, ylab=&quot;Voltage drop&quot;, xlab=&quot;Time (seconds)&quot;, pch=19, ylim=c(0,15)) lines(time[i], fitted(mod1)[i], col=&#39;red&#39;, lwd=3) lines(time[i], fitted(mod2)[i], col=&#39;blue&#39;, lwd=6) lines(time[i], fitted(mod3)[i], col=&#39;orange&#39;, lwd=1) legend(&quot;bottomright&quot;, lwd=c(3, 6, 2), col=c(&#39;red&#39;, &#39;blue&#39;, &#39;orange&#39;), legend=c(&quot;Cubic polynomial model&quot;, &quot;Cubic spline manually&quot;, &quot;Using bs()&quot;), bty=&quot;n&quot;) abline(v=c(6.5, 13), lty=&#39;dotted&#39;, col=&quot;tomato&quot;) # adding cutpoints De la figura anterior se observa que el modelo mod3 coincide con el modelo mod2. Ejemplo Como ilustración vamos a usar los datos del ejemplo 3.1 del libro de E. &amp;. V. Montgomery D. &amp; Peck (2006). En el ejemplo 3.1 los autores ajustaron un modelo de regresión lineal múltiple para explicar el Tiempo necesario para que un trabajador haga el mantenimiento y surta una máquina dispensadora de refrescos en función de las variables Número de Cajas y Distancia. El objetivo de este ejemplo es crear los siguientes modelos para compararlos con el MSE (mean squared error). modelo lineal con cantidad y distancia. modelo lineal con splines cúbicos de cantidad y distancia. Solución Los datos del ejemplo están disponibles en el paquete MPV (por los apellidos de los autores). A continuación el código para cargar los datos y una muestra de las 6 primeras observaciones de la base de datos, en total se disponen de 20 observaciones. El siguiente código sirve para construir los dos modelos solicitados. library(MPV) colnames(softdrink) &lt;- c(&#39;tiempo&#39;, &#39;cantidad&#39;, &#39;distancia&#39;) library(splines) mod1 &lt;- lm(tiempo ~ cantidad + distancia, data=softdrink) mod2 &lt;- lm(tiempo ~ bs(cantidad) + bs(distancia), data=softdrink) Para obtener los valores del MSE con ambos modelos hacemos lo siguiente: y_true &lt;- softdrink$tiempo y_hat1 &lt;- predict(mod1, newdata=softdrink) y_hat2 &lt;- predict(mod2, newdata=softdrink) mse1 &lt;- mean((y_true - y_hat1)^2) mse2 &lt;- mean((y_true - y_hat2)^2) cbind(mse1, mse2) ## mse1 mse2 ## [1,] 9.349267 4.765153 De la salida anterior vemos que el MSE del modelo mod2 que tiene splines cúbicos es menor (no necesariamente mejor) que el modelo mod1. El MSE en este ejemplo se pudo haber obtenido usando directamente los residuales. ¿Será que el modelo mod2 está sobre parametrizado? Función lowess La función lowess (LOcally WEighted Scatterplot Smoothing) permite constuir una curva suavizada a partir de muchas regresiones de orden uno localizadas, usando una ventana que incluye un porcentaje f de puntos. A continuación se muestra una figura ilustrativa para entender lo que hace lowess. La estructura de la función lowess se muestra a continuación. lowess(x, y = NULL, f = 2/3, iter = 3, delta = 0.01 * diff(range(x))) Los argumentos básicos de la función son: x: vector con los valores de la covariable. y: vector con los valores de la variable respuesta. f: porcentaje de puntos dentro de la ventana, por defecto es 2/3. Ejemplo Como ejemplo vamos a usar los datos de la base Prestige que se encuentra en el paquete car. Queremos constuir un modelo lowess para explicar la variable prestige en función de la variable income. A continuación el código para crear el diagrama de dispersión que muestra la relación entre las variables. library(car) plot(prestige ~ income, xlab=&quot;Average Income&quot;, ylab=&quot;Prestige&quot;, data=Prestige, pch=19) Para crear el modelo lowess a los datos anteriores se usa el siguiente código. mod_lowess &lt;- lowess(x=Prestige$income, y=Prestige$prestige, f=2/3) El objeto mod_lowess es una lista con las coordenadas \\(x\\) e \\(y\\) por donde pasa la curva suavizada. A continuación el código para agregar la curva al diagrama de dispersión. plot(prestige ~ income, xlab=&quot;Average Income&quot;, ylab=&quot;Prestige&quot;, data=Prestige, pch=19) lines(mod_lowess, lwd=4, col=&#39;tomato&#39;) De la figura anterior vemos que la curva suavizada logra capturar el patrón de variación de los datos. ¿Qué sucede con la curva cuando cambiamos el parámetro f? El efecto de f se puede ver con claridad en la siguiente figura. Cuando f es pequeño la curva es muy rugosa, para valores altos de f la curva se suaviza. Función loess La función loess (LOcally Estimated Scatterplot Smoothing) permite constuir una curva o superficie suavizada a partir de muchas regresiones de orden uno o dos localizadas, usando una ventana que incluye un porcentaje span de puntos. La función loess es una generalización de lowess. La estructura de la función loess se muestra a continuación. loess(formula, data, weights, subset, na.action, model = FALSE, span = 0.75, enp.target, degree = 2, parametric = FALSE, drop.square = FALSE, normalize = TRUE, family = c(&quot;gaussian&quot;, &quot;symmetric&quot;), method = c(&quot;loess&quot;, &quot;model.frame&quot;), control = loess.control(...), ...) Los argumentos básicos de la función son: formula: fórmula usual (y ~ x1 + x2 + x3 + x4) para indicar la variable respuesta y las covariables. Máximo se pueden incluir 4 covariables. data: marco de datos con las variables. degree: el grado de los polinomios locales a usar, se puede elegir entre grado 1 o grado 2. span: vector con los valores de la variable respuesta. Los otros parámetros tienen valores por defecto que se pueden cambiar para obtener mejores ajustes. Ejemplo Como ejemplo vamos a usar los datos de la base Prestige que se encuentra en el paquete car. Queremos constuir un modelo loess para explicar la variable prestige en función de las variables income y education. A continuación el código para crear el diagrama de dispersión que muestra la relación entre las variables. library(plotly) plot_ly(x=Prestige$income, y=Prestige$education, z=Prestige$prestige, type=&quot;scatter3d&quot;, color=Prestige$prestige) %&gt;% layout(scene = list(xaxis = list(title = &#39;Income&#39;), yaxis = list(title = &#39;Education&#39;), zaxis = list(title = &#39;Prestige&#39;))) Para crear el modelo loess a los datos anteriores se usa el siguiente código. mod_loess &lt;- loess(prestige ~ income + education, data=Prestige, degree=2, span=0.75) A continuación el código para agregar la superficie al diagrama de dispersión. Los objetos inc y edu son secuencias de valores en el rango de los datos originales. El objeto newdata es un marco de datos con todas las combinaciones de valores de inc y edu. El objeto fit.prestige es una matriz con los valores estimados de la variable prestige. Luego todos esos objetos entran a la función plot_ly. inc &lt;- with(Prestige, seq(min(income), max(income), len=25)) edu &lt;- with(Prestige, seq(min(education), max(education), len=25)) newdata &lt;- expand.grid(income=inc, education=edu) fit.prestige &lt;- matrix(predict(mod_loess, newdata), 25, 25) plot_ly(x=inc, y=edu, z=fit.prestige) %&gt;% add_surface() %&gt;% layout(scene = list(xaxis = list(title = &#39;Income&#39;), yaxis = list(title = &#39;Education&#39;), zaxis = list(title = &#39;Prestige&#39;))) Es posible crear un gráfico con los puntos originales y la superficie del modelo, a continuación el código necesario. library(&quot;plot3D&quot;) scatter3D(x=Prestige$income, y=Prestige$education, z=Prestige$prestige, ticktype=&quot;detailed&quot;, pch=20, bty=&quot;f&quot;, colkey=FALSE, phi=30, theta=45, type=&quot;h&quot;, xlab=&#39;Income&#39;, ylab=&#39;Education&#39;, zlab=&#39;Prestige&#39;, surf=list(x=inc, y=edu, z=fit.prestige, NAcol=&quot;black&quot;, shade=0.1)) ¿Qué sucede con la superficie cuando cambiamos el parámetro span? El efecto de span se puede ver con claridad en la siguiente figura. Cuando span es pequeño la superficie es muy rugosa, para valores altos de span la suferficie se suaviza. Optimización con superficies de respuesta Cuando se tiene una superficie de respuesta, obtenida por métodos paramétricos o no paramétricos, dos preguntas posibles son: ¿Cuáles valores de las covariables maximizan la variable respuesta? ¿Cuáles valores de las covariables minimizan la variable respuesta? Ambos problemas se denominan problemas de optimización. Cuando se tiene una o dos covariables, el problema es sencillo, y se puede resolver dibujando la curva o la superficie, y visualmente se puede obtener el valor o valores de las \\(x&#39;\\)s que optimizan la variable respuesta. Cuando se tienen tres o más covariables es necesario usar métodos de optimización, algunas de las funciones más usuales de R para hacer esto son: función nlminb, función optim. Función nlminb La estructura de esta función es la siguiente: nlminb(start, objective, gradient = NULL, hessian = NULL, ..., scale = 1, control = list(), lower = -Inf, upper = Inf) Los parámetros de la función son: start: vector con los valores donde inicia la búsqueda. objective: función a MINIMIZAR. El primer argumento de esta función debe ser un vector y ella debe entregar un valor. gradient: función opcional que calcula el gradiente. hessian: función opcional que calcula la hessiana. lower: vector con los valores mínimos de la región de búsqueda. upper: vector con los valores máximos de la región de búsqueda. Maximizar la función \\(f(x)\\) es equivalente a minimizar la función \\(-f(x)\\). Esto es importante para hacer optimización. Ejemplo Vamos a utilizar aquí unos datos de un experimento en el cual se estudió la influencia de la temperatura y la concentración sobre el rendimiento obtenido en un proceso químico. A continuación los datos del experimento. temp &lt;- c(200, 250, 200, 250, 189.65, 260.35, 225, 225, 225, 225, 225, 225) conc &lt;- c(15, 15, 25, 25, 20, 20, 12.93, 27.07, 20, 20, 20, 20) rend &lt;- c(43, 78, 69, 73, 48, 76, 65, 74, 76, 79, 83, 81) Vamos a construir un diagrama de dispersión para ver la relación de las variables. library(scatterplot3d) scatterplot3d(x=temp, y=conc, z=rend, pch=16, cex.lab=1.5, highlight.3d=TRUE, type=&quot;h&quot;) El objetivo es ajustar el siguiente modelo: \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 temp_{i} + \\beta_2 conc_{i} + \\beta_3 temp_i^2 + \\beta_4 conc_i^2 + \\beta_5 temp \\times conc, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] El código para ajustar el modelo es el siguiente. mod &lt;- lm(rend ~ temp + conc + I(temp^2) + I(conc^2) + temp * conc) Usando el modelo ajustado mod es posible dibujar la superfice de respuesta para determinar de forma visual los valores que maximizan el rendimiento. # Se crean 30 valores de las variables para crear la rejilla Temperatura &lt;- seq(from=189.65, to=260.35, length.out=30) Concentracion &lt;- seq(from=12.93, to=27.07, length.out=30) # Rend es la funcion a dibujar Rend &lt;- function(temp, conc) { res &lt;- coef(mod) * c(1, temp, conc, temp^2, conc^2, temp * conc) sum(res) } Rend &lt;- Vectorize(Rend) # La funcion a dibujar debe estar vectorizada # La matriz Rendimiento con las alturas de la superficie se crea con outer Rendimiento &lt;- outer(Temperatura, Concentracion, Rend) # Para dibujar la superficie de respuesta persp(x=Temperatura, y=Concentracion, z=Rendimiento, theta=40, phi=30, ticktype = &quot;detailed&quot;, col=&#39;salmon1&#39;) Se puede también construir un gráfico con curvas de nivel para determinar de forma visual los valores que maximizan el rendimiento. contour(x=Temperatura, y=Concentracion, z=Rendimiento, nlevels=10, col=gray(0.3), lwd=2, lty=&#39;solid&#39;, xlab=&#39;Temperatura&#39;, ylab=&#39;Concentracion&#39;, las=1) Un gráfico de calor es también útil para determinar de forma visual los valores que maximizan el rendimiento. filled.contour(x=Temperatura, y=Concentracion, z=Rendimiento, nlevels=10, xlab=&#39;Temperatura&#39;, ylab=&#39;Concentracion&#39;, las=1, color.palette = cm.colors) Para encontrar los valores exactos que maximizan el rendimiento se usa la función nlminb. A continuación se crea la función minus_rend que representa \\(-f(x)\\) la cual va a ser minimizada. Se define el punto de inicio de la búsqueda en el objeto inicio y luego el resultado de nlminb se almacena en el objeto res. minus_rend &lt;- function(x) { temp &lt;- x[1] conc &lt;- x[2] new.data &lt;- data.frame(temp=c(1, temp), conc=c(1, conc)) -predict(mod, new.data)[2] } inicio &lt;- c(192, 15) # valores iniciales para la busqueda names(inicio) &lt;- c(&#39;Temperatura&#39;, &#39;Concentracion&#39;) # Colocando nombres res &lt;- nlminb(start=inicio, objective=minus_rend, lower=c(189.65, 12.93), # minimos de las variables upper=c(260.35, 27.07), # maximos de las variables control=list(trace=0)) res$par # Valores optimos ## Temperatura Concentracion ## 238.95190 19.94697 -res$objective # Valor del objetivo ## [1] 82.46933 Dentro del objeto res hay mucha más información. Es fundamental que el lector explore esos elementos con la ayuda de la función nlminb. Una reto interesante para el lector es que replique el ejemplo con la función optim para ver otra forma alternativa de optimizar. Polinomios ortogonales Estamos preparando el contenido, revisar luego. References Montgomery, E. &amp; Vining, D. &amp; Peck. 2006. Introducción Al Análisis de Regresión Lineal. 3ed ed. México: Cecsa. "],["allpossibleregre.html", "15 Todas las regresiones posibles Criterios para elegir modelos Paquete olsrr", " 15 Todas las regresiones posibles En este capítulo se muestra como elegir el mejor modelo de regresión cuando es posible ajustar o entrenar todos los posibles modelos relacionados a un problema. Si un modelo tiene \\(k\\) variables cuantitativas, se pueden construir \\(2^k\\) modelos diferentes con subconjuntos de las variables originales. A continuación un ejemplo de cómo crece el número de todas las regresiones posibles en función del número \\(k\\) de covariables. ## k num_de_regresiones ## [1,] 2 4 ## [2,] 4 16 ## [3,] 8 256 ## [4,] 16 65536 ## [5,] 32 4294967296 Criterios para elegir modelos A continuación una lista de los posibles indicadores y el ideal de cada uno de ellos. \\(R^2\\): coeficiente de determinación, entre más grande mejor. \\(R^2_A\\): coeficiente de determinación ajustado, entre más grande mejor. \\(C_p\\) de Mallows: el mejor modelo es aquél para el cual \\(C_p\\) es el más pequeño posible. \\(AIC\\): Akaike Information Criterium, entre más pequeño mejor. Paquete olsrr El paquete olsrr creado por Hebbali (2024) contiene varias funciones útiles para modelación, en particular se destacan dos funciones. ols_step_all_possible(model) ols_step_best_subset(model, max_order = NULL, include = NULL, exclude = NULL, metric = c(&quot;rsquare&quot;, &quot;adjr&quot;, &quot;predrsq&quot;, &quot;cp&quot;, &quot;aic&quot;, &quot;sbic&quot;, &quot;sbc&quot;, &quot;msep&quot;, &quot;fpe&quot;, &quot;apc&quot;, &quot;hsp&quot;)) Ejemplo En este ejemplo vamos a usar la base de datos mtcars para crear todas las regresiones posibles para explicar mpg en función de las covariables disp, hp, wt, qsec, a continuación una parte de la base de datos. head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Solución Lo primero que se debe hacer es ajustar el modelo saturado (o modelo full) para luego crear todas las regresiones posibles que incluyan las covariables disp, hp, wt, qsec. model &lt;- lm(mpg ~ disp + hp + wt + qsec, data=mtcars) library(olsrr) res &lt;- ols_step_all_possible(model) res # Este objeto contiene los resultados ## Index N Predictors R-Square Adj. R-Square Mallow&#39;s Cp ## 3 1 1 wt 0.7528328 0.7445939 0.70869536 ## 1 2 1 disp 0.7183433 0.7089548 0.67512054 ## 2 3 1 hp 0.6024373 0.5891853 0.50969578 ## 4 4 1 qsec 0.1752963 0.1478062 0.07541973 ## 8 5 2 hp wt 0.8267855 0.8148396 0.78108710 ## 10 6 2 wt qsec 0.8264161 0.8144448 0.77856272 ## 6 7 2 disp wt 0.7809306 0.7658223 0.72532105 ## 5 8 2 disp hp 0.7482402 0.7308774 0.69454380 ## 7 9 2 disp qsec 0.7215598 0.7023571 0.66395284 ## 9 10 2 hp qsec 0.6368769 0.6118339 0.52014395 ## 14 11 3 hp wt qsec 0.8347678 0.8170643 0.78199548 ## 11 12 3 disp hp wt 0.8268361 0.8082829 0.76789526 ## 13 13 3 disp wt qsec 0.8264170 0.8078189 0.76988533 ## 12 14 3 disp hp qsec 0.7541953 0.7278591 0.68301440 ## 15 15 4 disp hp wt qsec 0.8351443 0.8107212 0.77102968 Los resultados anteriores se pueden ver de forma gráfica así: plot(res) Ejemplo En este ejemplo vamos a usar la base de datos mtcars para identificar el mejor modelo que explique mpg en función de las covariables disp, hp, wt, qsec. La restricción que se impone es que el modelo TIENE que incluir la variable hp y la métrica a usar será el \\(R^2_{Adj}\\). Solución model &lt;- lm(mpg ~ disp + hp + wt + qsec, data=mtcars) library(olsrr) res &lt;- ols_step_best_subset(model, include=&quot;hp&quot;, metric=&quot;adjr&quot;) res # Este objeto contiene los resultados ## Best Subsets Regression ## ------------------------------ ## Model Index Predictors ## ------------------------------ ## 1 hp ## 2 hp wt ## 3 hp wt qsec ## 4 disp hp wt qsec ## ------------------------------ ## ## Subsets Regression Summary ## ---------------------------------------------------------------------------------------------------------------------------------- ## Adj. Pred ## Model R-Square R-Square R-Square C(p) AIC SBIC SBC MSEP FPE HSP APC ## ---------------------------------------------------------------------------------------------------------------------------------- ## 1 0.6024 0.5892 0.5097 37.1126 181.2386 87.8752 185.6358 477.5836 15.8551 0.5146 0.4506 ## 2 0.8268 0.8148 0.7811 2.3690 156.6523 66.5755 162.5153 215.5104 7.3563 0.2402 0.2091 ## 3 0.8348 0.8171 0.782 3.0617 157.1426 67.7238 164.4713 213.1929 7.4756 0.2461 0.2124 ## 4 0.8351 0.8107 0.771 5.0000 159.0696 70.0408 167.8640 220.8882 7.9497 0.2644 0.2259 ## ---------------------------------------------------------------------------------------------------------------------------------- ## AIC: Akaike Information Criteria ## SBIC: Sawa&#39;s Bayesian Information Criteria ## SBC: Schwarz Bayesian Criteria ## MSEP: Estimated error of prediction, assuming multivariate normality ## FPE: Final Prediction Error ## HSP: Hocking&#39;s Sp ## APC: Amemiya Prediction Criteria Los resultados anteriores se pueden ver de forma gráfica así: plot(res) Se le recomienda al lector explorar las viñetas del paquete olsrr para conocer todas las bondades del paquete. References Hebbali, Aravind. 2024. Olsrr: Tools for Building OLS Regression Models. https://olsrr.rsquaredacademy.com/. "],["selec.html", "16 Selección de variables Akaike Information Criterion (\\(AIC\\)) Funciones logLik y AIC Métodos Función stepAIC Funciones addterm y dropterm Paquete mixlm Paquete leaps", " 16 Selección de variables En este capítulo se muestra como realizar selección de variables en un modelo de regresión lineal. Akaike Information Criterion (\\(AIC\\)) El \\(AIC\\) se define como: \\[AIC = - 2 \\times logLik + k \\times n_{par},\\] donde \\(logLik\\) corresponde al valor de log-verosimilitud del modelo para el vector de parámetros \\(\\hat{\\Theta}\\), \\(k\\) es un valor de penalización por el exceso de parámetros y \\(n_{par}\\) corresponde al número de parámetros del modelo. Se debe recordar siempre que: El mejor modelo es aquel que \\(logLik\\) ↑. El mejor modelo es aquel que \\(AIC\\) ↓. Cuando el valor de penalización \\(k=\\log(n)\\) entonces el \\(AIC\\) se llamada en \\(BIC\\) o \\(SBC\\) (Schwarz’s Bayesian criterion). Funciones logLik y AIC La función logLik sirve para obtener el valor de log-verosimilitud de un modelo y la función AIC entrega el Akaike Information Criterion. La estructura de ambas funciones se muestra a continuación. logLik(object) AIC(object, k=2) Métodos Los métodos para realizar selcción de variables se pueden clasificar de la siguiente manera: Todas las regresiones posibles. Selección de variables. Forward Backward A continuación una imagen ilustrativa para entender ambos métodos. Figure 16.1: Ilustración de los métodos Forward y Backward. Un término ingresa al modelo si su presencia disminuye el \\(AIC\\). Un términa sale del modelo si su ausencia disminuye el \\(AIC\\). Función stepAIC La función stepAIC del paquete MASS (Ripley 2024) es útil para hacer selección de variables en un modelo de regresión. La estructura de la función se muestra a continuación. stepAIC(object, scope, scale = 0, direction = c(&quot;both&quot;, &quot;backward&quot;, &quot;forward&quot;), trace = 1, keep = NULL, steps = 1000, use.start = FALSE, k = 2, ...) Algunos de los argumentos son: object: un objeto con un modelo. scope: fórmula(s) con los límites de búsqueda. direction: los posibles valores son both, backward, forward. trace: valor lógico para indicar si se desea ver el paso a paso de la selección. k: valor de penalidad, por defecto es 2. Ejemplo En este ejemplo se busca encontrar un modelo de regresion lineal que explique la variable respuesta \\(y\\) en función de las covariables \\(x_1\\) a \\(x_{11}\\), los datos provienen del ejercicio 9.5 del libro de Montgomery, Peck and Vining (2003). A continuación se muestra el encabezado de la base de datos y la definición de las variables. Figure 16.2: Ilustración de los métodos Forward y Backward. Nota: Type of transmission (1=automatic, 0=manual). Antes de iniciar es necesario revisar si hay NA's y eliminarlos. library(MPV) # Aqui estan los datos table.b3[22:26, ] # Can you see the missing values? ## y x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 ## 22 21.47 360.0 180 290 8.4 2.45 2 3 214.2 76.3 4250 1 ## 23 16.59 400.0 185 NA 7.6 3.08 4 3 196.0 73.0 3850 1 ## 24 31.90 96.9 75 83 9.0 4.30 2 5 165.2 61.8 2275 0 ## 25 29.40 140.0 86 NA 8.0 2.92 2 4 176.4 65.4 2150 0 ## 26 13.27 460.0 223 366 8.0 3.00 4 3 228.0 79.8 5430 1 datos &lt;- table.b3[-c(23, 25), ] El objeto datos tiene la base de datos sin las líneas con NA, lo mismo se hubiese podido realizar usando la función na.omit. A continuación se muestran los diagramas de dispersión para las variables de la base de datos. ## ## Attaching package: &#39;psych&#39; ## The following object is masked from &#39;package:car&#39;: ## ## logit ## The following object is masked from &#39;package:randomForest&#39;: ## ## outlier ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha Aplicación del método backward Vamos a crear un modelo saturado, es decir, el modelo mayor a considerar. full.model &lt;- lm(y ~ ., data=datos) summary(full.model) ## ## Call: ## lm(formula = y ~ ., data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3441 -1.6711 -0.4486 1.4906 5.2508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.339838 30.355375 0.571 0.5749 ## x1 -0.075588 0.056347 -1.341 0.1964 ## x2 -0.069163 0.087791 -0.788 0.4411 ## x3 0.115117 0.088113 1.306 0.2078 ## x4 1.494737 3.101464 0.482 0.6357 ## x5 5.843495 3.148438 1.856 0.0799 . ## x6 0.317583 1.288967 0.246 0.8082 ## x7 -3.205390 3.109185 -1.031 0.3162 ## x8 0.180811 0.130301 1.388 0.1822 ## x9 -0.397945 0.323456 -1.230 0.2344 ## x10 -0.005115 0.005896 -0.868 0.3971 ## x11 0.638483 3.021680 0.211 0.8350 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.227 on 18 degrees of freedom ## Multiple R-squared: 0.8355, Adjusted R-squared: 0.7349 ## F-statistic: 8.31 on 11 and 18 DF, p-value: 5.231e-05 De la tabla anterior se puede pensar en que hay un efecto de enmascaramiento entre las variables ya que ninguna parece significativa marginalmente. Se usa la función stepAIC y se elije trace=TRUE para obtener detalles del proceso de selección. library(MASS) # Para poder usar la funcion stepAIC modback &lt;- stepAIC(full.model, trace=TRUE, direction=&quot;backward&quot;) ## Start: AIC=78.96 ## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 ## ## Df Sum of Sq RSS AIC ## - x11 1 0.465 187.87 77.036 ## - x6 1 0.632 188.03 77.063 ## - x4 1 2.418 189.82 77.346 ## - x2 1 6.462 193.86 77.979 ## - x10 1 7.836 195.24 78.190 ## - x7 1 11.065 198.47 78.683 ## &lt;none&gt; 187.40 78.962 ## - x9 1 15.758 203.16 79.384 ## - x3 1 17.770 205.17 79.679 ## - x1 1 18.736 206.14 79.820 ## - x8 1 20.047 207.45 80.011 ## - x5 1 35.864 223.26 82.215 ## ## Step: AIC=77.04 ## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 ## ## Df Sum of Sq RSS AIC ## - x6 1 0.536 188.40 75.121 ## - x4 1 2.363 190.23 75.411 ## - x2 1 6.642 194.51 76.078 ## - x10 1 7.985 195.85 76.285 ## &lt;none&gt; 187.87 77.036 ## - x7 1 14.124 201.99 77.211 ## - x9 1 16.914 204.78 77.622 ## - x3 1 17.815 205.68 77.754 ## - x1 1 18.280 206.15 77.822 ## - x8 1 20.301 208.17 78.114 ## - x5 1 36.370 224.24 80.345 ## ## Step: AIC=75.12 ## y ~ x1 + x2 + x3 + x4 + x5 + x7 + x8 + x9 + x10 ## ## Df Sum of Sq RSS AIC ## - x4 1 3.451 191.85 73.666 ## - x2 1 6.932 195.33 74.205 ## - x10 1 9.351 197.75 74.574 ## &lt;none&gt; 188.40 75.121 ## - x7 1 14.473 202.87 75.342 ## - x3 1 17.802 206.20 75.830 ## - x9 1 18.146 206.55 75.880 ## - x1 1 18.780 207.18 75.972 ## - x8 1 21.244 209.65 76.326 ## - x5 1 39.332 227.73 78.809 ## ## Step: AIC=73.67 ## y ~ x1 + x2 + x3 + x5 + x7 + x8 + x9 + x10 ## ## Df Sum of Sq RSS AIC ## - x2 1 10.780 202.63 73.306 ## - x7 1 11.113 202.97 73.355 ## &lt;none&gt; 191.85 73.666 ## - x10 1 14.988 206.84 73.923 ## - x1 1 16.602 208.46 74.156 ## - x9 1 18.072 209.92 74.366 ## - x3 1 21.314 213.17 74.826 ## - x8 1 28.835 220.69 75.867 ## - x5 1 40.323 232.18 77.389 ## ## Step: AIC=73.31 ## y ~ x1 + x3 + x5 + x7 + x8 + x9 + x10 ## ## Df Sum of Sq RSS AIC ## - x7 1 10.457 213.09 72.815 ## - x3 1 10.595 213.23 72.835 ## - x1 1 11.998 214.63 73.032 ## - x9 1 12.643 215.28 73.122 ## - x10 1 13.887 216.52 73.295 ## &lt;none&gt; 202.63 73.306 ## - x8 1 27.665 230.30 75.145 ## - x5 1 30.191 232.82 75.472 ## ## Step: AIC=72.82 ## y ~ x1 + x3 + x5 + x8 + x9 + x10 ## ## Df Sum of Sq RSS AIC ## - x3 1 4.8720 217.96 71.494 ## - x9 1 5.2049 218.29 71.539 ## - x1 1 5.3212 218.41 71.555 ## &lt;none&gt; 213.09 72.815 ## - x10 1 18.3677 231.46 73.296 ## - x5 1 23.3458 236.44 73.934 ## - x8 1 26.0316 239.12 74.273 ## ## Step: AIC=71.49 ## y ~ x1 + x5 + x8 + x9 + x10 ## ## Df Sum of Sq RSS AIC ## - x1 1 0.765 218.73 69.599 ## - x9 1 5.863 223.82 70.290 ## &lt;none&gt; 217.96 71.494 ## - x10 1 20.291 238.25 72.164 ## - x5 1 23.020 240.98 72.506 ## - x8 1 31.634 249.59 73.559 ## ## Step: AIC=69.6 ## y ~ x5 + x8 + x9 + x10 ## ## Df Sum of Sq RSS AIC ## - x9 1 5.097 223.82 68.290 ## &lt;none&gt; 218.73 69.599 ## - x5 1 40.404 259.13 72.684 ## - x8 1 57.407 276.13 74.591 ## - x10 1 135.105 353.83 82.029 ## ## Step: AIC=68.29 ## y ~ x5 + x8 + x10 ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 223.82 68.290 ## - x5 1 36.314 260.14 70.800 ## - x8 1 52.960 276.78 72.661 ## - x10 1 194.838 418.66 85.076 Para obtener un resumen del proceso se usa: modback$anova ## Stepwise Model Path ## Analysis of Deviance Table ## ## Initial Model: ## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 ## ## Final Model: ## y ~ x5 + x8 + x10 ## ## ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 18 187.4007 78.96155 ## 2 - x11 1 0.4648362 19 187.8655 77.03587 ## 3 - x6 1 0.5356445 20 188.4012 75.12128 ## 4 - x4 1 3.4514854 21 191.8526 73.66591 ## 5 - x2 1 10.7796848 22 202.6323 73.30587 ## 6 - x7 1 10.4571693 23 213.0895 72.81545 ## 7 - x3 1 4.8720101 24 217.9615 71.49363 ## 8 - x1 1 0.7654631 25 218.7270 69.59881 ## 9 - x9 1 5.0970905 26 223.8241 68.28989 Para ver la tabla de resultados del modelo modback. summary(modback) ## ## Call: ## lm(formula = y ~ x5 + x8 + x10, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6101 -1.9868 -0.6613 2.0369 5.8811 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.590404 11.771925 0.390 0.6998 ## x5 2.597240 1.264562 2.054 0.0502 . ## x8 0.217814 0.087817 2.480 0.0199 * ## x10 -0.009485 0.001994 -4.757 6.38e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.934 on 26 degrees of freedom ## Multiple R-squared: 0.8035, Adjusted R-squared: 0.7808 ## F-statistic: 35.44 on 3 and 26 DF, p-value: 2.462e-09 Aplicación del método forward Para aplicar este metodo se debe crear un modelo vacío (empty.model) del cual iniciará el proceso. Es necesario definir un punto final de búsqueda, ese punto es una fórmula que en este caso llamaremos horizonte. A continuación el codigo. empty.model &lt;- lm(y ~ 1, data=datos) horizonte &lt;- formula(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11) Se usa la función stepAIC y se elije trace=FALSE para que NO se muestren los detalles del proceso de selección. modforw &lt;- stepAIC(empty.model, trace=FALSE, direction=&quot;forward&quot;, scope=horizonte) modforw$anova ## Stepwise Model Path ## Analysis of Deviance Table ## ## Initial Model: ## y ~ 1 ## ## Final Model: ## y ~ x1 + x4 ## ## ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 29 1139.1050 111.10402 ## 2 + x1 1 866.49528 28 272.6097 70.20532 ## 3 + x4 1 18.57161 27 254.0381 70.08861 Para ver la tabla de resultados del modelo modforw. summary(modforw) ## ## Call: ## lm(formula = y ~ x1 + x4, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.5011 -2.1243 -0.3884 1.9964 6.9582 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.179421 18.787955 0.382 0.705 ## x1 -0.044479 0.005225 -8.513 3.98e-09 *** ## x4 3.077228 2.190294 1.405 0.171 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.067 on 27 degrees of freedom ## Multiple R-squared: 0.777, Adjusted R-squared: 0.7605 ## F-statistic: 47.03 on 2 and 27 DF, p-value: 1.594e-09 Como la variable \\(x_4\\) no es significativa, entonces se puede refinar o actualizar el modelo modforw sacando \\(x_4\\), esto se puede realizar fácilmente por medio de la función update así: modforw &lt;- update(modforw, y ~ x1) summary(modforw) ## ## Call: ## lm(formula = y ~ x1, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.6063 -2.0276 -0.0457 1.4531 7.0213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.490010 1.535476 21.811 &lt; 2e-16 *** ## x1 -0.047026 0.004985 -9.434 3.43e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.12 on 28 degrees of freedom ## Multiple R-squared: 0.7607, Adjusted R-squared: 0.7521 ## F-statistic: 89 on 1 and 28 DF, p-value: 3.429e-10 En este enlace usted podrá encontrar la respuesta que le dieron a Audrey al preguntar “Why stepAIC gives a model with insignificant variables?”. Aplicación del método both Para aplicar este método se debe crear un modelo vacío del cual iniciará el proceso. Es necesario definir un punto final de búsqueda, ese punto es una formula que en este caso llamaremos horizonte. A continuación el código. modboth &lt;- stepAIC(empty.model, trace=FALSE, direction=&quot;both&quot;, scope=horizonte) modboth$anova ## Stepwise Model Path ## Analysis of Deviance Table ## ## Initial Model: ## y ~ 1 ## ## Final Model: ## y ~ x1 + x4 ## ## ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 29 1139.1050 111.10402 ## 2 + x1 1 866.49528 28 272.6097 70.20532 ## 3 + x4 1 18.57161 27 254.0381 70.08861 El modelo modboth y modforw son el mismo. A continuación vamos a realizar una comparación de los modelos obtenidos. Comparando \\(R^2_{Adj}\\) Para extraer el \\(R^2_{Adj}\\) de la tabla de resultados se usa: summary(modback)$adj.r.squared ## [1] 0.7808368 summary(modforw)$adj.r.squared ## [1] 0.7521337 Comparando \\(\\hat{\\sigma}\\) Para extraer el \\(\\hat{\\sigma}\\) de la tabla de resultados se usa: summary(modback)$sigma ## [1] 2.934045 summary(modforw)$sigma ## [1] 3.120266 Comparando los residuales par(mfrow=c(1, 2)) plot(modback, main=&quot;Backward&quot;, pch=19, cex=1, which=1) plot(modforw, main=&quot;Forward&quot;, pch=19, cex=1, which=1) Ejercicios ¿Qué patrón observa en los gráficos? Para cada uno de los dos modelos incluya términos cuadráticos con el objetivo de explicar ese patrón cuadrático no explicado y mostrado en los gráficos de residuales anteriores. Funciones addterm y dropterm Estas dos funciones pertenecen al paquete MASS (Ripley 2024) y son útiles para agregar/quitar 1 variable con respecto al modelo ingresado. A continuación la estructura de las funciones. addterm(object, ...) dropterm (object, ...) Ejemplo Usando datos anteriores ajuste un modelo para explicar y en función de x2 y x5, luego use addterm para determinar cual de las variables x1, x4 y x6 se debería ingresar. mod1 &lt;- lm(y ~ x2 + x5, data=datos) maximo &lt;- formula(~ x1 + x2 + x3 + x4 + x5 + x6) addterm(mod1, scope=maximo) ## Single term additions ## ## Model: ## y ~ x2 + x5 ## Df Sum of Sq RSS AIC ## &lt;none&gt; 353.19 79.974 ## x1 1 88.511 264.67 73.319 ## x3 1 47.296 305.89 77.661 ## x4 1 19.915 333.27 80.233 ## x6 1 19.452 333.73 80.274 De la salida anterior se ve que ingresar x1 mejoraría el modelo porque lo llevaría de un \\(AIC=79.974\\) a uno con \\(AIC=73.319\\). Paquete mixlm El paquete mixlm creado por Liland (2023) contiene un buen número de funciones para modelación. Algunas de las funciones a destacar son forward, backward, stepWise. Este paquete contiene otras funciones lm y glm que se pueden confundir con las funciones lm y glm del paquete stats. Por esta razón el usuario debe tener cuidado de usar la apropiada, en estos casos se recomienda usar stats::lm(y ~ x) o mixlm::lm(y ~ x) para obligar a R a que use la que el usuario desea. Ejemplo En este ejemplo se retoman los datos del ejercicio 9.5 del libro de Montgomery, Peck and Vining (2003). En este ejemplo se busca encontrar un modelo de regresion lineal que explique la variable respuesta \\(y\\) en función de las covariables \\(x_1\\) a \\(x_{11}\\), usando el modelo backward y que todas las variables sean significativas a un nivel del 4%. Para realizar lo solicitado se usa el siguiente código: library(MPV) # Aqui estan los datos datos &lt;- table.b3[-c(23, 25), ] # Eliminando 2 observaciones con NA modelo &lt;- lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11, data=datos) library(mixlm) backward(modelo, alpha=0.04) ## Backward elimination, alpha-to-remove: 0.04 ## ## Full model: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 ## ## Step RSS AIC R2pred Cp F value Pr(&gt;F) ## x11 1 187.87 77.036 0.41925 10.04465 0.0446 0.83503 ## x6 2 188.40 75.121 0.51334 8.09610 0.0542 0.81844 ## x4 3 191.85 73.666 0.53495 6.42762 0.3664 0.55178 ## x2 4 202.63 73.306 0.54991 5.46301 1.1799 0.28968 ## x7 5 213.09 72.815 0.61766 4.46743 1.1353 0.29819 ## x3 6 217.96 71.494 0.63898 2.93539 0.5259 0.47566 ## x1 7 218.73 69.599 0.66089 1.00892 0.0843 0.77406 ## x9 8 223.82 68.290 0.73638 -0.50150 0.5826 0.45244 ## x5 9 260.14 70.800 0.70705 0.98652 4.2184 0.05017 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: ## lm(formula = y ~ x8 + x10, data = datos) ## ## Coefficients: ## (Intercept) x8 x10 ## 16.23496 0.21234 -0.01022 De la salida anterior vemos que el modelo final es y ~ x8 + x10. Si comparamos este modelo con el obtenido al usar la función stepAIC, vemos que se eliminó la variable x5 ya que su valor-P era 5.02%, superior al límite definido aquí del 4%. Paquete leaps El paquete leaps creado por Lumley (2020) está basado en Fortran y es útil cuando nos interese encontrar subconjuntos de covariables para optimizar las características de un modelo. La función regsubsets realiza una búsqueda exhaustiva de los mejores subconjuntos de variables \\(x\\) para explicar \\(y\\). La búsqueda utiliza un algoritmo eficiente de ramificación y unión. La estructura de la función es la siguiente: regsubsets(x, data, nbest, nvmax, ...) Algunos de los parámetros más usuados en la función son: x: fórmula usual. data: marco de datos. nbest: número de subconjuntos de cada tamaño a guardar. nvmax: máximo número de subconjuntos a evaluar. Ejemplo En este ejemplo vamos a usar la base de datos mtcars para encontrar los dos mejores modelos con 1, 2, 3 y 4 covariables para explicar mpg en función de las variables disp, hp, wt, qsec. Para hacer la búsqueda usamos el siguiente código. nbest=2 porque queremos los mejores dos modelos con cada número de covariables posible. library(leaps) model_subset &lt;- regsubsets(mpg ~ disp + hp + wt + qsec, data=mtcars, nbest=2, nvmax=13) El model_subset es un objeto de la clase regsubsets y es posible usar la función S3 summary para objetos de esa clase. A continuación los elementos que componen summary. names(summary(model_subset)) ## [1] &quot;which&quot; &quot;rsq&quot; &quot;rss&quot; &quot;adjr2&quot; &quot;cp&quot; &quot;bic&quot; &quot;outmat&quot; &quot;obj&quot; El primer elemento del summary se obtiene así: summary(model_subset)$which ## (Intercept) disp hp wt qsec ## 1 TRUE FALSE FALSE TRUE FALSE ## 1 TRUE TRUE FALSE FALSE FALSE ## 2 TRUE FALSE TRUE TRUE FALSE ## 2 TRUE FALSE FALSE TRUE TRUE ## 3 TRUE FALSE TRUE TRUE TRUE ## 3 TRUE TRUE TRUE TRUE FALSE ## 4 TRUE TRUE TRUE TRUE TRUE De las dos primeras líneas de la salida anterior se observa que los dos mejores modelos con una sola covariable son mpg ~ wt y mpg ~ disp. De forma similar, los dos mejores modelos con dos covariables son mpg ~ hp + wt y mpg ~ wt + qsec. De forma análoga se interpretan las líneas de la salida anterior. Es posible mostrar gráficamente los resultados anteriores usando el método S3 plot para objetos de la clase regsubsets. A continuación la estructura de la función plot. El parámetro scale nos permite explorar los mejores modelos para cada uno de los cuatro criterios \\(BIC\\), \\(C_p\\), \\(R^2_{adj}\\) y \\(R^2\\). plot(x, labels=obj$xnames, main=NULL, scale=c(&quot;bic&quot;, &quot;Cp&quot;, &quot;adjr2&quot;, &quot;r2&quot;), col=gray(seq(0, 0.9, length = 10)),...) A continuación el código para mostrar gráficamente los mejores modelos usando el criterio \\(R^2_{adj}\\) y \\(BIC\\). par(mfrow=c(1, 2)) plot(model_subset, scale=&quot;adjr2&quot;, main=expression(R[Adj]^2)) plot(model_subset, scale=&quot;bic&quot;, main=&quot;BIC&quot;) De la figura de la izquierda vemos que: El modelo con mayor \\(R^2_{adj}\\) es mpg ~ hp + wt + qsec. El modelo mpg ~ hp + wt tiene igual \\(R^2_{adj}\\) que mpg ~ wt + qsec. El mejor modelo con una sola covariable es mpg ~ wt. La figura de la derecha se puede analizar de forma análoga. Para obtener los valores exacto de \\(R^2_{adj}\\) y \\(BIC\\) mostrados en la figura anterior se usa el siguiente código. summary(model_subset)$adjr2 ## [1] 0.7445939 0.7089548 0.8148396 0.8144448 0.8170643 0.8082829 0.8107212 summary(model_subset)$bic ## [1] -37.79462 -33.61466 -45.70597 -45.63781 -43.74996 -42.24960 -40.35723 Para encontrar el modelo con el mejor \\(BIC\\) se puede usar lo siguiente: which(summary(model_subset)$bic == min(summary(model_subset)$bic)) ## [1] 3 Para determinar la estructura del modelo 3 identificado en la salida anterior usamos: summary(model_subset)$which[3, ] ## (Intercept) disp hp wt qsec ## TRUE FALSE TRUE TRUE FALSE References Liland, Kristian Hovde. 2023. Mixlm: Mixed Model ANOVA and Statistics for Education. https://github.com/khliland/mixlm/. Lumley, Thomas. 2020. Leaps: Regression Subset Selection. https://CRAN.R-project.org/package=leaps. Ripley, Brian. 2024. MASS: Support Functions and Datasets for Venables and Ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/. "],["feature_selec.html", "17 Feature selection Feature Selection with the Caret R Package", " 17 Feature selection En este capítulo se muestra otra forma de seleccionar variables en la construcción de modelos. Feature Selection with the Caret R Package En este enlace Jason Brownlee presenta un publicación muy interesante de como realizar feature selection por medio del paquete caret. En particular Jason muestra tres formas de seleccionar variables y son: Usando correlaciones. Organizando las variables por importancia en el modelo. Feature selection. Recomiendo al lector que visite el enlace, vea los ejemplos de Jason y que replique los ejemplos de su publicación. "],["multicoli.html", "18 Multicolinealidad ¿Qué es multicolinealidad? Multicolinealidad inducida ¿Cómo identificar la multicolinealidad? Matriz de correlaciones Factor de Inflación de la Varianza (VIF) Análisis de los valores propios de \\(\\mathbf{X}^\\top\\mathbf{X}\\)", " 18 Multicolinealidad En este capítulo se presenta el problema de multicolinealidad y algunas formas de detectarla. ¿Qué es multicolinealidad? Si no hay relación lineal entre los regresores, se dice que éstos son ortogonales, sin embargo, en la mayor parte de las aplicaciones de regresión los regresores no son ortogonales. A veces no es grave la falta de ortogonalidad, pero en algunos casos, los regresores tienen una relación lineal casi perfecta y las inferencias basadas en el modelo de regresión pueden ser engañosas o erróneas. Cuando hay dependencias casi lineales entre los regresores, se dice que existe el problema de colinealidad. Hay cuatro fuentes de multicolinealidad principales: El método de recolección de datos que se empleó (subespacios). Restricciones en el modelo o en la población (variables correlacionadas de hecho). Especificación del modelo (polinomios). Un modelo sobredefinido (más variables que observaciones). ¿Cuáles son los problemas de la multicolinealidad? Grandes varianzas y covarianzas de los estimadores. Estimaciones para los coeficientes demasiado grandes. Pequeños cambios en los datos o en la especificación provocan grandes cambios en las estimaciones de los coeficientes. La estimaciones de los coeficientes suelen presentar signos distintos a los esperados y magnitudes poco razonables. Multicolinealidad inducida La mejor forma de comprender el efecto de la multicolinealidad es crear un conjunto de datos donde exista el problema y ver los efectos que ella tiene. La función que se muestra a continuación simula datos de un modelo de regresión lineal en el cual la variable \\(X_2\\) es múltiplo de la variable \\(X_1\\). El vector de parámetros en la simulación es \\(\\boldsymbol{\\theta}=(-3, 2, -4, \\sigma=2)^\\top\\). gen_dat &lt;- function(n) { x1 &lt;- runif(n=n, min=0, max=10) x2 &lt;- x1 * 2 + rnorm(n=n, sd=0.01) # x2 es el doble de x1 + ruido y &lt;- rnorm(n=n, mean= - 3 + 2 * x1 - 4 * x2, sd=2) data.frame(y, x1, x2) } Vamos a simular dos conjuntos de datos con \\(n=20\\) observaciones cada uno. Se fija la semilla en el valor 12345 para que usted pueda replicar el ejemplo y obtener los mismo resultados. set.seed(12345) datos &lt;- gen_dat(n=40) datos1 &lt;- datos[1:20, ] datos2 &lt;- datos[21:40, ] Ahora vamos a ajustar un modelo con datos1. mod1 &lt;- stats::lm(y ~ x1 + x2, data=datos1) summary(mod1) ## ## Call: ## stats::lm(formula = y ~ x1 + x2, data = datos1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9549 -1.7802 0.5549 1.5669 4.0319 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.560 1.135 -1.374 0.187 ## x1 100.994 94.085 1.073 0.298 ## x2 -53.574 47.073 -1.138 0.271 ## ## Residual standard error: 2.341 on 17 degrees of freedom ## Multiple R-squared: 0.9864, Adjusted R-squared: 0.9848 ## F-statistic: 617.5 on 2 and 17 DF, p-value: &lt; 2.2e-16 Ahora los resultados con el segundo conjunto de datos. mod2 &lt;- stats::lm(y ~ x1 + x2, data=datos2) summary(mod2) ## ## Call: ## stats::lm(formula = y ~ x1 + x2, data = datos2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7246 -1.8012 0.2883 1.2949 3.9046 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.402 1.140 -2.107 0.0503 . ## x1 -36.608 81.561 -0.449 0.6592 ## x2 15.323 40.776 0.376 0.7117 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.303 on 17 degrees of freedom ## Multiple R-squared: 0.9819, Adjusted R-squared: 0.9798 ## F-statistic: 461.3 on 2 and 17 DF, p-value: 1.545e-15 Al observar las salidas anterior se tiene que \\(\\hat{\\boldsymbol{\\theta}}_1=(-1.56, 100.99, -53.57, 2.34)^\\top\\) y que \\(\\hat{\\boldsymbol{\\theta}}_2=(-2.40, -36.31, 15.32, 2.30)^\\top\\). Al comparar con el vector verdadero \\(\\boldsymbol{\\theta}=(-3, 2, -4, \\sigma=2)^\\top\\) se observa que las estimaciones son muy diferentes. De las tablas de resumen también se observa que los errores estándar son muy grandes y por lo tanto los resultados de las pruebas de hipótesis individuales (\\(H_0: \\beta_j=0\\) vs \\(H_A: \\beta_j \\neq 0\\)) no se rechaza \\(H_0\\) por los valores-P grandes. De las tablas de resumen se observan \\(R^2_{Adj}\\) grandes, ¿qué opina de esto. En este ejemplo se usó stats::lm para indicar que queremos la función lm usual del paquete stats. ¿Cómo identificar la multicolinealidad? Se puede realizar de dos manera: Matriz de correlaciones Una primera aproximación consiste en obtener los coeficientes de correlación muestral simples para cada par de variables explicativas y ver si el grado de correlación entre estas variables es alto. Ejemplo Usando la base de datos mtcars explore las variables que están fuertemente correlacionadas. Solución Primero exploremos la base de datos. library(dplyr) mtcars %&gt;% glimpse() ## Rows: 32 ## Columns: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,… ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,… ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16… ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180… ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,… ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.… ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18… ## $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,… ## $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,… ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,… ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,… Ahora vamos a calcular la matriz de correlaciones. mtcars %&gt;% cor(method=&quot;pearson&quot;) %&gt;% round(digits=2) -&gt; mat_cor mat_cor ## mpg cyl disp hp drat wt qsec vs am gear carb ## mpg 1.00 -0.85 -0.85 -0.78 0.68 -0.87 0.42 0.66 0.60 0.48 -0.55 ## cyl -0.85 1.00 0.90 0.83 -0.70 0.78 -0.59 -0.81 -0.52 -0.49 0.53 ## disp -0.85 0.90 1.00 0.79 -0.71 0.89 -0.43 -0.71 -0.59 -0.56 0.39 ## hp -0.78 0.83 0.79 1.00 -0.45 0.66 -0.71 -0.72 -0.24 -0.13 0.75 ## drat 0.68 -0.70 -0.71 -0.45 1.00 -0.71 0.09 0.44 0.71 0.70 -0.09 ## wt -0.87 0.78 0.89 0.66 -0.71 1.00 -0.17 -0.55 -0.69 -0.58 0.43 ## qsec 0.42 -0.59 -0.43 -0.71 0.09 -0.17 1.00 0.74 -0.23 -0.21 -0.66 ## vs 0.66 -0.81 -0.71 -0.72 0.44 -0.55 0.74 1.00 0.17 0.21 -0.57 ## am 0.60 -0.52 -0.59 -0.24 0.71 -0.69 -0.23 0.17 1.00 0.79 0.06 ## gear 0.48 -0.49 -0.56 -0.13 0.70 -0.58 -0.21 0.21 0.79 1.00 0.27 ## carb -0.55 0.53 0.39 0.75 -0.09 0.43 -0.66 -0.57 0.06 0.27 1.00 Mirar la matriz para encontrar las variables que tienen mayor correlación (positiva o negativa) es difícil, lo mejor es usar herramientas gráficas. library(corrplot) ## corrplot 0.92 loaded corrplot(mat_cor, type=&quot;upper&quot;, order=&quot;hclust&quot;, tl.col=&quot;black&quot;, tl.srt=45) Combinando los resultados de la matriz anterior y el conocimiento de las variables es posible identificar variables que podrían generar el problema de multicolinealidad. Factor de Inflación de la Varianza (VIF) El Factor de Inflación de la Varianza se define así: \\[ VIF = \\frac{1}{1 - {R}^{2}_{k}}, \\] siendo \\(R^2_k\\) el coeficiente de determinación de la regresión auxiliar de la varaible \\(X_k\\) sobre el resto de las variables explicativas. El \\(VIF\\) toma valores entre 1 e \\(\\infty\\). El \\(VIF\\) para cada término del modelo mide el efecto combinado que tienen las dependencias entre los regresores sobre la varianza de ese término. Si hay uno o más \\(VIF\\) grandes, hay multicolinealidad. La experiencia indica que si cualquiera de los \\(VIF\\) es mayor que 5 o 10, es indicio de que los coeficientes asociados de regresión están mal estimados debido a la multicolinealidad (E. &amp;. V. Montgomery D. &amp; Peck 2006). La regla de decisión se puede resumir así: Si \\(\\text{VIF} \\leq 5\\) no hay problemas de multicolinealidad. Si \\(5 &lt; \\text{VIF} \\leq 10\\) hay problemas de multicolinealidad moderada. Si \\(\\text{VIF} &gt; 10\\) hay problemas de multicolinealidad graves. Ejemplo Simular un conjunto de datos en el cual existan variables que sean combinación lineal de otras variables. Usar el \\(VIF\\) para identificar las posibles variables colineales y luego comprobar los resultados con la matriz de correlaciones. Solución Vamos a crear una función para generar datos de un modelo lineal donde algunas de las variables que son combinación de otras. Mire el siguiente código e identifique las líneas donde se indujo la colinealidad. gen_dat &lt;- function(n) { x1 &lt;- sample(5:25, size=n, replace=TRUE) x2 &lt;- rpois(n, lambda=5) x3 &lt;- rbinom(n, size=10, prob=0.4) x4 &lt;- rbeta(n=n, shape1=0.5, shape2=0.7) ruido1 &lt;- runif(n=n, min=-0.5, max=0.5) ruido2 &lt;- runif(n=n, min=-0.5, max=0.5) x5 &lt;- 2 * x1 - 3 * x2 + ruido1 x6 &lt;- 3* x2 - 4 * x3 + ruido2 y &lt;- rnorm(n=n, mean= - 3 + 2 * x1 - 4 * x2, sd=2) data.frame(y, x1, x2, x3, x4, x5, x6) } Ahora vamos a generar una base de datos, luego vamos a ajustar el un modelo de regresión para explicar la media de la variable \\(Y\\) en función de las otras variables y por último vamos a calcular los \\(VIF\\) para cada variable explicativa usando la función vif de paquete car creado por John Fox, Weisberg, and Price (2023). datos &lt;- gen_dat(n=30) mod &lt;- lm(y ~ ., data=datos, x=TRUE) car::vif(mod) ## x1 x2 x3 x4 x5 x6 ## 2179.55685 2093.65153 298.35108 1.05934 3675.23843 920.11083 De la salida anterior vemos que los \\(VIF\\) de todas las variables, excepto \\(X_4\\), son muy grandes, eso indica que hay un problema de colinealidad entre esas variables. El \\(VIF\\) para la variable \\(X_4\\) fue pequeño porque en la generación de los datos esa variable no tuvo nada que ver con las restantes. Vamos ahora a dibujar la matriz de correlaciones de Pearson así: mat_cor &lt;- cor(datos[, -1]) library(corrplot) corrplot(mat_cor, type=&quot;upper&quot;, tl.col=&quot;black&quot;, tl.srt=45) De la figura anterior se puede ver que las variables \\(X_5\\), \\(X_1\\), \\(X_2\\), \\(X_6\\) y \\(X_3\\) son las que presentan mayores correlaciones absolutas entre ellas. La variable \\(X_4\\) no muestra correlación con ninguna de las otras covariables. Análisis de los valores propios de \\(\\mathbf{X}^\\top\\mathbf{X}\\) Número de condición Mide la dispersión en el espectro de los valores propios de la matriz \\(\\mathbf{X}^\\top\\mathbf{X}\\). Se calcula como: \\(\\kappa = \\lambda_{\\max} / \\lambda_{\\min}\\). Si \\(\\sqrt{\\kappa} \\leq 10\\) no hay problemas de multicolinealidad. Si \\(10 &lt; \\sqrt{\\kappa} \\leq 31.62\\) hay problemas de multicolinealidad moderada. Si \\(\\sqrt{\\kappa} &gt; 31.62\\) hay problemas de multicolinealidad graves. Ejemplo Considere los datos del ejemplo anterior y calcule \\(\\sqrt{\\kappa}\\). Solución En el modelo ajustado mod está la matriz \\(\\mathbf{X}\\) que nos permite obtener \\(\\sqrt{\\kappa}\\). A continuación el código a usar. X &lt;- mod$x res &lt;- eigen(t(X) %*% X) k &lt;- max(res$values) / min(res$values) sqrt(k) ## [1] 472.4536 De la salida anterior vemos que \\(\\sqrt{\\kappa}\\) es muy pero muy grande, eso significa que hay problemas de multicolinealidad, como era de esperarse. Índice de condición Es una medida útil para determinar el número de dependencias casi lineales en \\(\\mathbf{X}^\\top\\mathbf{X}\\). Se calcula como: \\(\\kappa_j = \\lambda_{\\max} / \\lambda_j,\\ j = 1, \\ldots, p\\). El criterio para detectar multicolinealidad es: Si \\(\\sqrt{\\kappa_j} \\leq 10\\ \\forall j\\), no hay problemas de multicolinealidad. Si al menos para un \\(j\\), \\(10 &lt; \\sqrt{\\kappa_j} \\leq 31.62\\), entonces hay problemas de multicolinealidad moderada. Si al menos para un \\(j\\), \\(\\sqrt{\\kappa_j} &gt; 31.62\\), entonces hay problemas de multicolinealidad graves (). Ejemplo Considere los datos del ejemplo anterior y calcule \\(\\sqrt{\\kappa_j}\\). Solución En el modelo ajustado mod está la matriz \\(\\mathbf{X}\\) que nos permite obtener \\(\\sqrt{\\kappa_j}\\). A continuación el código a usar. X &lt;- mod$x res &lt;- eigen(t(X) %*% X) k &lt;- max(res$values) / res$values sqrt(k) ## [1] 1.000000 2.129467 4.975017 70.662683 116.906646 306.561681 472.453635 De la salida anterior vemos que \\(\\sqrt{\\kappa_j}\\) es muy pero muy grande, eso significa que hay problemas de multicolinealidad, como era de esperarse. References Fox, John, Sanford Weisberg, and Brad Price. 2023. Car: Companion to Applied Regression. https://r-forge.r-project.org/projects/car/. Montgomery, E. &amp; Vining, D. &amp; Peck. 2006. Introducción Al Análisis de Regresión Lineal. 3ed ed. México: Cecsa. "],["algmat.html", "19 Algebra matricial con R Matrices Operaciones básicas sobre una matriz Rango de una matriz", " 19 Algebra matricial con R En este capítulo se muestran algunas funciones útiles para álgebra lineal con R. Matrices Una matriz es un arreglo bidimensional (filas y columna) de números. La función para crear una matriz es matrix. A continuación se muestran los argumentos de la función. matrix(data=NA, nrow=1, ncol=1, byrow=FALSE, dimnames=NULL) Ejemplo Crear en R la matriz \\(M\\) siguiente. \\[ M = \\begin{pmatrix} 1 &amp; 2 \\\\ 1 &amp; 5 \\end{pmatrix} \\] Solución Para crear la matriz podemos usar el siguiente código. M &lt;- matrix(c(1, 2, 1, 5), nrow=2, byrow=TRUE) M ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 5 Operaciones básicas sobre una matriz En la siguiente tabla se listan algunas de las funciones que se pueden utilizar sobre matrices. Los objetos A y B son matrices mientras que los objetos x y b son vectores. Operador Descripción A * B Element-wise multiplication A %*% B Matrix multiplication A %o% B Outer product. \\(AB^\\top\\). t(A) Transpose diag(x) Creates diagonal matrix with elements of x in the principal diagonal diag(A) Returns a vector containing the elements of the principal diagonal diag(k) If k is a scalar, this creates a \\(k \\times k\\) identity matrix. Go figure. solve(A, b) Returns vector x in the equation b = Ax (i.e., A-1b) solve(A) Inverse of A where A is a square matrix. ginv(A) Moore-Penrose Generalized Inverse of A. ginv(A) requires loading the MASS package. cbind(A,B,...) Combine matrices(vectors) horizontally. Returns a matrix. rbind(A,B,...) Combine matrices(vectors) vertically. Returns a matrix. rowMeans(A) Returns vector of row means. rowSums(A) Returns vector of row sums. colMeans(A) Returns vector of column means. colSums(A) Returns vector of column sums. Rango de una matriz El rango de una matriz es el número máximo de columnas (filas respectivamente) que son linealmente independientes. Ejemplo Calcular el rango de la matriz \\(M\\) siguiente. \\[ M = \\begin{pmatrix} 1 &amp; 2 &amp; 5 \\\\ 1 &amp; 5 &amp; 2 \\\\ 1 &amp; 4 &amp; 7 \\end{pmatrix} \\] Solución Lo primero es crear la matriz \\(M\\). M &lt;- matrix(c(1, 2, 5, 1, 5, 2, 1, 4, 7), ncol=3, byrow=TRUE) M ## [,1] [,2] [,3] ## [1,] 1 2 5 ## [2,] 1 5 2 ## [3,] 1 4 7 Una forma de obtener el rango es por medio de la función qr que hace una descomposición QR de la matriz. qr(M)$rank ## [1] 3 De la salida anterior vemos que el rango de \\(M\\) es 3, eso quiere decir que el número máximo de columnas linealmente independientes es 3. Otra forma para obtener el rango de la matriz es por medio de la función rankMatrix del paquete Matrix (Bates, Maechler, and Jagan 2024) así: library(Matrix) rankMatrix(M)[1] ## [1] 3 Ejemplo Calcular el rango de la matriz \\(N\\) siguiente. \\[ N = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 1 &amp; 5 &amp; 6 \\\\ 1 &amp; 7 &amp; 8 \\end{pmatrix} \\] Solución Lo primero es crear la matriz \\(N\\). N &lt;- matrix(c(1, 2, 3, 1, 5, 6, 1, 7, 8), ncol=3, byrow=TRUE) N ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 1 5 6 ## [3,] 1 7 8 Al mirar con calma las columnas de la matriz se observa un detalle, la tercera columna de \\(N\\) se obtiene al sumar la primera y segunda columna de \\(N\\), eso significa que la tercera columna es combinación lineal de las dos primeras. Vamos a calcular el rango de \\(N\\). qr(N)$rank ## [1] 2 El valor de 2 obtenido no nos sorprende porque ya habíamos detectado que la tercera columna es combinación lineal de las dos primeras. References Bates, Douglas, Martin Maechler, and Mikael Jagan. 2024. Matrix: Sparse and Dense Matrix Classes and Methods. https://Matrix.R-forge.R-project.org. "],["WilRog.html", "20 Notación de Wilkinson y Rogers ¿Qué es una fórmula en R? ¿Para qué sirve la notación de Wilkinson y Rogers? Paquete Formula de R", " 20 Notación de Wilkinson y Rogers En este capítulo se muestra la notación de Wilkinson y Rogers (1973) para escribir fórmulas que representan modelos de regresión. ¿Qué es una fórmula en R? Una fórmula es un objeto especial de R para representar un modelo de regresión y se construye usando la función formula. Una fórmula básica tiene tres elementos: La virgulilla ~ que actua como separador. Lado izquierdo que representa la variable respuesta. Lado derecho que contiene las variables explicativas y sus relaciones. A continuación se muestra un ejemplo de una fórmula para representar el modelo estadístico \\(precio = \\beta_0 + \\beta_1 area + \\beta_2 habit\\). my_form &lt;- formula(precio ~ area + habit) my_form ## precio ~ area + habit Para conocer la clase de my_form usamos: class(my_form) ## [1] &quot;formula&quot; Para conocer la longitud de my_form usamos: length(my_form) ## [1] 3 Para explorar los tres elementos de my_form usamos: my_form[1] # el separador ## `~`() my_form[2] # el lado izquierdo ## precio() my_form[3] # el lado derecho ## (area + habit)() ¿Para qué sirve la notación de Wilkinson y Rogers? La notación de Wilkinson y Rogers (1973) permite representar la fórmula algebraica que define un modelo estadístico en una fórmula de R usando unas instrucciones sencillas. A continuación se muestra una tabla en la cual aparecen los símbolos que ayudan a escribir fórmulas para representar modelos estadísticos, la tabla fue tomada de este enlace. Usando los símbolos anteriores se puede escribir gran variedad de modelos, en la siguiente tabla de muestran varios ejemplos. Notación algebraica Notación computacional \\(y = \\beta_0\\) y ~ 1 \\(y = \\beta_0 + \\beta_1 x_1\\) y ~ 1 + x1 ó y ~ x1 \\(y = \\beta_1 x_1\\) y ~ 0 + x1 ó y ~ -1 + x1 \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) y ~ x1 + x2 \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\) y ~ x1*x2 \\(y = \\beta_0 + \\beta_1 x_1 x_2\\) y ~ x1:x2 \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\) y ~ x1 + x2 + x1:x2 \\(y = \\beta_0 + \\sum_{i=1}^{i=999} \\beta_i x_i\\) y ~ . \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^5\\) y ~ x + I(x^5) Paquete Formula de R El manejo de fórmulas es un aspecto importante y por esa razón existe el paquete Formula (Zeileis and Croissant 2023). El lector podrá encontrar una explicación detallada del paquete en este enlace. References Zeileis, Achim, and Yves Croissant. 2023. Formula: Extended Model Formulas. https://CRAN.R-project.org/package=Formula. "],["xy.html", "21 ¿Cómo obtener la matriz \\(\\boldsymbol{X}\\) y el vector \\(\\boldsymbol{y}\\)? Ejemplo", " 21 ¿Cómo obtener la matriz \\(\\boldsymbol{X}\\) y el vector \\(\\boldsymbol{y}\\)? En este corto capítulo se muestra cómo crear la matriz \\(\\boldsymbol{X}\\) y el vector \\(\\boldsymbol{y}\\) necesario para aplicar varios modelos predictivos. Las funciones model.matrix, model.frame y model.extract son útiles para obtener la matriz \\(\\boldsymbol{X}\\) y el vector \\(\\boldsymbol{y}\\). A continuación se muestra un ejemplo ilustrativo. Ejemplo Usando la base de datos de abajo construya la matriz \\(\\boldsymbol{X}\\) y el vector \\(\\boldsymbol{y}\\) para crear un modelo de regresión que explique la variable Precio del apartamento en función del Area del apartamento y Pisci. Tome el nivel Sin de la variable Pisci como nivel de referencia. Solución Lo primero es crear el marco de datos así: datos &lt;- data.frame(Precio = c(12, 15, 25, 11, 16, 7), Area = c(3, 4, 1, 6, 5, 3), Pisci = factor(x=c(&#39;Grande&#39;, &#39;Sin&#39;, &#39;Pequena&#39;, &#39;Pequena&#39;, &#39;Sin&#39;, &#39;Grande&#39;), levels=c(&#39;Sin&#39;,&#39;Pequena&#39;,&#39;Grande&#39;))) datos ## Precio Area Pisci ## 1 12 3 Grande ## 2 15 4 Sin ## 3 25 1 Pequena ## 4 11 6 Pequena ## 5 16 5 Sin ## 6 7 3 Grande Luego usamos la función model.matrix para obtener \\(\\boldsymbol{X}\\). form &lt;- formula(Precio ~ Area + Pisci) X &lt;- model.matrix(object=form, data=datos) X ## (Intercept) Area PisciPequena PisciGrande ## 1 1 3 0 1 ## 2 1 4 0 0 ## 3 1 1 1 0 ## 4 1 6 1 0 ## 5 1 5 0 0 ## 6 1 3 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 2 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$Pisci ## [1] &quot;contr.treatment&quot; Luego usamos las funciones model.frame y model.extract para obtener el vector \\(\\boldsymbol{y}\\). mf &lt;- model.frame(Precio ~ Area + Pisci, data=datos) y &lt;- model.extract(mf, &quot;response&quot;) y ## 1 2 3 4 5 6 ## 12 15 25 11 16 7 "],["important.html", "22 Otros aspectos importantes ¿Qué otras funciones hay en R para ajustar un modelo? Modelos de regresión con grandes conjuntos de datos Fast lm in R", " 22 Otros aspectos importantes En este capítulo se presentan algunos aspectos importantes relacionados con modelación y que no se pudieron incluir en los capítulos anteriores. ¿Qué otras funciones hay en R para ajustar un modelo? Hay 5 funciones en el paquete básico stats para ajustar un modelo de regresión, esas funciones son: lm(formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...) lm.fit(x, y, offset = NULL, method = &quot;qr&quot;, tol = 1e-7, singular.ok = TRUE, ...) lm.wfit(x, y, w, offset = NULL, method = &quot;qr&quot;, tol = 1e-7, singular.ok = TRUE, ...) .lm.fit(x, y, tol = 1e-7) lsfit(x, y, wt = NULL, intercept = TRUE, tolerance = 1e-07, yname = NULL) De todas las anteriores funciones la más conocida y usada es lm. Invito al lector a que consulte la ayuda de las funciones y explore las ventajas que cada una de ellas tiene. Ejemplo Invéntese un problema de regresión lineal simple y use la función lm.fit para encontrar los parámetros del modelo. Solución Supongamos que tenemos las variables que se muestran a continuación. y &lt;- c(4, 2, 3, 1, 5) x &lt;- c(2, 5, 7, 9, 1) Para usar la función lm.fit es necesario que el argumento x sea una matriz, por esa razón vamos a construir la matriz X usando el vector x. X &lt;- cbind(intercepto=1, variable=x) X ## intercepto variable ## [1,] 1 2 ## [2,] 1 5 ## [3,] 1 7 ## [4,] 1 9 ## [5,] 1 1 Ahora ya podemos usar la función lm.fit de la siguiente forma. fit &lt;- lm.fit(x=X, y=y) coef(fit) ## intercepto variable ## 5.0357143 -0.4241071 De la salida anterior tenemos que \\(\\hat{\\beta}_0=5.036\\) y \\(\\hat{\\beta}_1=-0.424\\). Modelos de regresión con grandes conjuntos de datos En ocasiones tenemos grandes bases de datos y cuando queremos ajustar un modelo de regresión se nos pueden presentar problemas porque saturamos la memoria de nuestro computador. En este enlace usted podrá encontrar una publicación de Yixuan Qiu en el 2011 quien explica qué hacer para abordar este tipo de situaciones. Fast lm in R Martin Maechler en 2015 escribió una publicación en Rpubs en la cual compara el tiempo que tardan las funciones lm, lsfit y .lm.fit para ajustar un modelo de regresión. Martin Maechler creó 5 funciones basadas en lm, lsfit y .lm.fit para obtener los parámetros de un modelo y luego comparó los tiempos de procesamiento. Abajo una figura con los resultados de la comparación de tiempos. ¿Cuáles opciones demoran menos tiempo? "],["pasos.html", "23 Pasos para construir modelos Pasos para construir UN modelo Pasos para comparar varios modelos", " 23 Pasos para construir modelos En este capítulo se sugieren unos pasos para la construcción de modelos de regresión. Pasos para construir UN modelo A continuación se muestran los pasos sugeridos. Pasos para comparar varios modelos A continuación se muestran los pasos sugeridos. "],["misspecification.html", "24 Mala especificación", " 24 Mala especificación En este capítulo se muestra consignan algunas estrategias a usar para evitar el efecto de una mala especificación en un modelo lineal. Gherardi (2023) nos presenta una publicación corta de cómo proceder cuando se violan algunos de los supuestos de un modelo lineal. References Gherardi, Valerio. 2023. “Valerio Gherardi: Model Misspecification and Linear Sandwiches.” https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/. "],["references.html", "References", " References Bates, Douglas, Martin Maechler, and Mikael Jagan. 2024. Matrix: Sparse and Dense Matrix Classes and Methods. https://Matrix.R-forge.R-project.org. Braun, W. J., and S. MacQueen. 2023. MPV: Data Sets from Montgomery, Peck and Vining. https://CRAN.R-project.org/package=MPV. Breusch, Trevor S, and Adrian R Pagan. 1979. “A Simple Test for Heteroscedasticity and Random Coefficient Variation.” Econometrica: Journal of the Econometric Society, 1287–94. Cook, R. Dennis, and Sanford Weisberg. 1997. “Graphics for Assessing the Adequacy of Regression Models.” Journal of the American Statistical Association 92 (438): 490–99. https://doi.org/10.1080/01621459.1997.10474002. Fox, John. 2015. Applied Regression Analysis and Generalized Linear Models. Sage Publications. Fox, John, Sanford Weisberg, and Brad Price. 2023. Car: Companion to Applied Regression. https://r-forge.r-project.org/projects/car/. Fox, J., and W. Weisberg. 2019. An r Companion to Applied Regression. Third. SAGE Publications, Inc. Frank Bretz, Peter Westfall, Torsten Hothorn. 2010. Multiple Comparisons Using r. Chapman; Hall/CRC. Gherardi, Valerio. 2023. “Valerio Gherardi: Model Misspecification and Linear Sandwiches.” https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/. Hebbali, Aravind. 2024. Olsrr: Tools for Building OLS Regression Models. https://olsrr.rsquaredacademy.com/. Hernandez, Freddy, and Olga Usuga. 2024. Model: This Package Contains Useful Functions for Modeling Regresion. https://fhernanb.github.io/model. Hothorn, Torsten, Frank Bretz, and Peter Westfall. 2023. Multcomp: Simultaneous Inference in General Parametric Models. http://multcomp.R-forge.R-project.org. Hothorn, Torsten, Achim Zeileis, Richard W. Farebrother, and Clint Cummins. 2022. Lmtest: Testing Linear Regression Models. https://CRAN.R-project.org/package=lmtest. Kutner, M., C. Nachtsheim, J. Neter, and W. Li. 2005. Applied Linear Statistical Models. Fifth. McGraw Hill/Irwin. Liland, Kristian Hovde. 2023. Mixlm: Mixed Model ANOVA and Statistics for Education. https://github.com/khliland/mixlm/. Lumley, Thomas. 2020. Leaps: Regression Subset Selection. https://CRAN.R-project.org/package=leaps. Montgomery, Douglas C., Elizabeth A. Peck, and Geoffrey G. Vining. 2012. Introduction to Linear Regression Analysis (5th Ed.). Wiley &amp; Sons. Montgomery, E. &amp; Vining, D. &amp; Peck. 2006. Introducción Al Análisis de Regresión Lineal. 3ed ed. México: Cecsa. Ripley, Brian. 2024. MASS: Support Functions and Datasets for Venables and Ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/. White, Halbert. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.” Econometrica: Journal of the Econometric Society, 817–38. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ———. 2024. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. Zeileis, Achim, and Yves Croissant. 2023. Formula: Extended Model Formulas. https://CRAN.R-project.org/package=Formula. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
